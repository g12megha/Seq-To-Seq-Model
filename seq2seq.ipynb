{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a seq to seq model using encoders and decoders.Encoder and Decoder embedding matrices will be created which will be used for lookup of word representation for encoder inputs as well as decoder outputs. Here out input will be a seq of numbers and output will be the same sequence shifted by 1 . Output of the encoder will be used as input to decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "num_units = 512\n",
    "MAX_SEQLEN = 10\n",
    "encoder_vocabulary_size = 23\n",
    "encoder_embedding_size = 10\n",
    "decoder_vocabulary_size = 23\n",
    "decoder_embedding_size = 10\n",
    "NLAYERS = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Cell Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Encoder inputs first get converted into a word representation\n",
    "cells = [tf.contrib.rnn.GRUCell(num_units) for _ in range(NLAYERS)]\n",
    "encoder_cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "ENCODER_INPUTS = tf.placeholder(dtype=tf.int32,shape=[None,None],name=\"Encoder_inputs\") #BATCH_SIZE X SEQLEN\n",
    "##embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "encoder_embedings = tf.get_variable(name=\"ENCODER_EMBEDDINGS\",shape=[encoder_vocabulary_size,encoder_embedding_size],initializer=tf.contrib.layers.xavier_initializer(uniform=True,seed=None,dtype=tf.float32),\n",
    "                                     dtype=tf.float32)\n",
    "embed = tf.nn.embedding_lookup(encoder_embedings,ENCODER_INPUTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EncoderSeqLength= tf.placeholder(dtype=tf.int32,shape=[None])\n",
    "output,state = tf.nn.dynamic_rnn(encoder_cell,embed,dtype=tf.float32,sequence_length=EncoderSeqLength)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Cell Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "DecoderSeqLength = tf.placeholder(dtype=tf.int32,shape=[None]);\n",
    "DECODER_INPUTS = tf.placeholder(dtype=tf.int32,shape=[None,None],name=\"DecoderInputs\")\n",
    "\n",
    "decoder_embeddings = tf.get_variable(name=\"DECODER_EMBEDDINGS\",shape=[decoder_vocabulary_size,decoder_embedding_size],initializer=tf.contrib.layers.xavier_initializer(uniform=True,seed=None,dtype=tf.float32),\n",
    "                                     dtype=tf.float32)\n",
    "decoder_embed = tf.nn.embedding_lookup(decoder_embeddings,DECODER_INPUTS)\n",
    "##A helper for use during training. Only reads inputs of dimension decoder_embedding_size \n",
    "##Returned sample_ids are the argmax of the RNN output logits.\n",
    "helper = tf.contrib.seq2seq.TrainingHelper(decoder_embed,sequence_length=DecoderSeqLength)\n",
    "decoder_output_layer = tf.contrib.keras.layers.Dense(decoder_vocabulary_size)\n",
    "## implementation of dense layer : output = activation(dot(input, kernel) + bias) where activation is the element-wise \n",
    "##activation function passed as the activation argument, kernel is a weights matrix created by the layer, \n",
    "##and bias is a bias vector created by the layer (only applicable if use_bias is True).\n",
    "##input paramater will be num_units that is size of output logit \n",
    "## If you don't specify any activation function, no activation is applied(ie. \"linear\" activation: `a(x) = x`)\n",
    "## So here the no activation is applied. Later we will apply the softmax layer on this\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Mechanism and do dynamic decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Apply Attention\n",
    "##creates a class providing functionality of storing the memory(Encoder output states) and the depth of memory to which the query will be done .\n",
    "attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(num_units,output)\n",
    "#multi layer decoder cell\n",
    "d_cells = [tf.contrib.rnn.GRUCell(num_units) for _ in range(NLAYERS)]\n",
    "# apply attention on the top/last layer of the decoder cell and provide it as a wrapper . Output attention true then return attention else if false, then output = output\n",
    "#Working of attention mechanism :\n",
    "attention_cell = tf.contrib.seq2seq.AttentionWrapper(d_cells[-1],attention_mechanism,output_attention=False)\n",
    "d_cells[-1] = attention_cell\n",
    "decoder_cell = tf.contrib.rnn.MultiRNNCell(d_cells)\n",
    "## shape of decoder cell --  (NLayer-1) * RNNCell + AttentionWrappedCell\n",
    "## since the last layer in the cell is of type wrapped, so the last layer in the encoder final state should also be of form wrapped . So modify last layer state to be of type wrapped\n",
    "decoder_batch_size = tf.placeholder(dtype=tf.int32,shape=[1],name=\"DECODER_BATCH_SIZE\") \n",
    "d_state = attention_cell.zero_state(decoder_batch_size,dtype=tf.float32)\n",
    "decoder_input_state = [x for x in state] # convert tuple to array so that item assignment can be done later\n",
    "d_state = d_state.clone(cell_state=decoder_input_state[-1])\n",
    "decoder_input_state[-1] = d_state\n",
    "decoder_input_state = tuple(decoder_input_state)\n",
    "basic_decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell,helper,decoder_input_state,decoder_output_layer)\n",
    "final_outputs, final_state, final_sequence_lengths= tf.contrib.seq2seq.dynamic_decode(basic_decoder)\n",
    "## dynamic_decode calls initialise methof on the basic_decoder once, calls step function repetadely for\n",
    "##sequence_length parameter of training helper times. Initialse method returns (finished, first_inputs, initial_state).\n",
    "##step function takes inputs as step(time,inputs,state,name=None),performs a decoding step and returns as output (outputs, next_state, next_inputs, finished).\n",
    "###Attention model is basically a pool of memory where we randomly access a state which will better determine our current state.Alignment  : which part of source we will be next translating \n",
    "##previous hidden state will be used \n",
    "## score -- score(ht-1 , hs) score each position \n",
    "## alignment -- softmax (score) /pd of diff places of memeory\n",
    "## context vector - sum over alignment * hs\n",
    "## input to current cell : last hidden state + . context vector + decoder input\n",
    "##score : a - > h(t-1)Transpose * hs\n",
    "##        b -> h(t-1)Transpose * W * hs  // interaction between h(t-1) and hs\n",
    "##        c -> v Transpose * tanh(W * [h(t-1) hs]\n",
    "##Perform a step of attention-wrapped RNN.\n",
    "\n",
    "#Step 1: Mix the inputs and previous step's attention output via cell_input_fn - array_ops.concat([inputs, attention],-1): the input and attention both should have same dimensions \n",
    "##except the last column along which conactenation should be performed. Here the concatenated input will be of of length inputs.colum length + attention.colum length(internal cell size)\n",
    "#Step 2: Call the wrapped cell with this input and its previous state.\n",
    "#Step 3: Score the cell's output with attention_mechanism.\n",
    "#Step 4: Calculate the alignments by passing the score through the normalizer.\n",
    "#Step 5: Calculate the context vector as the inner product between the alignments and the attention_mechanism's values (memory).\n",
    "#Step 6: Calculate the attention output by concatenating the cell output and context through the attention layer (a linear layer with attention_layer_size outputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'rnn/while/Exit_2:0' shape=(?, 512) dtype=float32>,\n",
       " <tf.Tensor 'rnn/while/Exit_3:0' shape=(?, 512) dtype=float32>,\n",
       " AttentionWrapperState(cell_state=<tf.Tensor 'rnn/while/Exit_4:0' shape=(?, 512) dtype=float32>, attention=<tf.Tensor 'AttentionWrapperZeroState/zeros_1:0' shape=(?, 512) dtype=float32>, time=<tf.Tensor 'AttentionWrapperZeroState/zeros:0' shape=() dtype=int32>, alignments=<tf.Tensor 'AttentionWrapperZeroState/zeros_2:0' shape=(?, ?) dtype=float32>, alignment_history=()))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##decoder_logits = tf.contrib.layers.linear(final_outputs.rnn_output,decoder_vocabulary_size)\n",
    "decoder_logits = final_outputs.rnn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'decoder/transpose:0' shape=(?, ?, 23) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_prediction = tf.arg_max(tf.nn.softmax(decoder_logits),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_targets = tf.placeholder(dtype=tf.int32,shape=[None,None],name = \"DecoderTargets\")\n",
    "lr = tf.placeholder(dtype=tf.float32,name='lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(decoder_targets,depth=decoder_vocabulary_size,dtype=tf.float32),logits=decoder_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(stepwise_cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimiser = tf.train.AdamOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source = open(\"sources.txt\")\n",
    "train = [[int(word) for word in line.split()] for line in source]\n",
    "target = open(\"targets.txt\")\n",
    "target = [[int(word) for word in line.split()] for line in target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process train data and generate batch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "EOS = 20 ## decoder outputs will end with EOS\n",
    "PAD = 21 ## PAD will be the extra word after EOS or end of encoder inputs to so that length of seq of all inputs in a batch is same\n",
    "GO = 22  ## Decoder Inputs will start with a GO\n",
    "epoch = 20\n",
    "def modifyBatch(input):\n",
    "    inputSeqLen = [len(seq) for seq in input]\n",
    "    maxSourceSeqLen = max(inputSeqLen)\n",
    "    batchedInput = np.ones(shape=(len(input),maxSourceSeqLen),dtype=np.int32)*PAD\n",
    "    for i,seq in enumerate(input):\n",
    "        for j,word in enumerate(seq):\n",
    "            batchedInput[i][j] = word\n",
    "    return batchedInput,inputSeqLen\n",
    "        \n",
    "def generateBatch(source,target,batch_size,epochs):\n",
    "    numBatches = int(len(source)/batch_size)\n",
    "    DecoderInputs = [[GO] + word for word in target ]\n",
    "    DecoderTargets = [word + [EOS] for word in target]\n",
    "    for it in range(epochs):\n",
    "        for i in range(numBatches):\n",
    "            encoderInputs,encoderInputLength = modifyBatch(source[i*batch_size:(i+1)*batch_size])\n",
    "            decoderInputs,decoderInputLength = modifyBatch(DecoderInputs[i*batch_size:(i+1)*batch_size])\n",
    "            decoderTargets,decoderTargetLength = modifyBatch(DecoderTargets[i*batch_size:(i+1)*batch_size])\n",
    "            yield(encoderInputs,encoderInputLength,decoderInputs,decoderInputLength,decoderTargets,decoderTargetLength,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timestamp = str(math.trunc(time.time()))\n",
    "if not os.path.exists(\"seq_seq_checkpoints\"):\n",
    "    os.mkdir(\"seq_seq_checkpoints\")\n",
    "saver = tf.train.Saver(max_to_keep=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch i  0 3.12509\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-0\n",
      "Loss for batch i  1 2.88251\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-1\n",
      "Loss for batch i  2 2.55561\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-2\n",
      "Loss for batch i  3 2.41586\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-3\n",
      "Loss for batch i  4 2.30658\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-4\n",
      "Loss for batch i  5 2.20684\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-5\n",
      "Loss for batch i  6 2.27398\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-6\n",
      "Loss for batch i  7 2.33265\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-7\n",
      "Loss for batch i  8 2.12041\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-8\n",
      "Loss for batch i  9 2.19521\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-9\n",
      "Loss for batch i  10 2.07583\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-10\n",
      "Loss for batch i  11 2.07454\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-11\n",
      "Loss for batch i  12 2.04695\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-12\n",
      "Loss for batch i  13 1.94485\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-13\n",
      "Loss for batch i  14 2.15406\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-14\n",
      "Loss for batch i  15 1.98904\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-15\n",
      "Loss for batch i  16 2.08517\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-16\n",
      "Loss for batch i  17 1.96694\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-17\n",
      "Loss for batch i  18 2.04047\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-18\n",
      "Loss for batch i  19 2.08286\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-19\n",
      "Loss for batch i  20 1.94045\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-20\n",
      "Loss for batch i  21 1.94253\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-21\n",
      "Loss for batch i  22 1.97209\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-22\n",
      "Loss for batch i  23 2.24337\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-23\n",
      "Loss for batch i  24 2.0882\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-24\n",
      "Loss for batch i  25 1.95119\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-25\n",
      "Loss for batch i  26 1.99711\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-26\n",
      "Loss for batch i  27 2.13946\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-27\n",
      "Loss for batch i  28 1.87287\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-28\n",
      "Loss for batch i  29 1.96895\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-29\n",
      "Loss for batch i  30 1.89905\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-30\n",
      "Loss for batch i  31 2.05314\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-31\n",
      "Loss for batch i  32 1.85138\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-32\n",
      "Loss for batch i  33 2.04387\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-33\n",
      "Loss for batch i  34 1.95169\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-34\n",
      "Loss for batch i  35 1.78617\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-35\n",
      "Loss for batch i  36 1.89207\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-36\n",
      "Loss for batch i  37 1.86138\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-37\n",
      "Loss for batch i  38 2.03559\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-38\n",
      "Loss for batch i  39 1.80868\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-39\n",
      "Loss for batch i  40 1.78954\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-40\n",
      "Loss for batch i  41 1.84583\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-41\n",
      "Loss for batch i  42 1.90934\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-42\n",
      "Loss for batch i  43 1.64412\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-43\n",
      "Loss for batch i  44 1.77382\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-44\n",
      "Loss for batch i  45 1.7189\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-45\n",
      "Loss for batch i  46 1.77828\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-46\n",
      "Loss for batch i  47 1.79993\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-47\n",
      "Loss for batch i  48 1.7187\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-48\n",
      "Loss for batch i  49 1.69716\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-49\n",
      "Loss for batch i  50 1.9181\n",
      "Loss for batch i  51 1.79936\n",
      "Loss for batch i  52 1.6511\n",
      "Loss for batch i  53 1.76274\n",
      "Loss for batch i  54 1.88338\n",
      "Loss for batch i  55 1.75411\n",
      "Loss for batch i  56 1.72337\n",
      "Loss for batch i  57 1.86265\n",
      "Loss for batch i  58 1.80617\n",
      "Loss for batch i  59 1.77598\n",
      "Loss for batch i  60 1.62713\n",
      "Loss for batch i  61 1.81376\n",
      "Loss for batch i  62 1.6872\n",
      "Loss for batch i  63 1.75031\n",
      "Loss for batch i  64 1.84892\n",
      "Loss for batch i  65 1.99161\n",
      "Loss for batch i  66 1.71811\n",
      "Loss for batch i  67 1.6678\n",
      "Loss for batch i  68 1.57485\n",
      "Loss for batch i  69 1.7382\n",
      "Loss for batch i  70 1.74258\n",
      "Loss for batch i  71 2.01964\n",
      "Loss for batch i  72 1.7543\n",
      "Loss for batch i  73 1.59973\n",
      "Loss for batch i  74 1.77782\n",
      "Loss for batch i  75 1.79158\n",
      "Loss for batch i  76 1.67478\n",
      "Loss for batch i  77 1.70755\n",
      "Loss for batch i  78 1.63389\n",
      "Loss for batch i  79 1.73641\n",
      "Loss for batch i  80 1.73618\n",
      "Loss for batch i  81 1.55084\n",
      "Loss for batch i  82 1.56367\n",
      "Loss for batch i  83 1.7923\n",
      "Loss for batch i  84 1.61042\n",
      "Loss for batch i  85 1.57802\n",
      "Loss for batch i  86 1.79764\n",
      "Loss for batch i  87 1.65518\n",
      "Loss for batch i  88 1.68655\n",
      "Loss for batch i  89 1.62222\n",
      "Loss for batch i  90 1.44961\n",
      "Loss for batch i  91 1.46273\n",
      "Loss for batch i  92 1.66302\n",
      "Loss for batch i  93 1.6334\n",
      "Loss for batch i  94 1.7532\n",
      "Loss for batch i  95 1.64452\n",
      "Loss for batch i  96 1.7535\n",
      "Loss for batch i  97 1.53119\n",
      "Loss for batch i  98 1.71706\n",
      "Loss for batch i  99 1.57907\n",
      "Loss for batch i  100 1.75528\n",
      "Loss for batch i  101 1.75175\n",
      "Loss for batch i  102 1.38869\n",
      "Loss for batch i  103 1.65506\n",
      "Loss for batch i  104 1.76153\n",
      "Loss for batch i  105 1.64468\n",
      "Loss for batch i  106 1.63954\n",
      "Loss for batch i  107 1.61226\n",
      "Loss for batch i  108 1.49846\n",
      "Loss for batch i  109 1.51999\n",
      "Loss for batch i  110 1.61094\n",
      "Loss for batch i  111 1.57121\n",
      "Loss for batch i  112 1.53644\n",
      "Loss for batch i  113 1.54826\n",
      "Loss for batch i  114 1.42591\n",
      "Loss for batch i  115 1.5552\n",
      "Loss for batch i  116 1.44632\n",
      "Loss for batch i  117 1.63847\n",
      "Loss for batch i  118 1.56946\n",
      "Loss for batch i  119 1.46903\n",
      "Loss for batch i  120 1.55622\n",
      "Loss for batch i  121 1.48169\n",
      "Loss for batch i  122 1.55613\n",
      "Loss for batch i  123 1.60692\n",
      "Loss for batch i  124 1.4621\n",
      "Loss for batch i  125 1.4574\n",
      "Loss for batch i  126 1.53533\n",
      "Loss for batch i  127 1.37019\n",
      "Loss for batch i  128 1.38935\n",
      "Loss for batch i  129 1.41349\n",
      "Loss for batch i  130 1.40133\n",
      "Loss for batch i  131 1.61235\n",
      "Loss for batch i  132 1.56918\n",
      "Loss for batch i  133 1.4712\n",
      "Loss for batch i  134 1.63033\n",
      "Loss for batch i  135 1.59314\n",
      "Loss for batch i  136 1.43472\n",
      "Loss for batch i  137 1.65765\n",
      "Loss for batch i  138 1.44139\n",
      "Loss for batch i  139 1.64595\n",
      "Loss for batch i  140 1.53571\n",
      "Loss for batch i  141 1.43395\n",
      "Loss for batch i  142 1.50262\n",
      "Loss for batch i  143 1.45141\n",
      "Loss for batch i  144 1.40921\n",
      "Loss for batch i  145 1.4098\n",
      "Loss for batch i  146 1.35161\n",
      "Loss for batch i  147 1.35952\n",
      "Loss for batch i  148 1.57166\n",
      "Loss for batch i  149 1.46754\n",
      "Loss for batch i  150 1.31488\n",
      "Loss for batch i  151 1.60036\n",
      "Loss for batch i  152 1.36016\n",
      "Loss for batch i  153 1.5341\n",
      "Loss for batch i  154 1.39279\n",
      "Loss for batch i  155 1.41847\n",
      "Loss for batch i  156 1.36975\n",
      "Loss for batch i  157 1.43547\n",
      "Loss for batch i  158 1.41424\n",
      "Loss for batch i  159 1.44524\n",
      "Loss for batch i  160 1.37882\n",
      "Loss for batch i  161 1.31434\n",
      "Loss for batch i  162 1.34729\n",
      "Loss for batch i  163 1.22433\n",
      "Loss for batch i  164 1.212\n",
      "Loss for batch i  165 1.32903\n",
      "Loss for batch i  166 1.27955\n",
      "Loss for batch i  167 1.15953\n",
      "Loss for batch i  168 1.31503\n",
      "Loss for batch i  169 1.35006\n",
      "Loss for batch i  170 1.30508\n",
      "Loss for batch i  171 1.12221\n",
      "Loss for batch i  172 1.294\n",
      "Loss for batch i  173 1.2668\n",
      "Loss for batch i  174 1.23978\n",
      "Loss for batch i  175 1.27563\n",
      "Loss for batch i  176 1.35383\n",
      "Loss for batch i  177 1.20179\n",
      "Loss for batch i  178 1.29706\n",
      "Loss for batch i  179 1.3089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch i  180 1.33583\n",
      "Loss for batch i  181 1.15898\n",
      "Loss for batch i  182 1.11725\n",
      "Loss for batch i  183 1.2831\n",
      "Loss for batch i  184 1.18022\n",
      "Loss for batch i  185 1.17896\n",
      "Loss for batch i  186 1.08055\n",
      "Loss for batch i  187 1.10654\n",
      "Loss for batch i  188 1.13361\n",
      "Loss for batch i  189 1.24295\n",
      "Loss for batch i  190 1.20901\n",
      "Loss for batch i  191 1.05234\n",
      "Loss for batch i  192 1.15643\n",
      "Loss for batch i  193 1.44597\n",
      "Loss for batch i  194 1.1997\n",
      "Loss for batch i  195 1.22281\n",
      "Loss for batch i  196 1.26457\n",
      "Loss for batch i  197 1.01377\n",
      "Loss for batch i  198 1.27869\n",
      "Loss for batch i  199 1.06704\n",
      "Loss for batch i  0 1.14541\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-0\n",
      "Loss for batch i  1 1.36935\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-1\n",
      "Loss for batch i  2 1.26601\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-2\n",
      "Loss for batch i  3 1.16735\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-3\n",
      "Loss for batch i  4 1.44876\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-4\n",
      "Loss for batch i  5 1.12035\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-5\n",
      "Loss for batch i  6 1.31486\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-6\n",
      "Loss for batch i  7 1.29647\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-7\n",
      "Loss for batch i  8 1.13442\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-8\n",
      "Loss for batch i  9 1.13672\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-9\n",
      "Loss for batch i  10 1.15776\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-10\n",
      "Loss for batch i  11 1.04599\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-11\n",
      "Loss for batch i  12 0.968572\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-12\n",
      "Loss for batch i  13 1.00225\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-13\n",
      "Loss for batch i  14 1.05841\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-14\n",
      "Loss for batch i  15 1.02876\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-15\n",
      "Loss for batch i  16 1.02251\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-16\n",
      "Loss for batch i  17 0.903285\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-17\n",
      "Loss for batch i  18 0.945578\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-18\n",
      "Loss for batch i  19 0.946736\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-19\n",
      "Loss for batch i  20 0.838227\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-20\n",
      "Loss for batch i  21 0.856223\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-21\n",
      "Loss for batch i  22 0.835433\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-22\n",
      "Loss for batch i  23 0.886149\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-23\n",
      "Loss for batch i  24 0.9174\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-24\n",
      "Loss for batch i  25 0.807597\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-25\n",
      "Loss for batch i  26 0.922781\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-26\n",
      "Loss for batch i  27 0.927214\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-27\n",
      "Loss for batch i  28 0.871919\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-28\n",
      "Loss for batch i  29 0.795777\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-29\n",
      "Loss for batch i  30 0.650367\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-30\n",
      "Loss for batch i  31 0.72164\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-31\n",
      "Loss for batch i  32 0.611168\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-32\n",
      "Loss for batch i  33 0.664252\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-33\n",
      "Loss for batch i  34 0.63007\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-34\n",
      "Loss for batch i  35 0.870358\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-35\n",
      "Loss for batch i  36 1.06103\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-36\n",
      "Loss for batch i  37 1.16447\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-37\n",
      "Loss for batch i  38 0.869685\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-38\n",
      "Loss for batch i  39 0.909988\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-39\n",
      "Loss for batch i  40 0.920473\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-40\n",
      "Loss for batch i  41 1.04882\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-41\n",
      "Loss for batch i  42 1.27346\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-42\n",
      "Loss for batch i  43 0.570967\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-43\n",
      "Loss for batch i  44 0.826875\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-44\n",
      "Loss for batch i  45 0.676842\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-45\n",
      "Loss for batch i  46 0.673283\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-46\n",
      "Loss for batch i  47 0.637603\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-47\n",
      "Loss for batch i  48 0.628875\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-48\n",
      "Loss for batch i  49 0.685921\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-49\n",
      "Loss for batch i  50 0.641259\n",
      "Loss for batch i  51 0.622014\n",
      "Loss for batch i  52 0.503913\n",
      "Loss for batch i  53 0.606838\n",
      "Loss for batch i  54 0.597696\n",
      "Loss for batch i  55 0.510123\n",
      "Loss for batch i  56 0.514232\n",
      "Loss for batch i  57 0.508922\n",
      "Loss for batch i  58 0.435437\n",
      "Loss for batch i  59 0.417728\n",
      "Loss for batch i  60 0.368047\n",
      "Loss for batch i  61 0.391263\n",
      "Loss for batch i  62 0.359\n",
      "Loss for batch i  63 0.32305\n",
      "Loss for batch i  64 0.34881\n",
      "Loss for batch i  65 0.522347\n",
      "Loss for batch i  66 1.54559\n",
      "Loss for batch i  67 0.517343\n",
      "Loss for batch i  68 1.79238\n",
      "Loss for batch i  69 0.819451\n",
      "Loss for batch i  70 1.07966\n",
      "Loss for batch i  71 0.841408\n",
      "Loss for batch i  72 1.17854\n",
      "Loss for batch i  73 0.704044\n",
      "Loss for batch i  74 0.843039\n",
      "Loss for batch i  75 0.789577\n",
      "Loss for batch i  76 0.487055\n",
      "Loss for batch i  77 0.598719\n",
      "Loss for batch i  78 0.485923\n",
      "Loss for batch i  79 0.547775\n",
      "Loss for batch i  80 0.464851\n",
      "Loss for batch i  81 0.44536\n",
      "Loss for batch i  82 0.384179\n",
      "Loss for batch i  83 0.438903\n",
      "Loss for batch i  84 0.404752\n",
      "Loss for batch i  85 0.333919\n",
      "Loss for batch i  86 0.466846\n",
      "Loss for batch i  87 0.402218\n",
      "Loss for batch i  88 0.292212\n",
      "Loss for batch i  89 0.329376\n",
      "Loss for batch i  90 0.262167\n",
      "Loss for batch i  91 0.270998\n",
      "Loss for batch i  92 0.263754\n",
      "Loss for batch i  93 0.314958\n",
      "Loss for batch i  94 0.27915\n",
      "Loss for batch i  95 0.280548\n",
      "Loss for batch i  96 0.341537\n",
      "Loss for batch i  97 0.214435\n",
      "Loss for batch i  98 0.257517\n",
      "Loss for batch i  99 0.221747\n",
      "Loss for batch i  100 0.204693\n",
      "Loss for batch i  101 0.234829\n",
      "Loss for batch i  102 0.164504\n",
      "Loss for batch i  103 0.166656\n",
      "Loss for batch i  104 0.193328\n",
      "Loss for batch i  105 0.182184\n",
      "Loss for batch i  106 0.146143\n",
      "Loss for batch i  107 0.170334\n",
      "Loss for batch i  108 0.137766\n",
      "Loss for batch i  109 0.154728\n",
      "Loss for batch i  110 0.123718\n",
      "Loss for batch i  111 0.186711\n",
      "Loss for batch i  112 0.149814\n",
      "Loss for batch i  113 0.152992\n",
      "Loss for batch i  114 0.128035\n",
      "Loss for batch i  115 0.196601\n",
      "Loss for batch i  116 0.359676\n",
      "Loss for batch i  117 1.13958\n",
      "Loss for batch i  118 0.333265\n",
      "Loss for batch i  119 0.285303\n",
      "Loss for batch i  120 0.266987\n",
      "Loss for batch i  121 0.264554\n",
      "Loss for batch i  122 0.220387\n",
      "Loss for batch i  123 0.209779\n",
      "Loss for batch i  124 0.241432\n",
      "Loss for batch i  125 0.206361\n",
      "Loss for batch i  126 0.196019\n",
      "Loss for batch i  127 0.133731\n",
      "Loss for batch i  128 0.175361\n",
      "Loss for batch i  129 0.237411\n",
      "Loss for batch i  130 0.154069\n",
      "Loss for batch i  131 0.270237\n",
      "Loss for batch i  132 0.16796\n",
      "Loss for batch i  133 0.203427\n",
      "Loss for batch i  134 0.193779\n",
      "Loss for batch i  135 0.194021\n",
      "Loss for batch i  136 0.109417\n",
      "Loss for batch i  137 0.129433\n",
      "Loss for batch i  138 0.0966491\n",
      "Loss for batch i  139 0.121061\n",
      "Loss for batch i  140 0.139594\n",
      "Loss for batch i  141 0.0926217\n",
      "Loss for batch i  142 0.107574\n",
      "Loss for batch i  143 0.0830653\n",
      "Loss for batch i  144 0.111259\n",
      "Loss for batch i  145 0.0904573\n",
      "Loss for batch i  146 0.0725741\n",
      "Loss for batch i  147 0.0675176\n",
      "Loss for batch i  148 0.0717902\n",
      "Loss for batch i  149 0.0678896\n",
      "Loss for batch i  150 0.049316\n",
      "Loss for batch i  151 0.0797947\n",
      "Loss for batch i  152 0.0518575\n",
      "Loss for batch i  153 0.0569672\n",
      "Loss for batch i  154 0.0484819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch i  155 0.0481912\n",
      "Loss for batch i  156 0.0491657\n",
      "Loss for batch i  157 0.0552197\n",
      "Loss for batch i  158 0.0479785\n",
      "Loss for batch i  159 0.0390131\n",
      "Loss for batch i  160 0.040968\n",
      "Loss for batch i  161 0.0361332\n",
      "Loss for batch i  162 0.0402215\n",
      "Loss for batch i  163 0.0360328\n",
      "Loss for batch i  164 0.0372847\n",
      "Loss for batch i  165 0.0388913\n",
      "Loss for batch i  166 0.0291943\n",
      "Loss for batch i  167 0.0285364\n",
      "Loss for batch i  168 0.0426318\n",
      "Loss for batch i  169 0.0500227\n",
      "Loss for batch i  170 0.0282381\n",
      "Loss for batch i  171 0.0224281\n",
      "Loss for batch i  172 0.0516338\n",
      "Loss for batch i  173 0.0242627\n",
      "Loss for batch i  174 0.0393281\n",
      "Loss for batch i  175 0.0292565\n",
      "Loss for batch i  176 0.0535849\n",
      "Loss for batch i  177 0.019901\n",
      "Loss for batch i  178 0.0292108\n",
      "Loss for batch i  179 0.0312912\n",
      "Loss for batch i  180 0.034669\n",
      "Loss for batch i  181 0.0207051\n",
      "Loss for batch i  182 0.0305843\n",
      "Loss for batch i  183 0.0246986\n",
      "Loss for batch i  184 0.0219268\n",
      "Loss for batch i  185 0.022483\n",
      "Loss for batch i  186 0.0451079\n",
      "Loss for batch i  187 0.0275747\n",
      "Loss for batch i  188 0.0388197\n",
      "Loss for batch i  189 0.0451392\n",
      "Loss for batch i  190 0.0256675\n",
      "Loss for batch i  191 0.0801616\n",
      "Loss for batch i  192 0.0852461\n",
      "Loss for batch i  193 0.231177\n",
      "Loss for batch i  194 0.0451121\n",
      "Loss for batch i  195 0.633767\n",
      "Loss for batch i  196 3.17706\n",
      "Loss for batch i  197 3.34236\n",
      "Loss for batch i  198 3.74625\n",
      "Loss for batch i  199 2.74008\n",
      "Loss for batch i  0 2.82939\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-0\n",
      "Loss for batch i  1 2.48289\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-1\n",
      "Loss for batch i  2 2.48419\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-2\n",
      "Loss for batch i  3 1.94805\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-3\n",
      "Loss for batch i  4 1.93016\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-4\n",
      "Loss for batch i  5 1.88832\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-5\n",
      "Loss for batch i  6 1.80421\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-6\n",
      "Loss for batch i  7 1.68761\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-7\n",
      "Loss for batch i  8 1.32673\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-8\n",
      "Loss for batch i  9 1.1885\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-9\n",
      "Loss for batch i  10 1.2576\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-10\n",
      "Loss for batch i  11 1.24578\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-11\n",
      "Loss for batch i  12 1.13402\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-12\n",
      "Loss for batch i  13 1.06983\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-13\n",
      "Loss for batch i  14 1.18269\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-14\n",
      "Loss for batch i  15 1.09726\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-15\n",
      "Loss for batch i  16 1.12395\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-16\n",
      "Loss for batch i  17 0.968894\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-17\n",
      "Loss for batch i  18 1.03741\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-18\n",
      "Loss for batch i  19 1.06279\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-19\n",
      "Loss for batch i  20 0.902166\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-20\n",
      "Loss for batch i  21 0.873123\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-21\n",
      "Loss for batch i  22 0.874236\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-22\n",
      "Loss for batch i  23 0.846092\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-23\n",
      "Loss for batch i  24 0.892758\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-24\n",
      "Loss for batch i  25 0.628161\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-25\n",
      "Loss for batch i  26 0.870427\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-26\n",
      "Loss for batch i  27 0.61493\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-27\n",
      "Loss for batch i  28 0.881836\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-28\n",
      "Loss for batch i  29 0.551983\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-29\n",
      "Loss for batch i  30 0.741754\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-30\n",
      "Loss for batch i  31 0.587487\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-31\n",
      "Loss for batch i  32 0.769139\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-32\n",
      "Loss for batch i  33 0.586656\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-33\n",
      "Loss for batch i  34 0.697717\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-34\n",
      "Loss for batch i  35 0.534438\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-35\n",
      "Loss for batch i  36 0.464015\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-36\n",
      "Loss for batch i  37 0.432547\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-37\n",
      "Loss for batch i  38 0.443336\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-38\n",
      "Loss for batch i  39 0.346919\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-39\n",
      "Loss for batch i  40 0.911189\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-40\n",
      "Loss for batch i  41 1.18077\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-41\n",
      "Loss for batch i  42 1.01084\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-42\n",
      "Loss for batch i  43 0.739282\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-43\n",
      "Loss for batch i  44 1.07402\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-44\n",
      "Loss for batch i  45 0.840661\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-45\n",
      "Loss for batch i  46 0.919669\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-46\n",
      "Loss for batch i  47 1.14198\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-47\n",
      "Loss for batch i  48 0.766396\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-48\n",
      "Loss for batch i  49 0.756038\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-49\n",
      "Loss for batch i  50 1.01995\n",
      "Loss for batch i  51 0.760665\n",
      "Loss for batch i  52 0.615794\n",
      "Loss for batch i  53 0.820195\n",
      "Loss for batch i  54 0.665147\n",
      "Loss for batch i  55 0.521626\n",
      "Loss for batch i  56 0.355059\n",
      "Loss for batch i  57 0.578222\n",
      "Loss for batch i  58 0.410644\n",
      "Loss for batch i  59 0.372987\n",
      "Loss for batch i  60 0.356207\n",
      "Loss for batch i  61 0.333735\n",
      "Loss for batch i  62 0.301246\n",
      "Loss for batch i  63 0.245287\n",
      "Loss for batch i  64 0.310583\n",
      "Loss for batch i  65 0.347787\n",
      "Loss for batch i  66 0.205911\n",
      "Loss for batch i  67 0.211911\n",
      "Loss for batch i  68 0.218702\n",
      "Loss for batch i  69 0.201646\n",
      "Loss for batch i  70 0.155925\n",
      "Loss for batch i  71 0.244939\n",
      "Loss for batch i  72 0.270024\n",
      "Loss for batch i  73 0.22558\n",
      "Loss for batch i  74 0.232257\n",
      "Loss for batch i  75 0.182423\n",
      "Loss for batch i  76 0.222267\n",
      "Loss for batch i  77 0.176485\n",
      "Loss for batch i  78 0.403653\n",
      "Loss for batch i  79 0.804385\n",
      "Loss for batch i  80 0.899215\n",
      "Loss for batch i  81 1.08551\n",
      "Loss for batch i  82 1.08222\n",
      "Loss for batch i  83 1.22849\n",
      "Loss for batch i  84 1.1446\n",
      "Loss for batch i  85 0.84636\n",
      "Loss for batch i  86 0.852267\n",
      "Loss for batch i  87 0.88463\n",
      "Loss for batch i  88 0.905457\n",
      "Loss for batch i  89 0.56605\n",
      "Loss for batch i  90 0.399191\n",
      "Loss for batch i  91 0.399656\n",
      "Loss for batch i  92 0.461363\n",
      "Loss for batch i  93 0.523401\n",
      "Loss for batch i  94 0.395342\n",
      "Loss for batch i  95 0.364037\n",
      "Loss for batch i  96 0.591858\n",
      "Loss for batch i  97 0.417374\n",
      "Loss for batch i  98 0.584124\n",
      "Loss for batch i  99 0.438686\n",
      "Loss for batch i  100 0.849662\n",
      "Loss for batch i  101 0.3699\n",
      "Loss for batch i  102 0.418932\n",
      "Loss for batch i  103 0.44503\n",
      "Loss for batch i  104 0.449249\n",
      "Loss for batch i  105 0.525393\n",
      "Loss for batch i  106 0.592033\n",
      "Loss for batch i  107 0.312363\n",
      "Loss for batch i  108 0.506\n",
      "Loss for batch i  109 0.278348\n",
      "Loss for batch i  110 0.476582\n",
      "Loss for batch i  111 0.342797\n",
      "Loss for batch i  112 0.270574\n",
      "Loss for batch i  113 0.477565\n",
      "Loss for batch i  114 0.24869\n",
      "Loss for batch i  115 0.390647\n",
      "Loss for batch i  116 0.467665\n",
      "Loss for batch i  117 0.486007\n",
      "Loss for batch i  118 0.409206\n",
      "Loss for batch i  119 0.454939\n",
      "Loss for batch i  120 0.484575\n",
      "Loss for batch i  121 0.412721\n",
      "Loss for batch i  122 0.661171\n",
      "Loss for batch i  123 0.45037\n",
      "Loss for batch i  124 0.248665\n",
      "Loss for batch i  125 0.451923\n",
      "Loss for batch i  126 0.348318\n",
      "Loss for batch i  127 0.269391\n",
      "Loss for batch i  128 0.295793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch i  129 0.486648\n",
      "Loss for batch i  130 0.736463\n",
      "Loss for batch i  131 1.46314\n",
      "Loss for batch i  132 1.34401\n",
      "Loss for batch i  133 1.05794\n",
      "Loss for batch i  134 1.0669\n",
      "Loss for batch i  135 1.93496\n",
      "Loss for batch i  136 1.97284\n",
      "Loss for batch i  137 1.95819\n",
      "Loss for batch i  138 1.58857\n",
      "Loss for batch i  139 1.26403\n",
      "Loss for batch i  140 0.827934\n",
      "Loss for batch i  141 1.09314\n",
      "Loss for batch i  142 1.0379\n",
      "Loss for batch i  143 1.1397\n",
      "Loss for batch i  144 0.828622\n",
      "Loss for batch i  145 0.663601\n",
      "Loss for batch i  146 0.676688\n",
      "Loss for batch i  147 0.742539\n",
      "Loss for batch i  148 0.89335\n",
      "Loss for batch i  149 0.637776\n",
      "Loss for batch i  150 0.576373\n",
      "Loss for batch i  151 1.17395\n",
      "Loss for batch i  152 0.644996\n",
      "Loss for batch i  153 0.770395\n",
      "Loss for batch i  154 0.692845\n",
      "Loss for batch i  155 0.568577\n",
      "Loss for batch i  156 0.439648\n",
      "Loss for batch i  157 0.427085\n",
      "Loss for batch i  158 0.36065\n",
      "Loss for batch i  159 0.366325\n",
      "Loss for batch i  160 0.251583\n",
      "Loss for batch i  161 0.389227\n",
      "Loss for batch i  162 0.311226\n",
      "Loss for batch i  163 0.287898\n",
      "Loss for batch i  164 0.232173\n",
      "Loss for batch i  165 0.232004\n",
      "Loss for batch i  166 0.22417\n",
      "Loss for batch i  167 0.389554\n",
      "Loss for batch i  168 0.376736\n",
      "Loss for batch i  169 0.511355\n",
      "Loss for batch i  170 0.413102\n",
      "Loss for batch i  171 0.382694\n",
      "Loss for batch i  172 0.354894\n",
      "Loss for batch i  173 0.390549\n",
      "Loss for batch i  174 0.404458\n",
      "Loss for batch i  175 0.444714\n",
      "Loss for batch i  176 0.465341\n",
      "Loss for batch i  177 0.591068\n",
      "Loss for batch i  178 0.363358\n",
      "Loss for batch i  179 1.57357\n",
      "Loss for batch i  180 1.77993\n",
      "Loss for batch i  181 1.64217\n",
      "Loss for batch i  182 1.62542\n",
      "Loss for batch i  183 1.61891\n",
      "Loss for batch i  184 1.28807\n",
      "Loss for batch i  185 1.18423\n",
      "Loss for batch i  186 1.29087\n",
      "Loss for batch i  187 1.08249\n",
      "Loss for batch i  188 1.29238\n",
      "Loss for batch i  189 1.3196\n",
      "Loss for batch i  190 0.994276\n",
      "Loss for batch i  191 0.900253\n",
      "Loss for batch i  192 0.989576\n",
      "Loss for batch i  193 1.01337\n",
      "Loss for batch i  194 1.23035\n",
      "Loss for batch i  195 0.901876\n",
      "Loss for batch i  196 0.84884\n",
      "Loss for batch i  197 0.669214\n",
      "Loss for batch i  198 0.731051\n",
      "Loss for batch i  199 0.576746\n",
      "Loss for batch i  0 0.57117\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-0\n",
      "Loss for batch i  1 0.410808\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-1\n",
      "Loss for batch i  2 1.09962\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-2\n",
      "Loss for batch i  3 0.781088\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-3\n",
      "Loss for batch i  4 0.374785\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-4\n",
      "Loss for batch i  5 0.404763\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-5\n",
      "Loss for batch i  6 0.842532\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-6\n",
      "Loss for batch i  7 0.977342\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-7\n",
      "Loss for batch i  8 0.696042\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-8\n",
      "Loss for batch i  9 0.665874\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-9\n",
      "Loss for batch i  10 1.04337\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-10\n",
      "Loss for batch i  11 0.734421\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-11\n",
      "Loss for batch i  12 0.637471\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-12\n",
      "Loss for batch i  13 0.84879\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-13\n",
      "Loss for batch i  14 1.20386\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-14\n",
      "Loss for batch i  15 0.699945\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-15\n",
      "Loss for batch i  16 0.623828\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-16\n",
      "Loss for batch i  17 0.438633\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-17\n",
      "Loss for batch i  18 0.589731\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-18\n",
      "Loss for batch i  19 0.630921\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-19\n",
      "Loss for batch i  20 0.522985\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-20\n",
      "Loss for batch i  21 0.692258\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-21\n",
      "Loss for batch i  22 0.42966\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-22\n",
      "Loss for batch i  23 0.564416\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-23\n",
      "Loss for batch i  24 0.598036\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-24\n",
      "Loss for batch i  25 0.590342\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-25\n",
      "Loss for batch i  26 0.744121\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-26\n",
      "Loss for batch i  27 0.552979\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-27\n",
      "Loss for batch i  28 0.478607\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-28\n",
      "Loss for batch i  29 0.665645\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-29\n",
      "Loss for batch i  30 0.648477\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-30\n",
      "Loss for batch i  31 0.85914\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-31\n",
      "Loss for batch i  32 0.660633\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-32\n",
      "Loss for batch i  33 0.783347\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-33\n",
      "Loss for batch i  34 0.971366\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-34\n",
      "Loss for batch i  35 1.23611\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-35\n",
      "Loss for batch i  36 1.46508\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-36\n",
      "Loss for batch i  37 1.35164\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-37\n",
      "Loss for batch i  38 1.70578\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-38\n",
      "Loss for batch i  39 1.31935\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-39\n",
      "Loss for batch i  40 1.41983\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-40\n",
      "Loss for batch i  41 1.55186\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-41\n",
      "Loss for batch i  42 1.54771\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-42\n",
      "Loss for batch i  43 0.991282\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-43\n",
      "Loss for batch i  44 1.28589\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-44\n",
      "Loss for batch i  45 0.960054\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-45\n",
      "Loss for batch i  46 1.00381\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-46\n",
      "Loss for batch i  47 0.870327\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-47\n",
      "Loss for batch i  48 0.882967\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-48\n",
      "Loss for batch i  49 0.800787\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-49\n",
      "Loss for batch i  50 0.856598\n",
      "Loss for batch i  51 0.834736\n",
      "Loss for batch i  52 0.618045\n",
      "Loss for batch i  53 0.637104\n",
      "Loss for batch i  54 0.864493\n",
      "Loss for batch i  55 0.72291\n",
      "Loss for batch i  56 0.574433\n",
      "Loss for batch i  57 0.832104\n",
      "Loss for batch i  58 1.10801\n",
      "Loss for batch i  59 1.64972\n",
      "Loss for batch i  60 1.11287\n",
      "Loss for batch i  61 1.56119\n",
      "Loss for batch i  62 1.07671\n",
      "Loss for batch i  63 0.684928\n",
      "Loss for batch i  64 0.926712\n",
      "Loss for batch i  65 1.33369\n",
      "Loss for batch i  66 1.12659\n",
      "Loss for batch i  67 2.17325\n",
      "Loss for batch i  68 2.22654\n",
      "Loss for batch i  69 2.41821\n",
      "Loss for batch i  70 2.17652\n",
      "Loss for batch i  71 2.3501\n",
      "Loss for batch i  72 1.97838\n",
      "Loss for batch i  73 1.87783\n",
      "Loss for batch i  74 2.17416\n",
      "Loss for batch i  75 2.20506\n",
      "Loss for batch i  76 2.02814\n",
      "Loss for batch i  77 1.97084\n",
      "Loss for batch i  78 1.81621\n",
      "Loss for batch i  79 1.87956\n",
      "Loss for batch i  80 1.85653\n",
      "Loss for batch i  81 1.66993\n",
      "Loss for batch i  82 1.65829\n",
      "Loss for batch i  83 1.91547\n",
      "Loss for batch i  84 1.70913\n",
      "Loss for batch i  85 1.65609\n",
      "Loss for batch i  86 1.85115\n",
      "Loss for batch i  87 1.68576\n",
      "Loss for batch i  88 1.76074\n",
      "Loss for batch i  89 1.63917\n",
      "Loss for batch i  90 1.42979\n",
      "Loss for batch i  91 1.43633\n",
      "Loss for batch i  92 1.62603\n",
      "Loss for batch i  93 1.65151\n",
      "Loss for batch i  94 1.73572\n",
      "Loss for batch i  95 1.65096\n",
      "Loss for batch i  96 1.72801\n",
      "Loss for batch i  97 1.47772\n",
      "Loss for batch i  98 1.6453\n",
      "Loss for batch i  99 1.54812\n",
      "Loss for batch i  100 1.67884\n",
      "Loss for batch i  101 1.69709\n",
      "Loss for batch i  102 1.30093\n",
      "Loss for batch i  103 1.59302\n",
      "Loss for batch i  104 1.65629\n",
      "Loss for batch i  105 1.53079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch i  106 1.5522\n",
      "Loss for batch i  107 1.45672\n",
      "Loss for batch i  108 1.39815\n",
      "Loss for batch i  109 1.39091\n",
      "Loss for batch i  110 1.45642\n",
      "Loss for batch i  111 1.3676\n",
      "Loss for batch i  112 1.39246\n",
      "Loss for batch i  113 1.40207\n",
      "Loss for batch i  114 1.23843\n",
      "Loss for batch i  115 1.36726\n",
      "Loss for batch i  116 1.23626\n",
      "Loss for batch i  117 1.45044\n",
      "Loss for batch i  118 1.38365\n",
      "Loss for batch i  119 1.21536\n",
      "Loss for batch i  120 1.33587\n",
      "Loss for batch i  121 1.25728\n",
      "Loss for batch i  122 1.31247\n",
      "Loss for batch i  123 1.41109\n",
      "Loss for batch i  124 1.13524\n",
      "Loss for batch i  125 1.17161\n",
      "Loss for batch i  126 1.24248\n",
      "Loss for batch i  127 1.04402\n",
      "Loss for batch i  128 1.12356\n",
      "Loss for batch i  129 1.06904\n",
      "Loss for batch i  130 1.10354\n",
      "Loss for batch i  131 1.23228\n",
      "Loss for batch i  132 1.20233\n",
      "Loss for batch i  133 1.03572\n",
      "Loss for batch i  134 1.10253\n",
      "Loss for batch i  135 0.912729\n",
      "Loss for batch i  136 0.94349\n",
      "Loss for batch i  137 1.01672\n",
      "Loss for batch i  138 0.794445\n",
      "Loss for batch i  139 0.952637\n",
      "Loss for batch i  140 0.8768\n",
      "Loss for batch i  141 0.740783\n",
      "Loss for batch i  142 0.908843\n",
      "Loss for batch i  143 0.73982\n",
      "Loss for batch i  144 0.715883\n",
      "Loss for batch i  145 0.712366\n",
      "Loss for batch i  146 0.536296\n",
      "Loss for batch i  147 0.646815\n",
      "Loss for batch i  148 0.748411\n",
      "Loss for batch i  149 1.0647\n",
      "Loss for batch i  150 0.529082\n",
      "Loss for batch i  151 1.07716\n",
      "Loss for batch i  152 0.481747\n",
      "Loss for batch i  153 0.863823\n",
      "Loss for batch i  154 0.556153\n",
      "Loss for batch i  155 0.670429\n",
      "Loss for batch i  156 1.11433\n",
      "Loss for batch i  157 1.09538\n",
      "Loss for batch i  158 0.975582\n",
      "Loss for batch i  159 0.963467\n",
      "Loss for batch i  160 0.6896\n",
      "Loss for batch i  161 0.804452\n",
      "Loss for batch i  162 0.748834\n",
      "Loss for batch i  163 0.667088\n",
      "Loss for batch i  164 0.629736\n",
      "Loss for batch i  165 0.690508\n",
      "Loss for batch i  166 0.484884\n",
      "Loss for batch i  167 0.516931\n",
      "Loss for batch i  168 0.751327\n",
      "Loss for batch i  169 0.792821\n",
      "Loss for batch i  170 1.1025\n",
      "Loss for batch i  171 0.80199\n",
      "Loss for batch i  172 1.07021\n",
      "Loss for batch i  173 0.85895\n",
      "Loss for batch i  174 1.00953\n",
      "Loss for batch i  175 1.07868\n",
      "Loss for batch i  176 0.798194\n",
      "Loss for batch i  177 0.539461\n",
      "Loss for batch i  178 1.02534\n",
      "Loss for batch i  179 1.56553\n",
      "Loss for batch i  180 1.55223\n",
      "Loss for batch i  181 1.11935\n",
      "Loss for batch i  182 0.990066\n",
      "Loss for batch i  183 1.16107\n",
      "Loss for batch i  184 1.0728\n",
      "Loss for batch i  185 1.08097\n",
      "Loss for batch i  186 1.1209\n",
      "Loss for batch i  187 1.02274\n",
      "Loss for batch i  188 1.2553\n",
      "Loss for batch i  189 1.08081\n",
      "Loss for batch i  190 0.901795\n",
      "Loss for batch i  191 0.827206\n",
      "Loss for batch i  192 0.906713\n",
      "Loss for batch i  193 0.870124\n",
      "Loss for batch i  194 0.929468\n",
      "Loss for batch i  195 1.06228\n",
      "Loss for batch i  196 0.868906\n",
      "Loss for batch i  197 0.77658\n",
      "Loss for batch i  198 0.736851\n",
      "Loss for batch i  199 0.540209\n",
      "Loss for batch i  0 0.966094\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-0\n",
      "Loss for batch i  1 1.28631\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-1\n",
      "Loss for batch i  2 1.80404\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-2\n",
      "Loss for batch i  3 1.67994\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-3\n",
      "Loss for batch i  4 1.75296\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-4\n",
      "Loss for batch i  5 1.32119\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-5\n",
      "Loss for batch i  6 1.47005\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-6\n",
      "Loss for batch i  7 1.57054\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-7\n",
      "Loss for batch i  8 1.41179\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-8\n",
      "Loss for batch i  9 1.23679\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-9\n",
      "Loss for batch i  10 1.32505\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-10\n",
      "Loss for batch i  11 1.20685\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-11\n",
      "Loss for batch i  12 1.08737\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-12\n",
      "Loss for batch i  13 0.926606\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-13\n",
      "Loss for batch i  14 1.19617\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-14\n",
      "Loss for batch i  15 1.049\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-15\n",
      "Loss for batch i  16 1.13897\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-16\n",
      "Loss for batch i  17 0.989678\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-17\n",
      "Loss for batch i  18 1.10231\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-18\n",
      "Loss for batch i  19 1.10136\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-19\n",
      "Loss for batch i  20 0.886601\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-20\n",
      "Loss for batch i  21 0.756312\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-21\n",
      "Loss for batch i  22 0.724049\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-22\n",
      "Loss for batch i  23 0.875858\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-23\n",
      "Loss for batch i  24 0.503795\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-24\n",
      "Loss for batch i  25 0.557441\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-25\n",
      "Loss for batch i  26 0.564363\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-26\n",
      "Loss for batch i  27 0.444527\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-27\n",
      "Loss for batch i  28 0.485514\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-28\n",
      "Loss for batch i  29 0.560968\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-29\n",
      "Loss for batch i  30 0.56922\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-30\n",
      "Loss for batch i  31 0.52471\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-31\n",
      "Loss for batch i  32 0.505975\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-32\n",
      "Loss for batch i  33 0.387414\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-33\n",
      "Loss for batch i  34 0.592477\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-34\n",
      "Loss for batch i  35 0.507192\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-35\n",
      "Loss for batch i  36 0.481003\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-36\n",
      "Loss for batch i  37 0.523732\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-37\n",
      "Loss for batch i  38 0.43762\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-38\n",
      "Loss for batch i  39 0.31229\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-39\n",
      "Loss for batch i  40 0.722605\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-40\n",
      "Loss for batch i  41 0.636282\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-41\n",
      "Loss for batch i  42 0.619664\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-42\n",
      "Loss for batch i  43 0.325662\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-43\n",
      "Loss for batch i  44 0.741597\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-44\n",
      "Loss for batch i  45 0.424533\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-45\n",
      "Loss for batch i  46 0.330045\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-46\n",
      "Loss for batch i  47 0.378727\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-47\n",
      "Loss for batch i  48 0.334867\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-48\n",
      "Loss for batch i  49 0.555818\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-49\n",
      "Loss for batch i  50 0.388813\n",
      "Loss for batch i  51 0.443961\n",
      "Loss for batch i  52 0.282565\n",
      "Loss for batch i  53 0.342976\n",
      "Loss for batch i  54 0.48307\n",
      "Loss for batch i  55 0.583489\n",
      "Loss for batch i  56 0.457008\n",
      "Loss for batch i  57 0.833437\n",
      "Loss for batch i  58 0.492193\n",
      "Loss for batch i  59 0.616979\n",
      "Loss for batch i  60 0.521132\n",
      "Loss for batch i  61 0.719882\n",
      "Loss for batch i  62 0.65095\n",
      "Loss for batch i  63 0.680537\n",
      "Loss for batch i  64 1.22156\n",
      "Loss for batch i  65 1.76319\n",
      "Loss for batch i  66 1.34824\n",
      "Loss for batch i  67 0.924855\n",
      "Loss for batch i  68 0.846244\n",
      "Loss for batch i  69 1.07056\n",
      "Loss for batch i  70 1.13946\n",
      "Loss for batch i  71 1.53016\n",
      "Loss for batch i  72 0.966126\n",
      "Loss for batch i  73 0.902172\n",
      "Loss for batch i  74 0.965814\n",
      "Loss for batch i  75 0.995395\n",
      "Loss for batch i  76 0.783959\n",
      "Loss for batch i  77 1.11444\n",
      "Loss for batch i  78 0.951305\n",
      "Loss for batch i  79 1.07983\n",
      "Loss for batch i  80 1.20896\n",
      "Loss for batch i  81 0.80199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch i  82 0.986178\n",
      "Loss for batch i  83 1.40521\n",
      "Loss for batch i  84 1.13221\n",
      "Loss for batch i  85 1.02234\n",
      "Loss for batch i  86 1.53398\n",
      "Loss for batch i  87 1.27003\n",
      "Loss for batch i  88 1.31509\n",
      "Loss for batch i  89 1.22767\n",
      "Loss for batch i  90 1.01489\n",
      "Loss for batch i  91 0.894774\n",
      "Loss for batch i  92 1.2179\n",
      "Loss for batch i  93 1.06822\n",
      "Loss for batch i  94 1.12508\n",
      "Loss for batch i  95 1.10048\n",
      "Loss for batch i  96 1.34245\n",
      "Loss for batch i  97 1.16392\n",
      "Loss for batch i  98 1.53603\n",
      "Loss for batch i  99 1.20167\n",
      "Loss for batch i  100 1.4129\n",
      "Loss for batch i  101 1.69177\n",
      "Loss for batch i  102 1.50393\n",
      "Loss for batch i  103 2.04485\n",
      "Loss for batch i  104 2.14865\n",
      "Loss for batch i  105 2.18516\n",
      "Loss for batch i  106 2.09074\n",
      "Loss for batch i  107 2.05534\n",
      "Loss for batch i  108 1.91494\n",
      "Loss for batch i  109 1.8874\n",
      "Loss for batch i  110 2.02916\n",
      "Loss for batch i  111 1.93457\n",
      "Loss for batch i  112 1.92106\n",
      "Loss for batch i  113 1.8857\n",
      "Loss for batch i  114 1.74182\n",
      "Loss for batch i  115 1.91219\n",
      "Loss for batch i  116 1.73241\n",
      "Loss for batch i  117 1.99917\n",
      "Loss for batch i  118 1.95891\n",
      "Loss for batch i  119 1.69995\n",
      "Loss for batch i  120 1.86416\n",
      "Loss for batch i  121 1.73593\n",
      "Loss for batch i  122 1.83891\n",
      "Loss for batch i  123 1.85413\n",
      "Loss for batch i  124 1.7135\n",
      "Loss for batch i  125 1.67404\n",
      "Loss for batch i  126 1.77706\n",
      "Loss for batch i  127 1.57839\n",
      "Loss for batch i  128 1.56923\n",
      "Loss for batch i  129 1.64442\n",
      "Loss for batch i  130 1.55042\n",
      "Loss for batch i  131 1.82863\n",
      "Loss for batch i  132 1.68668\n",
      "Loss for batch i  133 1.57404\n",
      "Loss for batch i  134 1.72319\n",
      "Loss for batch i  135 1.52449\n",
      "Loss for batch i  136 1.54342\n",
      "Loss for batch i  137 1.56052\n",
      "Loss for batch i  138 1.47677\n",
      "Loss for batch i  139 1.65388\n",
      "Loss for batch i  140 1.56557\n",
      "Loss for batch i  141 1.42387\n",
      "Loss for batch i  142 1.53753\n",
      "Loss for batch i  143 1.37786\n",
      "Loss for batch i  144 1.37165\n",
      "Loss for batch i  145 1.37617\n",
      "Loss for batch i  146 1.32628\n",
      "Loss for batch i  147 1.35\n",
      "Loss for batch i  148 1.53207\n",
      "Loss for batch i  149 1.44867\n",
      "Loss for batch i  150 1.14558\n",
      "Loss for batch i  151 1.51796\n",
      "Loss for batch i  152 1.25068\n",
      "Loss for batch i  153 1.38328\n",
      "Loss for batch i  154 1.30506\n",
      "Loss for batch i  155 1.27388\n",
      "Loss for batch i  156 1.20082\n",
      "Loss for batch i  157 1.28989\n",
      "Loss for batch i  158 1.2405\n",
      "Loss for batch i  159 1.24676\n",
      "Loss for batch i  160 1.1417\n",
      "Loss for batch i  161 1.1725\n",
      "Loss for batch i  162 1.2833\n",
      "Loss for batch i  163 1.02932\n",
      "Loss for batch i  164 1.00041\n",
      "Loss for batch i  165 1.2\n",
      "Loss for batch i  166 1.04505\n",
      "Loss for batch i  167 1.00406\n",
      "Loss for batch i  168 1.14508\n",
      "Loss for batch i  169 1.04147\n",
      "Loss for batch i  170 1.06607\n",
      "Loss for batch i  171 0.760832\n",
      "Loss for batch i  172 0.927755\n",
      "Loss for batch i  173 0.928883\n",
      "Loss for batch i  174 0.865643\n",
      "Loss for batch i  175 1.02247\n",
      "Loss for batch i  176 0.89023\n",
      "Loss for batch i  177 0.772859\n",
      "Loss for batch i  178 1.02421\n",
      "Loss for batch i  179 1.05199\n",
      "Loss for batch i  180 1.19279\n",
      "Loss for batch i  181 1.02291\n",
      "Loss for batch i  182 1.49413\n",
      "Loss for batch i  183 1.67369\n",
      "Loss for batch i  184 1.47277\n",
      "Loss for batch i  185 1.48767\n",
      "Loss for batch i  186 1.43268\n",
      "Loss for batch i  187 1.43974\n",
      "Loss for batch i  188 1.58571\n",
      "Loss for batch i  189 1.46861\n",
      "Loss for batch i  190 1.32371\n",
      "Loss for batch i  191 1.23237\n",
      "Loss for batch i  192 1.29152\n",
      "Loss for batch i  193 1.2351\n",
      "Loss for batch i  194 1.44121\n",
      "Loss for batch i  195 1.35443\n",
      "Loss for batch i  196 1.30339\n",
      "Loss for batch i  197 1.09603\n",
      "Loss for batch i  198 1.17464\n",
      "Loss for batch i  199 0.940506\n",
      "Loss for batch i  0 0.99583\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-0\n",
      "Loss for batch i  1 0.942969\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-1\n",
      "Loss for batch i  2 1.11998\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-2\n",
      "Loss for batch i  3 1.03009\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-3\n",
      "Loss for batch i  4 1.06014\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-4\n",
      "Loss for batch i  5 0.885298\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-5\n",
      "Loss for batch i  6 0.96969\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-6\n",
      "Loss for batch i  7 1.02556\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-7\n",
      "Loss for batch i  8 1.05109\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-8\n",
      "Loss for batch i  9 0.849462\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-9\n",
      "Loss for batch i  10 1.02374\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-10\n",
      "Loss for batch i  11 0.892974\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-11\n",
      "Loss for batch i  12 0.898696\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-12\n",
      "Loss for batch i  13 0.675614\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-13\n",
      "Loss for batch i  14 0.884656\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-14\n",
      "Loss for batch i  15 0.785794\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-15\n",
      "Loss for batch i  16 0.952243\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-16\n",
      "Loss for batch i  17 0.786499\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-17\n",
      "Loss for batch i  18 0.838501\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-18\n",
      "Loss for batch i  19 0.690709\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-19\n",
      "Loss for batch i  20 0.928538\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-20\n",
      "Loss for batch i  21 0.772452\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-21\n",
      "Loss for batch i  22 0.873724\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-22\n",
      "Loss for batch i  23 0.880152\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-23\n",
      "Loss for batch i  24 0.886077\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-24\n",
      "Loss for batch i  25 1.15465\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-25\n",
      "Loss for batch i  26 1.33291\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-26\n",
      "Loss for batch i  27 1.36529\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-27\n",
      "Loss for batch i  28 1.32564\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-28\n",
      "Loss for batch i  29 1.20898\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-29\n",
      "Loss for batch i  30 1.0545\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-30\n",
      "Loss for batch i  31 1.19048\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-31\n",
      "Loss for batch i  32 0.942822\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-32\n",
      "Loss for batch i  33 1.11834\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-33\n",
      "Loss for batch i  34 1.00634\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-34\n",
      "Loss for batch i  35 0.79036\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-35\n",
      "Loss for batch i  36 0.638474\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-36\n",
      "Loss for batch i  37 0.828309\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-37\n",
      "Loss for batch i  38 0.878694\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-38\n",
      "Loss for batch i  39 0.668484\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-39\n",
      "Loss for batch i  40 0.536391\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-40\n",
      "Loss for batch i  41 0.69236\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-41\n",
      "Loss for batch i  42 0.614294\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-42\n",
      "Loss for batch i  43 0.553322\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-43\n",
      "Loss for batch i  44 0.544139\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-44\n",
      "Loss for batch i  45 0.437775\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-45\n",
      "Loss for batch i  46 0.450531\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-46\n",
      "Loss for batch i  47 0.603716\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-47\n",
      "Loss for batch i  48 0.960926\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-48\n",
      "Loss for batch i  49 0.773504\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-49\n",
      "Loss for batch i  50 0.492318\n",
      "Loss for batch i  51 0.5943\n",
      "Loss for batch i  52 0.630005\n",
      "Loss for batch i  53 0.932269\n",
      "Loss for batch i  54 0.71801\n",
      "Loss for batch i  55 0.742044\n",
      "Loss for batch i  56 1.11075\n",
      "Loss for batch i  57 1.49508\n",
      "Loss for batch i  58 1.47334\n",
      "Loss for batch i  59 1.98044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch i  60 1.88375\n",
      "Loss for batch i  61 2.0748\n",
      "Loss for batch i  62 1.70852\n",
      "Loss for batch i  63 1.66336\n",
      "Loss for batch i  64 1.91606\n",
      "Loss for batch i  65 2.12566\n",
      "Loss for batch i  66 1.77271\n",
      "Loss for batch i  67 1.7017\n",
      "Loss for batch i  68 1.55926\n",
      "Loss for batch i  69 1.70276\n",
      "Loss for batch i  70 1.69013\n",
      "Loss for batch i  71 1.98323\n",
      "Loss for batch i  72 1.62181\n",
      "Loss for batch i  73 1.37691\n",
      "Loss for batch i  74 1.63835\n",
      "Loss for batch i  75 1.67239\n",
      "Loss for batch i  76 1.5002\n",
      "Loss for batch i  77 1.56135\n",
      "Loss for batch i  78 1.48281\n",
      "Loss for batch i  79 1.48849\n",
      "Loss for batch i  80 1.50606\n",
      "Loss for batch i  81 1.27685\n",
      "Loss for batch i  82 1.31101\n",
      "Loss for batch i  83 1.56955\n",
      "Loss for batch i  84 1.33558\n",
      "Loss for batch i  85 1.21914\n",
      "Loss for batch i  86 1.45268\n",
      "Loss for batch i  87 1.23823\n",
      "Loss for batch i  88 1.26255\n",
      "Loss for batch i  89 1.22053\n",
      "Loss for batch i  90 0.963649\n",
      "Loss for batch i  91 0.934334\n",
      "Loss for batch i  92 1.11263\n",
      "Loss for batch i  93 1.11318\n",
      "Loss for batch i  94 1.16388\n",
      "Loss for batch i  95 1.09742\n",
      "Loss for batch i  96 1.06217\n",
      "Loss for batch i  97 0.894871\n",
      "Loss for batch i  98 1.11101\n",
      "Loss for batch i  99 1.1558\n",
      "Loss for batch i  100 1.47059\n",
      "Loss for batch i  101 1.33164\n",
      "Loss for batch i  102 0.980273\n",
      "Loss for batch i  103 1.29159\n",
      "Loss for batch i  104 1.49626\n",
      "Loss for batch i  105 1.28729\n",
      "Loss for batch i  106 1.08177\n",
      "Loss for batch i  107 1.0552\n",
      "Loss for batch i  108 1.04643\n",
      "Loss for batch i  109 1.10135\n",
      "Loss for batch i  110 1.04621\n",
      "Loss for batch i  111 1.05428\n",
      "Loss for batch i  112 0.855431\n",
      "Loss for batch i  113 1.90597\n",
      "Loss for batch i  114 1.92926\n",
      "Loss for batch i  115 2.02392\n",
      "Loss for batch i  116 1.87526\n",
      "Loss for batch i  117 2.1135\n",
      "Loss for batch i  118 2.02814\n",
      "Loss for batch i  119 1.87146\n",
      "Loss for batch i  120 2.13216\n",
      "Loss for batch i  121 2.00503\n",
      "Loss for batch i  122 2.05107\n",
      "Loss for batch i  123 2.14263\n",
      "Loss for batch i  124 1.91756\n",
      "Loss for batch i  125 1.90423\n",
      "Loss for batch i  126 1.98755\n",
      "Loss for batch i  127 1.80872\n",
      "Loss for batch i  128 1.85698\n",
      "Loss for batch i  129 1.88585\n",
      "Loss for batch i  130 1.80805\n",
      "Loss for batch i  131 2.06643\n",
      "Loss for batch i  132 1.92064\n",
      "Loss for batch i  133 1.81266\n",
      "Loss for batch i  134 1.96708\n",
      "Loss for batch i  135 1.74798\n",
      "Loss for batch i  136 1.82531\n",
      "Loss for batch i  137 1.79965\n",
      "Loss for batch i  138 1.73347\n",
      "Loss for batch i  139 1.98726\n",
      "Loss for batch i  140 1.83221\n",
      "Loss for batch i  141 1.74435\n",
      "Loss for batch i  142 1.8768\n",
      "Loss for batch i  143 1.74128\n",
      "Loss for batch i  144 1.76532\n",
      "Loss for batch i  145 1.71913\n",
      "Loss for batch i  146 1.64361\n",
      "Loss for batch i  147 1.71316\n",
      "Loss for batch i  148 1.96213\n",
      "Loss for batch i  149 1.83428\n",
      "Loss for batch i  150 1.61447\n",
      "Loss for batch i  151 1.9618\n",
      "Loss for batch i  152 1.70087\n",
      "Loss for batch i  153 1.88484\n",
      "Loss for batch i  154 1.77938\n",
      "Loss for batch i  155 1.77505\n",
      "Loss for batch i  156 1.73491\n",
      "Loss for batch i  157 1.82821\n",
      "Loss for batch i  158 1.79482\n",
      "Loss for batch i  159 1.85247\n",
      "Loss for batch i  160 1.78679\n",
      "Loss for batch i  161 1.69833\n",
      "Loss for batch i  162 1.74117\n",
      "Loss for batch i  163 1.6166\n",
      "Loss for batch i  164 1.60943\n",
      "Loss for batch i  165 1.73835\n",
      "Loss for batch i  166 1.67205\n",
      "Loss for batch i  167 1.61344\n",
      "Loss for batch i  168 1.80691\n",
      "Loss for batch i  169 1.82392\n",
      "Loss for batch i  170 1.75061\n",
      "Loss for batch i  171 1.5696\n",
      "Loss for batch i  172 1.77034\n",
      "Loss for batch i  173 1.67379\n",
      "Loss for batch i  174 1.70483\n",
      "Loss for batch i  175 1.80007\n",
      "Loss for batch i  176 1.76671\n",
      "Loss for batch i  177 1.64265\n",
      "Loss for batch i  178 1.80219\n",
      "Loss for batch i  179 1.82656\n",
      "Loss for batch i  180 1.77531\n",
      "Loss for batch i  181 1.62717\n",
      "Loss for batch i  182 1.63541\n",
      "Loss for batch i  183 1.75669\n",
      "Loss for batch i  184 1.66288\n",
      "Loss for batch i  185 1.7255\n",
      "Loss for batch i  186 1.68019\n",
      "Loss for batch i  187 1.69584\n",
      "Loss for batch i  188 1.78786\n",
      "Loss for batch i  189 1.75471\n",
      "Loss for batch i  190 1.65535\n",
      "Loss for batch i  191 1.64757\n",
      "Loss for batch i  192 1.72321\n",
      "Loss for batch i  193 1.64065\n",
      "Loss for batch i  194 1.82473\n",
      "Loss for batch i  195 1.73174\n",
      "Loss for batch i  196 1.68324\n",
      "Loss for batch i  197 1.58949\n",
      "Loss for batch i  198 1.70085\n",
      "Loss for batch i  199 1.49494\n",
      "Loss for batch i  0 1.59318\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-0\n",
      "Loss for batch i  1 1.57484\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-1\n",
      "Loss for batch i  2 1.74349\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-2\n",
      "Loss for batch i  3 1.61423\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-3\n",
      "Loss for batch i  4 1.6791\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-4\n",
      "Loss for batch i  5 1.52187\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-5\n",
      "Loss for batch i  6 1.68963\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-6\n",
      "Loss for batch i  7 1.75312\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-7\n",
      "Loss for batch i  8 1.61449\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-8\n",
      "Loss for batch i  9 1.57673\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-9\n",
      "Loss for batch i  10 1.65509\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-10\n",
      "Loss for batch i  11 1.57462\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-11\n",
      "Loss for batch i  12 1.53596\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-12\n",
      "Loss for batch i  13 1.51639\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-13\n",
      "Loss for batch i  14 1.65904\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-14\n",
      "Loss for batch i  15 1.58362\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-15\n",
      "Loss for batch i  16 1.70058\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-16\n",
      "Loss for batch i  17 1.59163\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-17\n",
      "Loss for batch i  18 1.67694\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-18\n",
      "Loss for batch i  19 1.75969\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-19\n",
      "Loss for batch i  20 1.61919\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-20\n",
      "Loss for batch i  21 1.6236\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-21\n",
      "Loss for batch i  22 1.65382\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-22\n",
      "Loss for batch i  23 1.79005\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-23\n",
      "Loss for batch i  24 1.62055\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-24\n",
      "Loss for batch i  25 1.59803\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-25\n",
      "Loss for batch i  26 1.65477\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-26\n",
      "Loss for batch i  27 1.73091\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-27\n",
      "Loss for batch i  28 1.55372\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-28\n",
      "Loss for batch i  29 1.66265\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-29\n",
      "Loss for batch i  30 1.57553\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-30\n",
      "Loss for batch i  31 1.74191\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-31\n",
      "Loss for batch i  32 1.5416\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-32\n",
      "Loss for batch i  33 1.73793\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-33\n",
      "Loss for batch i  34 1.69261\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-34\n",
      "Loss for batch i  35 1.48674\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-35\n",
      "Loss for batch i  36 1.58718\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-36\n",
      "Loss for batch i  37 1.58024\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-37\n",
      "Loss for batch i  38 1.7293\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-38\n",
      "Loss for batch i  39 1.53571\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-39\n",
      "Loss for batch i  40 1.5291\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-40\n",
      "Loss for batch i  41 1.62489\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-41\n",
      "Loss for batch i  42 1.66333\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-42\n",
      "Loss for batch i  43 1.39635\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-43\n",
      "Loss for batch i  44 1.56886\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-44\n",
      "Loss for batch i  45 1.47745\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-45\n",
      "Loss for batch i  46 1.62856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-46\n",
      "Loss for batch i  47 1.55156\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-47\n",
      "Loss for batch i  48 1.60576\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-48\n",
      "Loss for batch i  49 1.49701\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-49\n",
      "Loss for batch i  50 1.76826\n",
      "Loss for batch i  51 1.57808\n",
      "Loss for batch i  52 1.45434\n",
      "Loss for batch i  53 1.61787\n",
      "Loss for batch i  54 1.68531\n",
      "Loss for batch i  55 1.61601\n",
      "Loss for batch i  56 1.59303\n",
      "Loss for batch i  57 1.67795\n",
      "Loss for batch i  58 1.62631\n",
      "Loss for batch i  59 1.58504\n",
      "Loss for batch i  60 1.46057\n",
      "Loss for batch i  61 1.5763\n",
      "Loss for batch i  62 1.55764\n",
      "Loss for batch i  63 1.49784\n",
      "Loss for batch i  64 1.67047\n",
      "Loss for batch i  65 1.85301\n",
      "Loss for batch i  66 1.5847\n",
      "Loss for batch i  67 1.55019\n",
      "Loss for batch i  68 1.42528\n",
      "Loss for batch i  69 1.55507\n",
      "Loss for batch i  70 1.57438\n",
      "Loss for batch i  71 1.88597\n",
      "Loss for batch i  72 1.57443\n",
      "Loss for batch i  73 1.4462\n",
      "Loss for batch i  74 1.6279\n",
      "Loss for batch i  75 1.63533\n",
      "Loss for batch i  76 1.53112\n",
      "Loss for batch i  77 1.5663\n",
      "Loss for batch i  78 1.51916\n",
      "Loss for batch i  79 1.57832\n",
      "Loss for batch i  80 1.59751\n",
      "Loss for batch i  81 1.42335\n",
      "Loss for batch i  82 1.4236\n",
      "Loss for batch i  83 1.65021\n",
      "Loss for batch i  84 1.45221\n",
      "Loss for batch i  85 1.4546\n",
      "Loss for batch i  86 1.66449\n",
      "Loss for batch i  87 1.50426\n",
      "Loss for batch i  88 1.5386\n",
      "Loss for batch i  89 1.48761\n",
      "Loss for batch i  90 1.31246\n",
      "Loss for batch i  91 1.32622\n",
      "Loss for batch i  92 1.48986\n",
      "Loss for batch i  93 1.56502\n",
      "Loss for batch i  94 1.57865\n",
      "Loss for batch i  95 1.54618\n",
      "Loss for batch i  96 1.63056\n",
      "Loss for batch i  97 1.54246\n",
      "Loss for batch i  98 1.6743\n",
      "Loss for batch i  99 1.53864\n",
      "Loss for batch i  100 1.70659\n",
      "Loss for batch i  101 1.65899\n",
      "Loss for batch i  102 1.27007\n",
      "Loss for batch i  103 1.62723\n",
      "Loss for batch i  104 1.68164\n",
      "Loss for batch i  105 1.58053\n",
      "Loss for batch i  106 1.62242\n",
      "Loss for batch i  107 1.5669\n",
      "Loss for batch i  108 1.43361\n",
      "Loss for batch i  109 1.456\n",
      "Loss for batch i  110 1.56\n",
      "Loss for batch i  111 1.49345\n",
      "Loss for batch i  112 1.46385\n",
      "Loss for batch i  113 1.51163\n",
      "Loss for batch i  114 1.35106\n",
      "Loss for batch i  115 1.47465\n",
      "Loss for batch i  116 1.39736\n",
      "Loss for batch i  117 1.58365\n",
      "Loss for batch i  118 1.53419\n",
      "Loss for batch i  119 1.40375\n",
      "Loss for batch i  120 1.49867\n",
      "Loss for batch i  121 1.43569\n",
      "Loss for batch i  122 1.47733\n",
      "Loss for batch i  123 1.55322\n",
      "Loss for batch i  124 1.37831\n",
      "Loss for batch i  125 1.40137\n",
      "Loss for batch i  126 1.49712\n",
      "Loss for batch i  127 1.33692\n",
      "Loss for batch i  128 1.32089\n",
      "Loss for batch i  129 1.39159\n",
      "Loss for batch i  130 1.32094\n",
      "Loss for batch i  131 1.56273\n",
      "Loss for batch i  132 1.47562\n",
      "Loss for batch i  133 1.36756\n",
      "Loss for batch i  134 1.48611\n",
      "Loss for batch i  135 1.30525\n",
      "Loss for batch i  136 1.37186\n",
      "Loss for batch i  137 1.35997\n",
      "Loss for batch i  138 1.30005\n",
      "Loss for batch i  139 1.52027\n",
      "Loss for batch i  140 1.39728\n",
      "Loss for batch i  141 1.34506\n",
      "Loss for batch i  142 1.38564\n",
      "Loss for batch i  143 1.29748\n",
      "Loss for batch i  144 1.30887\n",
      "Loss for batch i  145 1.2968\n",
      "Loss for batch i  146 1.22141\n",
      "Loss for batch i  147 1.28929\n",
      "Loss for batch i  148 1.46527\n",
      "Loss for batch i  149 1.39194\n",
      "Loss for batch i  150 1.17839\n",
      "Loss for batch i  151 1.51524\n",
      "Loss for batch i  152 1.26948\n",
      "Loss for batch i  153 1.52819\n",
      "Loss for batch i  154 1.49595\n",
      "Loss for batch i  155 1.4717\n",
      "Loss for batch i  156 1.43964\n",
      "Loss for batch i  157 1.48344\n",
      "Loss for batch i  158 1.48281\n",
      "Loss for batch i  159 1.59566\n",
      "Loss for batch i  160 1.41483\n",
      "Loss for batch i  161 1.38001\n",
      "Loss for batch i  162 1.44965\n",
      "Loss for batch i  163 1.28938\n",
      "Loss for batch i  164 1.27839\n",
      "Loss for batch i  165 1.40272\n",
      "Loss for batch i  166 1.33309\n",
      "Loss for batch i  167 1.2438\n",
      "Loss for batch i  168 1.43755\n",
      "Loss for batch i  169 1.46712\n",
      "Loss for batch i  170 1.38151\n",
      "Loss for batch i  171 1.23236\n",
      "Loss for batch i  172 1.40511\n",
      "Loss for batch i  173 1.29377\n",
      "Loss for batch i  174 1.35105\n",
      "Loss for batch i  175 1.43192\n",
      "Loss for batch i  176 1.39528\n",
      "Loss for batch i  177 1.28686\n",
      "Loss for batch i  178 1.39505\n",
      "Loss for batch i  179 1.45701\n",
      "Loss for batch i  180 1.36477\n",
      "Loss for batch i  181 1.25648\n",
      "Loss for batch i  182 1.25752\n",
      "Loss for batch i  183 1.37337\n",
      "Loss for batch i  184 1.2796\n",
      "Loss for batch i  185 1.3959\n",
      "Loss for batch i  186 1.28096\n",
      "Loss for batch i  187 1.33766\n",
      "Loss for batch i  188 1.35701\n",
      "Loss for batch i  189 1.3912\n",
      "Loss for batch i  190 1.28332\n",
      "Loss for batch i  191 1.29114\n",
      "Loss for batch i  192 1.33416\n",
      "Loss for batch i  193 1.28562\n",
      "Loss for batch i  194 1.45055\n",
      "Loss for batch i  195 1.39674\n",
      "Loss for batch i  196 1.28451\n",
      "Loss for batch i  197 1.2609\n",
      "Loss for batch i  198 1.35822\n",
      "Loss for batch i  199 1.10688\n",
      "Loss for batch i  0 1.23212\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-0\n",
      "Loss for batch i  1 1.19415\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-1\n",
      "Loss for batch i  2 1.37535\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-2\n",
      "Loss for batch i  3 1.22004\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-3\n",
      "Loss for batch i  4 1.32573\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-4\n",
      "Loss for batch i  5 1.16281\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-5\n",
      "Loss for batch i  6 1.33998\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-6\n",
      "Loss for batch i  7 1.38257\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-7\n",
      "Loss for batch i  8 1.26003\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-8\n",
      "Loss for batch i  9 1.21666\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-9\n",
      "Loss for batch i  10 1.2675\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-10\n",
      "Loss for batch i  11 1.21472\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-11\n",
      "Loss for batch i  12 1.17537\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-12\n",
      "Loss for batch i  13 1.10992\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-13\n",
      "Loss for batch i  14 1.3077\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-14\n",
      "Loss for batch i  15 1.20091\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-15\n",
      "Loss for batch i  16 1.30327\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-16\n",
      "Loss for batch i  17 1.22213\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-17\n",
      "Loss for batch i  18 1.28113\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-18\n",
      "Loss for batch i  19 1.36098\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-19\n",
      "Loss for batch i  20 1.20905\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-20\n",
      "Loss for batch i  21 1.23576\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-21\n",
      "Loss for batch i  22 1.25086\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-22\n",
      "Loss for batch i  23 1.34387\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-23\n",
      "Loss for batch i  24 1.21674\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-24\n",
      "Loss for batch i  25 1.20436\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-25\n",
      "Loss for batch i  26 1.27864\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-26\n",
      "Loss for batch i  27 1.28938\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-27\n",
      "Loss for batch i  28 1.15653\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-28\n",
      "Loss for batch i  29 1.26571\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-29\n",
      "Loss for batch i  30 1.17476\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-30\n",
      "Loss for batch i  31 1.31308\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-31\n",
      "Loss for batch i  32 1.12181\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-32\n",
      "Loss for batch i  33 1.31973\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-33\n",
      "Loss for batch i  34 1.258\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-34\n",
      "Loss for batch i  35 1.08554\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-35\n",
      "Loss for batch i  36 1.14128\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-36\n",
      "Loss for batch i  37 1.14672\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-37\n",
      "Loss for batch i  38 1.3075\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-38\n",
      "Loss for batch i  39 1.14985\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch i  40 1.15074\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-40\n",
      "Loss for batch i  41 1.16797\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-41\n",
      "Loss for batch i  42 1.32464\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-42\n",
      "Loss for batch i  43 1.04338\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-43\n",
      "Loss for batch i  44 1.11602\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-44\n",
      "Loss for batch i  45 1.10167\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-45\n",
      "Loss for batch i  46 1.15238\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-46\n",
      "Loss for batch i  47 1.19546\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-47\n",
      "Loss for batch i  48 1.13298\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-48\n",
      "Loss for batch i  49 1.12312\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-49\n",
      "Loss for batch i  50 1.26045\n",
      "Loss for batch i  51 1.18009\n",
      "Loss for batch i  52 1.07489\n",
      "Loss for batch i  53 1.17938\n",
      "Loss for batch i  54 1.3168\n",
      "Loss for batch i  55 1.1302\n",
      "Loss for batch i  56 1.106\n",
      "Loss for batch i  57 1.24186\n",
      "Loss for batch i  58 1.1513\n",
      "Loss for batch i  59 1.10934\n",
      "Loss for batch i  60 0.999422\n",
      "Loss for batch i  61 1.14618\n",
      "Loss for batch i  62 1.09755\n",
      "Loss for batch i  63 1.04515\n",
      "Loss for batch i  64 1.20277\n",
      "Loss for batch i  65 1.30431\n",
      "Loss for batch i  66 1.10249\n",
      "Loss for batch i  67 1.08701\n",
      "Loss for batch i  68 0.969661\n",
      "Loss for batch i  69 1.0873\n",
      "Loss for batch i  70 1.08542\n",
      "Loss for batch i  71 1.40585\n",
      "Loss for batch i  72 1.26157\n",
      "Loss for batch i  73 0.997092\n",
      "Loss for batch i  74 1.40646\n",
      "Loss for batch i  75 1.13841\n",
      "Loss for batch i  76 1.18852\n",
      "Loss for batch i  77 1.12789\n",
      "Loss for batch i  78 1.13404\n",
      "Loss for batch i  79 1.15613\n",
      "Loss for batch i  80 1.29296\n",
      "Loss for batch i  81 1.07631\n",
      "Loss for batch i  82 1.11654\n",
      "Loss for batch i  83 1.26179\n",
      "Loss for batch i  84 1.12676\n",
      "Loss for batch i  85 1.13362\n",
      "Loss for batch i  86 1.26872\n",
      "Loss for batch i  87 1.0602\n",
      "Loss for batch i  88 1.1807\n",
      "Loss for batch i  89 1.06343\n",
      "Loss for batch i  90 0.979275\n",
      "Loss for batch i  91 0.91244\n",
      "Loss for batch i  92 1.06397\n",
      "Loss for batch i  93 1.08211\n",
      "Loss for batch i  94 1.1623\n",
      "Loss for batch i  95 1.07113\n",
      "Loss for batch i  96 1.15073\n",
      "Loss for batch i  97 1.05765\n",
      "Loss for batch i  98 1.13353\n",
      "Loss for batch i  99 1.12635\n",
      "Loss for batch i  100 1.10829\n",
      "Loss for batch i  101 1.27311\n",
      "Loss for batch i  102 0.842774\n",
      "Loss for batch i  103 1.17636\n",
      "Loss for batch i  104 1.10594\n",
      "Loss for batch i  105 1.11861\n",
      "Loss for batch i  106 1.06165\n",
      "Loss for batch i  107 1.03469\n",
      "Loss for batch i  108 0.930152\n",
      "Loss for batch i  109 0.953935\n",
      "Loss for batch i  110 0.968509\n",
      "Loss for batch i  111 0.944813\n",
      "Loss for batch i  112 0.900397\n",
      "Loss for batch i  113 0.953276\n",
      "Loss for batch i  114 0.823222\n",
      "Loss for batch i  115 0.961899\n",
      "Loss for batch i  116 0.884\n",
      "Loss for batch i  117 1.00921\n",
      "Loss for batch i  118 0.966416\n",
      "Loss for batch i  119 0.865681\n",
      "Loss for batch i  120 0.96428\n",
      "Loss for batch i  121 0.87221\n",
      "Loss for batch i  122 0.934927\n",
      "Loss for batch i  123 0.943779\n",
      "Loss for batch i  124 0.861839\n",
      "Loss for batch i  125 0.876973\n",
      "Loss for batch i  126 0.878108\n",
      "Loss for batch i  127 0.787309\n",
      "Loss for batch i  128 0.795596\n",
      "Loss for batch i  129 0.83139\n",
      "Loss for batch i  130 0.78533\n",
      "Loss for batch i  131 0.94269\n",
      "Loss for batch i  132 0.869231\n",
      "Loss for batch i  133 0.818719\n",
      "Loss for batch i  134 0.907681\n",
      "Loss for batch i  135 0.821238\n",
      "Loss for batch i  136 0.767464\n",
      "Loss for batch i  137 0.807414\n",
      "Loss for batch i  138 0.815692\n",
      "Loss for batch i  139 0.845264\n",
      "Loss for batch i  140 1.01144\n",
      "Loss for batch i  141 0.934389\n",
      "Loss for batch i  142 0.955568\n",
      "Loss for batch i  143 0.888619\n",
      "Loss for batch i  144 0.924512\n",
      "Loss for batch i  145 0.83528\n",
      "Loss for batch i  146 0.785216\n",
      "Loss for batch i  147 0.836252\n",
      "Loss for batch i  148 0.827291\n",
      "Loss for batch i  149 0.839074\n",
      "Loss for batch i  150 0.70343\n",
      "Loss for batch i  151 0.907067\n",
      "Loss for batch i  152 0.729845\n",
      "Loss for batch i  153 0.853948\n",
      "Loss for batch i  154 0.74764\n",
      "Loss for batch i  155 0.793487\n",
      "Loss for batch i  156 0.738565\n",
      "Loss for batch i  157 0.752411\n",
      "Loss for batch i  158 0.726701\n",
      "Loss for batch i  159 0.747174\n",
      "Loss for batch i  160 0.764529\n",
      "Loss for batch i  161 0.674201\n",
      "Loss for batch i  162 0.657167\n",
      "Loss for batch i  163 0.60657\n",
      "Loss for batch i  164 0.612881\n",
      "Loss for batch i  165 0.650165\n",
      "Loss for batch i  166 0.598432\n",
      "Loss for batch i  167 0.476714\n",
      "Loss for batch i  168 0.607299\n",
      "Loss for batch i  169 0.66057\n",
      "Loss for batch i  170 0.716301\n",
      "Loss for batch i  171 0.563955\n",
      "Loss for batch i  172 0.652669\n",
      "Loss for batch i  173 0.705788\n",
      "Loss for batch i  174 0.675281\n",
      "Loss for batch i  175 0.995834\n",
      "Loss for batch i  176 0.828479\n",
      "Loss for batch i  177 0.79357\n",
      "Loss for batch i  178 0.812901\n",
      "Loss for batch i  179 0.837686\n",
      "Loss for batch i  180 0.721015\n",
      "Loss for batch i  181 0.632908\n",
      "Loss for batch i  182 0.691945\n",
      "Loss for batch i  183 0.657084\n",
      "Loss for batch i  184 0.635855\n",
      "Loss for batch i  185 0.592002\n",
      "Loss for batch i  186 0.598458\n",
      "Loss for batch i  187 0.571263\n",
      "Loss for batch i  188 0.583502\n",
      "Loss for batch i  189 0.696943\n",
      "Loss for batch i  190 0.541322\n",
      "Loss for batch i  191 0.589634\n",
      "Loss for batch i  192 0.515829\n",
      "Loss for batch i  193 0.561074\n",
      "Loss for batch i  194 0.589379\n",
      "Loss for batch i  195 0.55484\n",
      "Loss for batch i  196 0.572597\n",
      "Loss for batch i  197 0.513445\n",
      "Loss for batch i  198 0.552815\n",
      "Loss for batch i  199 0.404685\n",
      "Loss for batch i  0 0.43758\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-0\n",
      "Loss for batch i  1 0.424257\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-1\n",
      "Loss for batch i  2 0.508012\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-2\n",
      "Loss for batch i  3 0.383796\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-3\n",
      "Loss for batch i  4 0.385013\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-4\n",
      "Loss for batch i  5 0.364141\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-5\n",
      "Loss for batch i  6 0.500289\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-6\n",
      "Loss for batch i  7 0.427082\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-7\n",
      "Loss for batch i  8 0.439685\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-8\n",
      "Loss for batch i  9 0.424539\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-9\n",
      "Loss for batch i  10 0.708591\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-10\n",
      "Loss for batch i  11 0.391857\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-11\n",
      "Loss for batch i  12 0.543503\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-12\n",
      "Loss for batch i  13 0.338841\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-13\n",
      "Loss for batch i  14 0.605453\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-14\n",
      "Loss for batch i  15 0.575264\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-15\n",
      "Loss for batch i  16 0.842826\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-16\n",
      "Loss for batch i  17 0.384268\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-17\n",
      "Loss for batch i  18 0.853304\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-18\n",
      "Loss for batch i  19 0.488733\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-19\n",
      "Loss for batch i  20 0.869714\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-20\n",
      "Loss for batch i  21 0.522739\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-21\n",
      "Loss for batch i  22 0.440265\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-22\n",
      "Loss for batch i  23 0.71729\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-23\n",
      "Loss for batch i  24 0.383374\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-24\n",
      "Loss for batch i  25 0.53802\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-25\n",
      "Loss for batch i  26 0.707052\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-26\n",
      "Loss for batch i  27 0.41368\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-27\n",
      "Loss for batch i  28 0.655203\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-28\n",
      "Loss for batch i  29 0.517395\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-29\n",
      "Loss for batch i  30 0.531636\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-30\n",
      "Loss for batch i  31 0.53195\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch i  32 0.451305\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-32\n",
      "Loss for batch i  33 0.723481\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-33\n",
      "Loss for batch i  34 0.566525\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-34\n",
      "Loss for batch i  35 0.698808\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-35\n",
      "Loss for batch i  36 0.585202\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-36\n",
      "Loss for batch i  37 0.531406\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-37\n",
      "Loss for batch i  38 0.538544\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-38\n",
      "Loss for batch i  39 0.459396\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-39\n",
      "Loss for batch i  40 0.551313\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-40\n",
      "Loss for batch i  41 0.687213\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-41\n",
      "Loss for batch i  42 0.576174\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-42\n",
      "Loss for batch i  43 0.651555\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-43\n",
      "Loss for batch i  44 0.80615\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-44\n",
      "Loss for batch i  45 0.813598\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-45\n",
      "Loss for batch i  46 0.687455\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-46\n",
      "Loss for batch i  47 0.633322\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-47\n",
      "Loss for batch i  48 0.59608\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-48\n",
      "Loss for batch i  49 0.502482\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-49\n",
      "Loss for batch i  50 0.503439\n",
      "Loss for batch i  51 0.539545\n",
      "Loss for batch i  52 0.524\n",
      "Loss for batch i  53 0.716562\n",
      "Loss for batch i  54 0.594732\n",
      "Loss for batch i  55 0.487692\n",
      "Loss for batch i  56 0.48209\n",
      "Loss for batch i  57 0.567409\n",
      "Loss for batch i  58 0.620172\n",
      "Loss for batch i  59 0.830682\n",
      "Loss for batch i  60 0.727627\n",
      "Loss for batch i  61 0.676729\n",
      "Loss for batch i  62 0.788709\n",
      "Loss for batch i  63 0.713287\n",
      "Loss for batch i  64 0.836436\n",
      "Loss for batch i  65 0.849219\n",
      "Loss for batch i  66 0.590393\n",
      "Loss for batch i  67 0.471826\n",
      "Loss for batch i  68 0.352423\n",
      "Loss for batch i  69 0.414563\n",
      "Loss for batch i  70 0.595916\n",
      "Loss for batch i  71 0.613611\n",
      "Loss for batch i  72 0.361222\n",
      "Loss for batch i  73 0.346649\n",
      "Loss for batch i  74 0.39477\n",
      "Loss for batch i  75 0.457538\n",
      "Loss for batch i  76 0.388601\n",
      "Loss for batch i  77 0.453593\n",
      "Loss for batch i  78 0.284869\n",
      "Loss for batch i  79 0.363403\n",
      "Loss for batch i  80 0.337318\n",
      "Loss for batch i  81 0.338296\n",
      "Loss for batch i  82 0.297024\n",
      "Loss for batch i  83 0.541463\n",
      "Loss for batch i  84 0.33595\n",
      "Loss for batch i  85 0.258391\n",
      "Loss for batch i  86 0.36523\n",
      "Loss for batch i  87 0.319663\n",
      "Loss for batch i  88 0.306485\n",
      "Loss for batch i  89 0.244292\n",
      "Loss for batch i  90 0.241202\n",
      "Loss for batch i  91 0.346572\n",
      "Loss for batch i  92 0.329336\n",
      "Loss for batch i  93 0.570071\n",
      "Loss for batch i  94 0.614134\n",
      "Loss for batch i  95 0.510231\n",
      "Loss for batch i  96 0.533684\n",
      "Loss for batch i  97 0.33894\n",
      "Loss for batch i  98 0.499674\n",
      "Loss for batch i  99 0.331761\n",
      "Loss for batch i  100 0.52185\n",
      "Loss for batch i  101 0.595007\n",
      "Loss for batch i  102 0.334773\n",
      "Loss for batch i  103 0.358684\n",
      "Loss for batch i  104 0.412379\n",
      "Loss for batch i  105 0.420807\n",
      "Loss for batch i  106 0.359891\n",
      "Loss for batch i  107 0.455845\n",
      "Loss for batch i  108 0.291295\n",
      "Loss for batch i  109 0.277314\n",
      "Loss for batch i  110 0.433909\n",
      "Loss for batch i  111 0.226695\n",
      "Loss for batch i  112 0.215472\n",
      "Loss for batch i  113 0.279728\n",
      "Loss for batch i  114 0.376136\n",
      "Loss for batch i  115 0.778768\n",
      "Loss for batch i  116 1.20213\n",
      "Loss for batch i  117 1.26697\n",
      "Loss for batch i  118 1.49489\n",
      "Loss for batch i  119 1.29487\n",
      "Loss for batch i  120 1.25339\n",
      "Loss for batch i  121 1.08729\n",
      "Loss for batch i  122 1.02673\n",
      "Loss for batch i  123 0.904553\n",
      "Loss for batch i  124 0.787203\n",
      "Loss for batch i  125 0.808623\n",
      "Loss for batch i  126 0.830522\n",
      "Loss for batch i  127 0.731598\n",
      "Loss for batch i  128 0.701156\n",
      "Loss for batch i  129 0.713515\n",
      "Loss for batch i  130 0.532287\n",
      "Loss for batch i  131 0.939118\n",
      "Loss for batch i  132 0.861658\n",
      "Loss for batch i  133 0.609488\n",
      "Loss for batch i  134 0.684021\n",
      "Loss for batch i  135 0.48866\n",
      "Loss for batch i  136 0.460629\n",
      "Loss for batch i  137 0.431704\n",
      "Loss for batch i  138 0.431834\n",
      "Loss for batch i  139 0.447232\n",
      "Loss for batch i  140 0.477375\n",
      "Loss for batch i  141 0.357286\n",
      "Loss for batch i  142 0.428741\n",
      "Loss for batch i  143 0.398529\n",
      "Loss for batch i  144 0.570228\n",
      "Loss for batch i  145 0.703531\n",
      "Loss for batch i  146 0.402078\n",
      "Loss for batch i  147 0.771267\n",
      "Loss for batch i  148 0.730423\n",
      "Loss for batch i  149 0.69385\n",
      "Loss for batch i  150 0.567133\n",
      "Loss for batch i  151 0.690732\n",
      "Loss for batch i  152 0.453917\n",
      "Loss for batch i  153 0.467508\n",
      "Loss for batch i  154 0.506946\n",
      "Loss for batch i  155 0.511774\n",
      "Loss for batch i  156 0.596503\n",
      "Loss for batch i  157 0.42305\n",
      "Loss for batch i  158 0.417947\n",
      "Loss for batch i  159 0.477417\n",
      "Loss for batch i  160 0.436702\n",
      "Loss for batch i  161 0.308313\n",
      "Loss for batch i  162 0.373065\n",
      "Loss for batch i  163 0.312184\n",
      "Loss for batch i  164 0.276932\n",
      "Loss for batch i  165 0.476932\n",
      "Loss for batch i  166 0.324686\n",
      "Loss for batch i  167 0.246985\n",
      "Loss for batch i  168 0.310905\n",
      "Loss for batch i  169 0.289976\n",
      "Loss for batch i  170 0.32665\n",
      "Loss for batch i  171 0.235261\n",
      "Loss for batch i  172 0.304951\n",
      "Loss for batch i  173 0.275231\n",
      "Loss for batch i  174 0.222275\n",
      "Loss for batch i  175 0.336689\n",
      "Loss for batch i  176 0.283904\n",
      "Loss for batch i  177 0.279718\n",
      "Loss for batch i  178 0.245735\n",
      "Loss for batch i  179 0.323768\n",
      "Loss for batch i  180 0.194006\n",
      "Loss for batch i  181 0.195011\n",
      "Loss for batch i  182 0.197504\n",
      "Loss for batch i  183 0.197717\n",
      "Loss for batch i  184 0.170674\n",
      "Loss for batch i  185 0.165422\n",
      "Loss for batch i  186 0.248754\n",
      "Loss for batch i  187 0.249528\n",
      "Loss for batch i  188 0.274005\n",
      "Loss for batch i  189 0.190002\n",
      "Loss for batch i  190 0.149975\n",
      "Loss for batch i  191 0.148451\n",
      "Loss for batch i  192 0.139234\n",
      "Loss for batch i  193 0.136106\n",
      "Loss for batch i  194 0.165283\n",
      "Loss for batch i  195 0.163156\n",
      "Loss for batch i  196 0.128279\n",
      "Loss for batch i  197 0.125564\n",
      "Loss for batch i  198 0.234383\n",
      "Loss for batch i  199 0.189897\n",
      "Loss for batch i  0 0.178358\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-0\n",
      "Loss for batch i  1 0.0992612\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-1\n",
      "Loss for batch i  2 0.139725\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-2\n",
      "Loss for batch i  3 0.122063\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-3\n",
      "Loss for batch i  4 0.197094\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-4\n",
      "Loss for batch i  5 0.111571\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-5\n",
      "Loss for batch i  6 0.127387\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-6\n",
      "Loss for batch i  7 0.111263\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-7\n",
      "Loss for batch i  8 0.129295\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-8\n",
      "Loss for batch i  9 0.0877346\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-9\n",
      "Loss for batch i  10 0.12268\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-10\n",
      "Loss for batch i  11 0.133434\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-11\n",
      "Loss for batch i  12 0.160931\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-12\n",
      "Loss for batch i  13 0.0863938\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-13\n",
      "Loss for batch i  14 0.196125\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-14\n",
      "Loss for batch i  15 0.256328\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-15\n",
      "Loss for batch i  16 0.189516\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-16\n",
      "Loss for batch i  17 0.104855\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-17\n",
      "Loss for batch i  18 0.16545\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-18\n",
      "Loss for batch i  19 0.165313\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-19\n",
      "Loss for batch i  20 0.252998\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-20\n",
      "Loss for batch i  21 0.197311\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-21\n",
      "Loss for batch i  22 0.146213\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch i  23 0.295386\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-23\n",
      "Loss for batch i  24 0.871983\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-24\n",
      "Loss for batch i  25 1.80537\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-25\n",
      "Loss for batch i  26 1.90762\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-26\n",
      "Loss for batch i  27 1.96885\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-27\n",
      "Loss for batch i  28 1.64414\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-28\n",
      "Loss for batch i  29 1.68101\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-29\n",
      "Loss for batch i  30 1.64719\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-30\n",
      "Loss for batch i  31 1.95112\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-31\n",
      "Loss for batch i  32 1.58137\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-32\n",
      "Loss for batch i  33 1.64166\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-33\n",
      "Loss for batch i  34 1.6354\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-34\n",
      "Loss for batch i  35 1.24916\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-35\n",
      "Loss for batch i  36 1.42715\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-36\n",
      "Loss for batch i  37 1.34444\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-37\n",
      "Loss for batch i  38 1.55795\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-38\n",
      "Loss for batch i  39 1.28154\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-39\n",
      "Loss for batch i  40 1.2334\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-40\n",
      "Loss for batch i  41 1.33354\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-41\n",
      "Loss for batch i  42 1.28901\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-42\n",
      "Loss for batch i  43 1.0514\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-43\n",
      "Loss for batch i  44 1.16037\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-44\n",
      "Loss for batch i  45 1.08593\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-45\n",
      "Loss for batch i  46 1.19745\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-46\n",
      "Loss for batch i  47 1.0741\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-47\n",
      "Loss for batch i  48 1.06649\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-48\n",
      "Loss for batch i  49 0.942602\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-49\n",
      "Loss for batch i  50 1.15044\n",
      "Loss for batch i  51 1.05373\n",
      "Loss for batch i  52 0.787483\n",
      "Loss for batch i  53 0.960888\n",
      "Loss for batch i  54 1.02072\n",
      "Loss for batch i  55 0.930808\n",
      "Loss for batch i  56 0.818838\n",
      "Loss for batch i  57 1.05052\n",
      "Loss for batch i  58 0.712446\n",
      "Loss for batch i  59 0.824264\n",
      "Loss for batch i  60 0.629457\n",
      "Loss for batch i  61 0.76111\n",
      "Loss for batch i  62 0.624714\n",
      "Loss for batch i  63 0.685357\n",
      "Loss for batch i  64 0.837495\n",
      "Loss for batch i  65 0.789619\n",
      "Loss for batch i  66 0.729079\n",
      "Loss for batch i  67 0.465511\n",
      "Loss for batch i  68 0.456805\n",
      "Loss for batch i  69 0.64387\n",
      "Loss for batch i  70 0.419987\n",
      "Loss for batch i  71 0.615332\n",
      "Loss for batch i  72 0.50729\n",
      "Loss for batch i  73 0.460594\n",
      "Loss for batch i  74 0.405656\n",
      "Loss for batch i  75 0.528484\n",
      "Loss for batch i  76 0.409179\n",
      "Loss for batch i  77 0.423469\n",
      "Loss for batch i  78 0.393055\n",
      "Loss for batch i  79 0.433205\n",
      "Loss for batch i  80 0.391708\n",
      "Loss for batch i  81 0.446426\n",
      "Loss for batch i  82 0.750692\n",
      "Loss for batch i  83 0.7758\n",
      "Loss for batch i  84 0.52012\n",
      "Loss for batch i  85 0.579756\n",
      "Loss for batch i  86 0.61522\n",
      "Loss for batch i  87 0.4837\n",
      "Loss for batch i  88 0.457509\n",
      "Loss for batch i  89 0.576841\n",
      "Loss for batch i  90 0.440412\n",
      "Loss for batch i  91 0.340198\n",
      "Loss for batch i  92 0.529682\n",
      "Loss for batch i  93 0.568686\n",
      "Loss for batch i  94 0.614585\n",
      "Loss for batch i  95 0.445957\n",
      "Loss for batch i  96 0.484582\n",
      "Loss for batch i  97 0.333841\n",
      "Loss for batch i  98 0.484293\n",
      "Loss for batch i  99 0.44558\n",
      "Loss for batch i  100 0.60461\n",
      "Loss for batch i  101 0.632813\n",
      "Loss for batch i  102 0.417784\n",
      "Loss for batch i  103 0.715819\n",
      "Loss for batch i  104 0.561607\n",
      "Loss for batch i  105 0.546302\n",
      "Loss for batch i  106 0.481331\n",
      "Loss for batch i  107 0.467547\n",
      "Loss for batch i  108 0.531051\n",
      "Loss for batch i  109 0.743831\n",
      "Loss for batch i  110 0.988554\n",
      "Loss for batch i  111 0.772874\n",
      "Loss for batch i  112 0.966187\n",
      "Loss for batch i  113 1.05018\n",
      "Loss for batch i  114 0.810257\n",
      "Loss for batch i  115 0.918083\n",
      "Loss for batch i  116 0.708011\n",
      "Loss for batch i  117 0.703397\n",
      "Loss for batch i  118 0.667499\n",
      "Loss for batch i  119 0.498642\n",
      "Loss for batch i  120 0.662491\n",
      "Loss for batch i  121 0.771577\n",
      "Loss for batch i  122 0.631196\n",
      "Loss for batch i  123 0.751878\n",
      "Loss for batch i  124 0.616682\n",
      "Loss for batch i  125 0.708435\n",
      "Loss for batch i  126 0.7172\n",
      "Loss for batch i  127 0.470465\n",
      "Loss for batch i  128 0.812589\n",
      "Loss for batch i  129 0.570243\n",
      "Loss for batch i  130 0.469939\n",
      "Loss for batch i  131 0.75233\n",
      "Loss for batch i  132 0.486194\n",
      "Loss for batch i  133 0.385248\n",
      "Loss for batch i  134 0.649119\n",
      "Loss for batch i  135 0.54001\n",
      "Loss for batch i  136 0.399962\n",
      "Loss for batch i  137 0.301329\n",
      "Loss for batch i  138 0.371879\n",
      "Loss for batch i  139 0.670399\n",
      "Loss for batch i  140 0.445644\n",
      "Loss for batch i  141 0.288123\n",
      "Loss for batch i  142 0.415314\n",
      "Loss for batch i  143 0.308752\n",
      "Loss for batch i  144 0.325078\n",
      "Loss for batch i  145 0.277026\n",
      "Loss for batch i  146 0.227681\n",
      "Loss for batch i  147 0.230488\n",
      "Loss for batch i  148 0.3169\n",
      "Loss for batch i  149 0.244106\n",
      "Loss for batch i  150 0.2\n",
      "Loss for batch i  151 0.276697\n",
      "Loss for batch i  152 0.199368\n",
      "Loss for batch i  153 0.247241\n",
      "Loss for batch i  154 0.31072\n",
      "Loss for batch i  155 0.325678\n",
      "Loss for batch i  156 0.279315\n",
      "Loss for batch i  157 0.214394\n",
      "Loss for batch i  158 0.206116\n",
      "Loss for batch i  159 0.282467\n",
      "Loss for batch i  160 0.192845\n",
      "Loss for batch i  161 0.191152\n",
      "Loss for batch i  162 0.242872\n",
      "Loss for batch i  163 0.261792\n",
      "Loss for batch i  164 0.167411\n",
      "Loss for batch i  165 0.229894\n",
      "Loss for batch i  166 0.156885\n",
      "Loss for batch i  167 0.171576\n",
      "Loss for batch i  168 0.157357\n",
      "Loss for batch i  169 0.206075\n",
      "Loss for batch i  170 0.159654\n",
      "Loss for batch i  171 0.22828\n",
      "Loss for batch i  172 0.195791\n",
      "Loss for batch i  173 0.247932\n",
      "Loss for batch i  174 0.675279\n",
      "Loss for batch i  175 1.43127\n",
      "Loss for batch i  176 1.65452\n",
      "Loss for batch i  177 1.40644\n",
      "Loss for batch i  178 1.73259\n",
      "Loss for batch i  179 1.67823\n",
      "Loss for batch i  180 1.67731\n",
      "Loss for batch i  181 1.30544\n",
      "Loss for batch i  182 1.29143\n",
      "Loss for batch i  183 1.49119\n",
      "Loss for batch i  184 1.27997\n",
      "Loss for batch i  185 1.31383\n",
      "Loss for batch i  186 1.32154\n",
      "Loss for batch i  187 1.4071\n",
      "Loss for batch i  188 1.3314\n",
      "Loss for batch i  189 1.27025\n",
      "Loss for batch i  190 1.21031\n",
      "Loss for batch i  191 1.07385\n",
      "Loss for batch i  192 1.08781\n",
      "Loss for batch i  193 1.01808\n",
      "Loss for batch i  194 1.27403\n",
      "Loss for batch i  195 1.12469\n",
      "Loss for batch i  196 1.00321\n",
      "Loss for batch i  197 0.986088\n",
      "Loss for batch i  198 0.993269\n",
      "Loss for batch i  199 0.725287\n",
      "Loss for batch i  0 0.802575\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-0\n",
      "Loss for batch i  1 0.704665\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-1\n",
      "Loss for batch i  2 0.874335\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-2\n",
      "Loss for batch i  3 0.686819\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-3\n",
      "Loss for batch i  4 0.748413\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-4\n",
      "Loss for batch i  5 0.666099\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-5\n",
      "Loss for batch i  6 0.693614\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-6\n",
      "Loss for batch i  7 0.728004\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-7\n",
      "Loss for batch i  8 0.600837\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-8\n",
      "Loss for batch i  9 0.50434\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-9\n",
      "Loss for batch i  10 0.706431\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-10\n",
      "Loss for batch i  11 0.570432\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-11\n",
      "Loss for batch i  12 0.338409\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-12\n",
      "Loss for batch i  13 0.351631\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-13\n",
      "Loss for batch i  14 0.349757\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch i  15 0.391228\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-15\n",
      "Loss for batch i  16 0.393838\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-16\n",
      "Loss for batch i  17 0.343789\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-17\n",
      "Loss for batch i  18 0.554869\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-18\n",
      "Loss for batch i  19 0.47526\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-19\n",
      "Loss for batch i  20 0.475316\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-20\n",
      "Loss for batch i  21 0.351097\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-21\n",
      "Loss for batch i  22 0.299544\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-22\n",
      "Loss for batch i  23 0.27755\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-23\n",
      "Loss for batch i  24 0.437579\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-24\n",
      "Loss for batch i  25 0.21504\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-25\n",
      "Loss for batch i  26 0.309225\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-26\n",
      "Loss for batch i  27 0.337494\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-27\n",
      "Loss for batch i  28 0.348372\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-28\n",
      "Loss for batch i  29 0.27309\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-29\n",
      "Loss for batch i  30 0.398815\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-30\n",
      "Loss for batch i  31 0.194436\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-31\n",
      "Loss for batch i  32 0.329807\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-32\n",
      "Loss for batch i  33 0.37758\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-33\n",
      "Loss for batch i  34 0.336703\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-34\n",
      "Loss for batch i  35 0.323456\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-35\n",
      "Loss for batch i  36 0.204423\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-36\n",
      "Loss for batch i  37 0.360405\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-37\n",
      "Loss for batch i  38 0.201405\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-38\n",
      "Loss for batch i  39 0.219575\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-39\n",
      "Loss for batch i  40 0.201475\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-40\n",
      "Loss for batch i  41 0.609771\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-41\n",
      "Loss for batch i  42 0.411168\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-42\n",
      "Loss for batch i  43 0.231045\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-43\n",
      "Loss for batch i  44 0.268045\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-44\n",
      "Loss for batch i  45 0.15992\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-45\n",
      "Loss for batch i  46 0.178858\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-46\n",
      "Loss for batch i  47 0.189562\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-47\n",
      "Loss for batch i  48 0.199897\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-48\n",
      "Loss for batch i  49 0.174277\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-49\n",
      "Loss for batch i  50 0.189325\n",
      "Loss for batch i  51 0.251115\n",
      "Loss for batch i  52 0.181808\n",
      "Loss for batch i  53 0.208315\n",
      "Loss for batch i  54 0.30165\n",
      "Loss for batch i  55 0.318971\n",
      "Loss for batch i  56 0.320661\n",
      "Loss for batch i  57 0.573088\n",
      "Loss for batch i  58 0.261093\n",
      "Loss for batch i  59 0.219092\n",
      "Loss for batch i  60 0.614334\n",
      "Loss for batch i  61 0.760456\n",
      "Loss for batch i  62 0.681693\n",
      "Loss for batch i  63 0.452593\n",
      "Loss for batch i  64 0.447673\n",
      "Loss for batch i  65 0.425613\n",
      "Loss for batch i  66 0.320422\n",
      "Loss for batch i  67 0.429945\n",
      "Loss for batch i  68 0.313918\n",
      "Loss for batch i  69 0.316499\n",
      "Loss for batch i  70 0.42012\n",
      "Loss for batch i  71 0.492945\n",
      "Loss for batch i  72 0.20859\n",
      "Loss for batch i  73 0.277653\n",
      "Loss for batch i  74 0.269242\n",
      "Loss for batch i  75 0.319169\n",
      "Loss for batch i  76 0.175588\n",
      "Loss for batch i  77 0.158428\n",
      "Loss for batch i  78 0.165927\n",
      "Loss for batch i  79 0.180242\n",
      "Loss for batch i  80 0.433438\n",
      "Loss for batch i  81 0.145286\n",
      "Loss for batch i  82 0.214317\n",
      "Loss for batch i  83 0.272607\n",
      "Loss for batch i  84 0.201514\n",
      "Loss for batch i  85 0.27862\n",
      "Loss for batch i  86 0.982425\n",
      "Loss for batch i  87 0.893904\n",
      "Loss for batch i  88 0.449009\n",
      "Loss for batch i  89 0.75535\n",
      "Loss for batch i  90 0.429536\n",
      "Loss for batch i  91 0.501448\n",
      "Loss for batch i  92 0.282353\n",
      "Loss for batch i  93 0.397741\n",
      "Loss for batch i  94 0.341954\n",
      "Loss for batch i  95 0.282844\n",
      "Loss for batch i  96 0.331072\n",
      "Loss for batch i  97 0.35847\n",
      "Loss for batch i  98 0.37765\n",
      "Loss for batch i  99 0.273654\n",
      "Loss for batch i  100 0.523797\n",
      "Loss for batch i  101 0.347898\n",
      "Loss for batch i  102 0.439625\n",
      "Loss for batch i  103 0.904063\n",
      "Loss for batch i  104 1.16842\n",
      "Loss for batch i  105 0.957393\n",
      "Loss for batch i  106 1.22752\n",
      "Loss for batch i  107 1.31667\n",
      "Loss for batch i  108 1.10476\n",
      "Loss for batch i  109 1.06441\n",
      "Loss for batch i  110 1.13382\n",
      "Loss for batch i  111 1.01476\n",
      "Loss for batch i  112 1.04617\n",
      "Loss for batch i  113 1.13168\n",
      "Loss for batch i  114 0.885806\n",
      "Loss for batch i  115 0.99316\n",
      "Loss for batch i  116 0.848002\n",
      "Loss for batch i  117 1.09792\n",
      "Loss for batch i  118 1.04685\n",
      "Loss for batch i  119 0.82737\n",
      "Loss for batch i  120 0.869159\n",
      "Loss for batch i  121 0.781065\n",
      "Loss for batch i  122 0.789764\n",
      "Loss for batch i  123 0.914103\n",
      "Loss for batch i  124 0.483258\n",
      "Loss for batch i  125 0.602307\n",
      "Loss for batch i  126 0.933511\n",
      "Loss for batch i  127 0.672519\n",
      "Loss for batch i  128 0.547128\n",
      "Loss for batch i  129 0.679293\n",
      "Loss for batch i  130 0.365472\n",
      "Loss for batch i  131 0.6832\n",
      "Loss for batch i  132 0.606822\n",
      "Loss for batch i  133 0.329057\n",
      "Loss for batch i  134 0.555516\n",
      "Loss for batch i  135 0.420512\n",
      "Loss for batch i  136 0.503873\n",
      "Loss for batch i  137 0.357303\n",
      "Loss for batch i  138 0.285686\n",
      "Loss for batch i  139 0.477444\n",
      "Loss for batch i  140 0.630302\n",
      "Loss for batch i  141 0.497425\n",
      "Loss for batch i  142 0.333232\n",
      "Loss for batch i  143 0.410982\n",
      "Loss for batch i  144 0.454117\n",
      "Loss for batch i  145 0.485124\n",
      "Loss for batch i  146 0.27445\n",
      "Loss for batch i  147 0.252056\n",
      "Loss for batch i  148 0.344745\n",
      "Loss for batch i  149 0.312313\n",
      "Loss for batch i  150 0.293637\n",
      "Loss for batch i  151 0.389387\n",
      "Loss for batch i  152 0.401\n",
      "Loss for batch i  153 0.337397\n",
      "Loss for batch i  154 0.404922\n",
      "Loss for batch i  155 0.242575\n",
      "Loss for batch i  156 0.207485\n",
      "Loss for batch i  157 0.370781\n",
      "Loss for batch i  158 0.235594\n",
      "Loss for batch i  159 0.215668\n",
      "Loss for batch i  160 0.211077\n",
      "Loss for batch i  161 0.19679\n",
      "Loss for batch i  162 0.287399\n",
      "Loss for batch i  163 0.187661\n",
      "Loss for batch i  164 0.270218\n",
      "Loss for batch i  165 0.201272\n",
      "Loss for batch i  166 0.12873\n",
      "Loss for batch i  167 0.183337\n",
      "Loss for batch i  168 0.164454\n",
      "Loss for batch i  169 0.192282\n",
      "Loss for batch i  170 0.204243\n",
      "Loss for batch i  171 0.162536\n",
      "Loss for batch i  172 0.237709\n",
      "Loss for batch i  173 0.374941\n",
      "Loss for batch i  174 0.266934\n",
      "Loss for batch i  175 0.582307\n",
      "Loss for batch i  176 0.871832\n",
      "Loss for batch i  177 0.499773\n",
      "Loss for batch i  178 0.595349\n",
      "Loss for batch i  179 0.700981\n",
      "Loss for batch i  180 0.34539\n",
      "Loss for batch i  181 0.423317\n",
      "Loss for batch i  182 0.332679\n",
      "Loss for batch i  183 0.302066\n",
      "Loss for batch i  184 0.362815\n",
      "Loss for batch i  185 0.318379\n",
      "Loss for batch i  186 0.289075\n",
      "Loss for batch i  187 0.275665\n",
      "Loss for batch i  188 0.352357\n",
      "Loss for batch i  189 0.481292\n",
      "Loss for batch i  190 0.200505\n",
      "Loss for batch i  191 0.169162\n",
      "Loss for batch i  192 0.186538\n",
      "Loss for batch i  193 0.232115\n",
      "Loss for batch i  194 0.364092\n",
      "Loss for batch i  195 0.285948\n",
      "Loss for batch i  196 0.20928\n",
      "Loss for batch i  197 0.222442\n",
      "Loss for batch i  198 0.231764\n",
      "Loss for batch i  199 0.117283\n",
      "Loss for batch i  0 0.163664\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-0\n",
      "Loss for batch i  1 0.129331\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-1\n",
      "Loss for batch i  2 0.290295\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-2\n",
      "Loss for batch i  3 0.25924\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-3\n",
      "Loss for batch i  4 0.126451\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-4\n",
      "Loss for batch i  5 0.174931\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch i  6 0.226697\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-6\n",
      "Loss for batch i  7 0.141185\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-7\n",
      "Loss for batch i  8 0.134168\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-8\n",
      "Loss for batch i  9 0.270625\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-9\n",
      "Loss for batch i  10 0.10435\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-10\n",
      "Loss for batch i  11 0.186507\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-11\n",
      "Loss for batch i  12 0.11472\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-12\n",
      "Loss for batch i  13 0.0853655\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-13\n",
      "Loss for batch i  14 0.166027\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-14\n",
      "Loss for batch i  15 0.100552\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-15\n",
      "Loss for batch i  16 0.186511\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-16\n",
      "Loss for batch i  17 0.0851151\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-17\n",
      "Loss for batch i  18 0.311747\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-18\n",
      "Loss for batch i  19 0.104294\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-19\n",
      "Loss for batch i  20 0.233624\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-20\n",
      "Loss for batch i  21 0.0824251\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-21\n",
      "Loss for batch i  22 0.0861119\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-22\n",
      "Loss for batch i  23 0.13115\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-23\n",
      "Loss for batch i  24 0.141768\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-24\n",
      "Loss for batch i  25 0.180573\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-25\n",
      "Loss for batch i  26 0.248582\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-26\n",
      "Loss for batch i  27 0.254686\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-27\n",
      "Loss for batch i  28 0.0859685\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-28\n",
      "Loss for batch i  29 0.102065\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-29\n",
      "Loss for batch i  30 0.0915083\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-30\n",
      "Loss for batch i  31 0.173845\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-31\n",
      "Loss for batch i  32 0.111472\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-32\n",
      "Loss for batch i  33 0.11716\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-33\n",
      "Loss for batch i  34 0.142103\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-34\n",
      "Loss for batch i  35 0.0839945\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-35\n",
      "Loss for batch i  36 0.0737445\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-36\n",
      "Loss for batch i  37 0.0734604\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-37\n",
      "Loss for batch i  38 0.214835\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-38\n",
      "Loss for batch i  39 0.0815628\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-39\n",
      "Loss for batch i  40 0.15303\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-40\n",
      "Loss for batch i  41 0.123997\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-41\n",
      "Loss for batch i  42 0.0863032\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-42\n",
      "Loss for batch i  43 0.0693888\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-43\n",
      "Loss for batch i  44 0.0828584\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-44\n",
      "Loss for batch i  45 0.076337\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-45\n",
      "Loss for batch i  46 0.0927062\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-46\n",
      "Loss for batch i  47 0.116875\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-47\n",
      "Loss for batch i  48 0.140049\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-48\n",
      "Loss for batch i  49 0.128242\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-49\n",
      "Loss for batch i  50 0.074563\n",
      "Loss for batch i  51 0.177468\n",
      "Loss for batch i  52 0.163777\n",
      "Loss for batch i  53 0.110297\n",
      "Loss for batch i  54 0.104067\n",
      "Loss for batch i  55 0.134121\n",
      "Loss for batch i  56 0.06203\n",
      "Loss for batch i  57 0.0980903\n",
      "Loss for batch i  58 0.0852105\n",
      "Loss for batch i  59 0.0734096\n",
      "Loss for batch i  60 0.0662913\n",
      "Loss for batch i  61 0.188423\n",
      "Loss for batch i  62 0.0748477\n",
      "Loss for batch i  63 0.0610479\n",
      "Loss for batch i  64 0.0849106\n",
      "Loss for batch i  65 0.0952245\n",
      "Loss for batch i  66 0.0647666\n",
      "Loss for batch i  67 0.0618535\n",
      "Loss for batch i  68 0.143592\n",
      "Loss for batch i  69 0.132017\n",
      "Loss for batch i  70 0.0633489\n",
      "Loss for batch i  71 0.0918688\n",
      "Loss for batch i  72 0.0697904\n",
      "Loss for batch i  73 0.13093\n",
      "Loss for batch i  74 0.0769217\n",
      "Loss for batch i  75 0.275052\n",
      "Loss for batch i  76 0.177506\n",
      "Loss for batch i  77 0.23887\n",
      "Loss for batch i  78 0.38334\n",
      "Loss for batch i  79 0.515428\n",
      "Loss for batch i  80 0.249199\n",
      "Loss for batch i  81 0.123298\n",
      "Loss for batch i  82 0.0759251\n",
      "Loss for batch i  83 0.192371\n",
      "Loss for batch i  84 0.139744\n",
      "Loss for batch i  85 0.203793\n",
      "Loss for batch i  86 0.292345\n",
      "Loss for batch i  87 0.143745\n",
      "Loss for batch i  88 0.0895604\n",
      "Loss for batch i  89 0.152315\n",
      "Loss for batch i  90 0.122739\n",
      "Loss for batch i  91 0.172449\n",
      "Loss for batch i  92 0.210233\n",
      "Loss for batch i  93 0.171631\n",
      "Loss for batch i  94 0.146287\n",
      "Loss for batch i  95 0.0787542\n",
      "Loss for batch i  96 0.136475\n",
      "Loss for batch i  97 0.0977461\n",
      "Loss for batch i  98 0.513632\n",
      "Loss for batch i  99 0.293978\n",
      "Loss for batch i  100 0.981495\n",
      "Loss for batch i  101 0.787997\n",
      "Loss for batch i  102 0.57043\n",
      "Loss for batch i  103 1.09886\n",
      "Loss for batch i  104 0.720431\n",
      "Loss for batch i  105 0.567435\n",
      "Loss for batch i  106 0.569882\n",
      "Loss for batch i  107 0.531031\n",
      "Loss for batch i  108 0.870457\n",
      "Loss for batch i  109 0.994743\n",
      "Loss for batch i  110 0.896338\n",
      "Loss for batch i  111 0.671477\n",
      "Loss for batch i  112 0.758696\n",
      "Loss for batch i  113 0.694751\n",
      "Loss for batch i  114 0.906558\n",
      "Loss for batch i  115 0.904827\n",
      "Loss for batch i  116 0.589044\n",
      "Loss for batch i  117 0.632594\n",
      "Loss for batch i  118 0.536279\n",
      "Loss for batch i  119 0.561675\n",
      "Loss for batch i  120 0.446233\n",
      "Loss for batch i  121 0.36007\n",
      "Loss for batch i  122 0.536562\n",
      "Loss for batch i  123 0.482337\n",
      "Loss for batch i  124 0.424063\n",
      "Loss for batch i  125 0.475871\n",
      "Loss for batch i  126 0.418828\n",
      "Loss for batch i  127 0.462418\n",
      "Loss for batch i  128 0.347407\n",
      "Loss for batch i  129 0.416427\n",
      "Loss for batch i  130 0.352515\n",
      "Loss for batch i  131 0.379739\n",
      "Loss for batch i  132 0.314424\n",
      "Loss for batch i  133 0.250148\n",
      "Loss for batch i  134 0.51866\n",
      "Loss for batch i  135 0.290882\n",
      "Loss for batch i  136 0.210148\n",
      "Loss for batch i  137 0.291887\n",
      "Loss for batch i  138 0.397144\n",
      "Loss for batch i  139 0.355357\n",
      "Loss for batch i  140 0.507775\n",
      "Loss for batch i  141 0.428572\n",
      "Loss for batch i  142 0.282349\n",
      "Loss for batch i  143 0.308996\n",
      "Loss for batch i  144 0.432068\n",
      "Loss for batch i  145 0.251938\n",
      "Loss for batch i  146 0.19668\n",
      "Loss for batch i  147 0.207346\n",
      "Loss for batch i  148 0.325431\n",
      "Loss for batch i  149 0.221742\n",
      "Loss for batch i  150 0.27855\n",
      "Loss for batch i  151 0.280292\n",
      "Loss for batch i  152 0.292726\n",
      "Loss for batch i  153 0.271466\n",
      "Loss for batch i  154 0.240378\n",
      "Loss for batch i  155 0.324422\n",
      "Loss for batch i  156 0.163081\n",
      "Loss for batch i  157 0.188762\n",
      "Loss for batch i  158 0.220111\n",
      "Loss for batch i  159 0.311205\n",
      "Loss for batch i  160 0.18199\n",
      "Loss for batch i  161 0.134928\n",
      "Loss for batch i  162 0.263816\n",
      "Loss for batch i  163 0.240807\n",
      "Loss for batch i  164 0.293452\n",
      "Loss for batch i  165 0.259957\n",
      "Loss for batch i  166 0.22738\n",
      "Loss for batch i  167 0.116106\n",
      "Loss for batch i  168 0.291222\n",
      "Loss for batch i  169 0.155816\n",
      "Loss for batch i  170 0.150058\n",
      "Loss for batch i  171 0.12745\n",
      "Loss for batch i  172 0.144463\n",
      "Loss for batch i  173 0.14764\n",
      "Loss for batch i  174 0.138943\n",
      "Loss for batch i  175 0.151915\n",
      "Loss for batch i  176 0.144671\n",
      "Loss for batch i  177 0.204169\n",
      "Loss for batch i  178 0.234579\n",
      "Loss for batch i  179 0.210047\n",
      "Loss for batch i  180 0.217507\n",
      "Loss for batch i  181 0.202159\n",
      "Loss for batch i  182 0.12341\n",
      "Loss for batch i  183 0.142737\n",
      "Loss for batch i  184 0.138952\n",
      "Loss for batch i  185 0.127877\n",
      "Loss for batch i  186 0.178934\n",
      "Loss for batch i  187 0.115031\n",
      "Loss for batch i  188 0.120033\n",
      "Loss for batch i  189 0.131378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch i  190 0.222965\n",
      "Loss for batch i  191 0.136931\n",
      "Loss for batch i  192 0.158184\n",
      "Loss for batch i  193 0.179997\n",
      "Loss for batch i  194 0.188631\n",
      "Loss for batch i  195 0.319939\n",
      "Loss for batch i  196 0.261382\n",
      "Loss for batch i  197 0.156238\n",
      "Loss for batch i  198 0.138048\n",
      "Loss for batch i  199 0.165472\n",
      "Loss for batch i  0 0.123969\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-0\n",
      "Loss for batch i  1 0.18253\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-1\n",
      "Loss for batch i  2 0.158241\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-2\n",
      "Loss for batch i  3 0.191867\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-3\n",
      "Loss for batch i  4 0.181643\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-4\n",
      "Loss for batch i  5 0.267824\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-5\n",
      "Loss for batch i  6 0.127417\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-6\n",
      "Loss for batch i  7 0.136945\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-7\n",
      "Loss for batch i  8 0.1566\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-8\n",
      "Loss for batch i  9 0.11069\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-9\n",
      "Loss for batch i  10 0.141096\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-10\n",
      "Loss for batch i  11 0.15292\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-11\n",
      "Loss for batch i  12 0.0977941\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-12\n",
      "Loss for batch i  13 0.212439\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-13\n",
      "Loss for batch i  14 0.254696\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-14\n",
      "Loss for batch i  15 0.0935395\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-15\n",
      "Loss for batch i  16 0.216299\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-16\n",
      "Loss for batch i  17 0.111198\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-17\n",
      "Loss for batch i  18 0.160151\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-18\n",
      "Loss for batch i  19 0.218308\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-19\n",
      "Loss for batch i  20 0.213575\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-20\n",
      "Loss for batch i  21 0.094509\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-21\n",
      "Loss for batch i  22 0.0924083\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-22\n",
      "Loss for batch i  23 0.113845\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-23\n",
      "Loss for batch i  24 0.075101\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-24\n",
      "Loss for batch i  25 0.082431\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-25\n",
      "Loss for batch i  26 0.091897\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-26\n",
      "Loss for batch i  27 0.105718\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-27\n",
      "Loss for batch i  28 0.118082\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-28\n",
      "Loss for batch i  29 0.077547\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-29\n",
      "Loss for batch i  30 0.0744291\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-30\n",
      "Loss for batch i  31 0.147563\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-31\n",
      "Loss for batch i  32 0.154873\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-32\n",
      "Loss for batch i  33 0.103559\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-33\n",
      "Loss for batch i  34 0.0795631\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-34\n",
      "Loss for batch i  35 0.194897\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-35\n",
      "Loss for batch i  36 0.147403\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-36\n",
      "Loss for batch i  37 0.149433\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-37\n",
      "Loss for batch i  38 0.129501\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-38\n",
      "Loss for batch i  39 0.0991449\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-39\n",
      "Loss for batch i  40 0.115423\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-40\n",
      "Loss for batch i  41 0.106507\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-41\n",
      "Loss for batch i  42 0.0909286\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-42\n",
      "Loss for batch i  43 0.0785979\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-43\n",
      "Loss for batch i  44 0.0870192\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-44\n",
      "Loss for batch i  45 0.100505\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-45\n",
      "Loss for batch i  46 0.0923458\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-46\n",
      "Loss for batch i  47 0.0831467\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-47\n",
      "Loss for batch i  48 0.0684052\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-48\n",
      "Loss for batch i  49 0.0643593\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-49\n",
      "Loss for batch i  50 0.0759828\n",
      "Loss for batch i  51 0.149793\n",
      "Loss for batch i  52 0.059924\n",
      "Loss for batch i  53 0.0547837\n",
      "Loss for batch i  54 0.0691848\n",
      "Loss for batch i  55 0.176279\n",
      "Loss for batch i  56 0.0733552\n",
      "Loss for batch i  57 0.0696812\n",
      "Loss for batch i  58 0.0627783\n",
      "Loss for batch i  59 0.098852\n",
      "Loss for batch i  60 0.0487525\n",
      "Loss for batch i  61 0.107483\n",
      "Loss for batch i  62 0.0913808\n",
      "Loss for batch i  63 0.0572092\n",
      "Loss for batch i  64 0.060489\n",
      "Loss for batch i  65 0.105681\n",
      "Loss for batch i  66 0.0590609\n",
      "Loss for batch i  67 0.0559426\n",
      "Loss for batch i  68 0.0425665\n",
      "Loss for batch i  69 0.0475542\n",
      "Loss for batch i  70 0.0535878\n",
      "Loss for batch i  71 0.0733892\n",
      "Loss for batch i  72 0.0482422\n",
      "Loss for batch i  73 0.0679448\n",
      "Loss for batch i  74 0.0641324\n",
      "Loss for batch i  75 0.0546751\n",
      "Loss for batch i  76 0.0417167\n",
      "Loss for batch i  77 0.0431752\n",
      "Loss for batch i  78 0.166193\n",
      "Loss for batch i  79 0.0594232\n",
      "Loss for batch i  80 0.0635678\n",
      "Loss for batch i  81 0.0423491\n",
      "Loss for batch i  82 0.0449511\n",
      "Loss for batch i  83 0.0549614\n",
      "Loss for batch i  84 0.0416713\n",
      "Loss for batch i  85 0.0383904\n",
      "Loss for batch i  86 0.261996\n",
      "Loss for batch i  87 0.0515147\n",
      "Loss for batch i  88 0.158626\n",
      "Loss for batch i  89 0.112298\n",
      "Loss for batch i  90 0.0409569\n",
      "Loss for batch i  91 0.0351134\n",
      "Loss for batch i  92 0.0423902\n",
      "Loss for batch i  93 0.107197\n",
      "Loss for batch i  94 0.0847974\n",
      "Loss for batch i  95 0.0490842\n",
      "Loss for batch i  96 0.0383095\n",
      "Loss for batch i  97 0.0391658\n",
      "Loss for batch i  98 0.0446811\n",
      "Loss for batch i  99 0.0384297\n",
      "Loss for batch i  100 0.133928\n",
      "Loss for batch i  101 0.0625901\n",
      "Loss for batch i  102 0.0400055\n",
      "Loss for batch i  103 0.0448792\n",
      "Loss for batch i  104 0.0420264\n",
      "Loss for batch i  105 0.0471919\n",
      "Loss for batch i  106 0.0474115\n",
      "Loss for batch i  107 0.0675883\n",
      "Loss for batch i  108 0.0449565\n",
      "Loss for batch i  109 0.0503369\n",
      "Loss for batch i  110 0.0400481\n",
      "Loss for batch i  111 0.0425942\n",
      "Loss for batch i  112 0.0383226\n",
      "Loss for batch i  113 0.0510146\n",
      "Loss for batch i  114 0.038144\n",
      "Loss for batch i  115 0.0432552\n",
      "Loss for batch i  116 0.122413\n",
      "Loss for batch i  117 0.211968\n",
      "Loss for batch i  118 0.0398243\n",
      "Loss for batch i  119 0.0441103\n",
      "Loss for batch i  120 0.422617\n",
      "Loss for batch i  121 0.107495\n",
      "Loss for batch i  122 0.0642215\n",
      "Loss for batch i  123 0.0602174\n",
      "Loss for batch i  124 0.0911124\n",
      "Loss for batch i  125 0.175572\n",
      "Loss for batch i  126 0.114165\n",
      "Loss for batch i  127 0.0731217\n",
      "Loss for batch i  128 0.108559\n",
      "Loss for batch i  129 0.147054\n",
      "Loss for batch i  130 0.0355858\n",
      "Loss for batch i  131 0.066484\n",
      "Loss for batch i  132 0.172422\n",
      "Loss for batch i  133 0.0774351\n",
      "Loss for batch i  134 0.145613\n",
      "Loss for batch i  135 0.243525\n",
      "Loss for batch i  136 0.235709\n",
      "Loss for batch i  137 0.0703456\n",
      "Loss for batch i  138 0.124446\n",
      "Loss for batch i  139 0.205972\n",
      "Loss for batch i  140 0.332836\n",
      "Loss for batch i  141 0.45215\n",
      "Loss for batch i  142 0.770404\n",
      "Loss for batch i  143 0.8783\n",
      "Loss for batch i  144 0.97859\n",
      "Loss for batch i  145 1.05234\n",
      "Loss for batch i  146 0.615333\n",
      "Loss for batch i  147 1.78962\n",
      "Loss for batch i  148 3.07307\n",
      "Loss for batch i  149 3.14242\n",
      "Loss for batch i  150 2.69943\n",
      "Loss for batch i  151 3.31196\n",
      "Loss for batch i  152 2.76862\n",
      "Loss for batch i  153 2.91228\n",
      "Loss for batch i  154 2.65835\n",
      "Loss for batch i  155 2.3408\n",
      "Loss for batch i  156 2.23685\n",
      "Loss for batch i  157 2.24998\n",
      "Loss for batch i  158 2.13464\n",
      "Loss for batch i  159 2.11566\n",
      "Loss for batch i  160 1.96417\n",
      "Loss for batch i  161 1.82467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch i  162 1.80894\n",
      "Loss for batch i  163 1.67258\n",
      "Loss for batch i  164 1.6931\n",
      "Loss for batch i  165 1.86799\n",
      "Loss for batch i  166 1.68497\n",
      "Loss for batch i  167 1.58075\n",
      "Loss for batch i  168 1.82724\n",
      "Loss for batch i  169 1.7781\n",
      "Loss for batch i  170 1.71103\n",
      "Loss for batch i  171 1.48429\n",
      "Loss for batch i  172 1.68307\n",
      "Loss for batch i  173 1.55176\n",
      "Loss for batch i  174 1.5709\n",
      "Loss for batch i  175 1.71404\n",
      "Loss for batch i  176 1.64869\n",
      "Loss for batch i  177 1.50822\n",
      "Loss for batch i  178 1.61257\n",
      "Loss for batch i  179 1.69234\n",
      "Loss for batch i  180 1.62855\n",
      "Loss for batch i  181 1.451\n",
      "Loss for batch i  182 1.49495\n",
      "Loss for batch i  183 1.5765\n",
      "Loss for batch i  184 1.48377\n",
      "Loss for batch i  185 1.51422\n",
      "Loss for batch i  186 1.44632\n",
      "Loss for batch i  187 1.4897\n",
      "Loss for batch i  188 1.54717\n",
      "Loss for batch i  189 1.5207\n",
      "Loss for batch i  190 1.43792\n",
      "Loss for batch i  191 1.37792\n",
      "Loss for batch i  192 1.41632\n",
      "Loss for batch i  193 1.40769\n",
      "Loss for batch i  194 1.54463\n",
      "Loss for batch i  195 1.44381\n",
      "Loss for batch i  196 1.35233\n",
      "Loss for batch i  197 1.27725\n",
      "Loss for batch i  198 1.39872\n",
      "Loss for batch i  199 1.10624\n",
      "Loss for batch i  0 1.22819\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-0\n",
      "Loss for batch i  1 1.21685\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-1\n",
      "Loss for batch i  2 1.40281\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-2\n",
      "Loss for batch i  3 1.22446\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-3\n",
      "Loss for batch i  4 1.24225\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-4\n",
      "Loss for batch i  5 1.13082\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-5\n",
      "Loss for batch i  6 1.30883\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-6\n",
      "Loss for batch i  7 1.31669\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-7\n",
      "Loss for batch i  8 1.15388\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-8\n",
      "Loss for batch i  9 1.14321\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-9\n",
      "Loss for batch i  10 1.18814\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-10\n",
      "Loss for batch i  11 1.10655\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-11\n",
      "Loss for batch i  12 1.04335\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-12\n",
      "Loss for batch i  13 1.00836\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-13\n",
      "Loss for batch i  14 1.23503\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-14\n",
      "Loss for batch i  15 1.03442\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-15\n",
      "Loss for batch i  16 1.12395\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-16\n",
      "Loss for batch i  17 0.955666\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-17\n",
      "Loss for batch i  18 1.11344\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-18\n",
      "Loss for batch i  19 1.10874\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-19\n",
      "Loss for batch i  20 1.00915\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-20\n",
      "Loss for batch i  21 1.04561\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-21\n",
      "Loss for batch i  22 0.999984\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-22\n",
      "Loss for batch i  23 1.0905\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-23\n",
      "Loss for batch i  24 0.985968\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-24\n",
      "Loss for batch i  25 0.799029\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-25\n",
      "Loss for batch i  26 0.98429\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-26\n",
      "Loss for batch i  27 0.971067\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-27\n",
      "Loss for batch i  28 0.785277\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-28\n",
      "Loss for batch i  29 0.969691\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-29\n",
      "Loss for batch i  30 0.821862\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-30\n",
      "Loss for batch i  31 1.06574\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-31\n",
      "Loss for batch i  32 0.766565\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-32\n",
      "Loss for batch i  33 0.872654\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-33\n",
      "Loss for batch i  34 0.918775\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-34\n",
      "Loss for batch i  35 0.657771\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-35\n",
      "Loss for batch i  36 0.716364\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-36\n",
      "Loss for batch i  37 0.703302\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-37\n",
      "Loss for batch i  38 0.776317\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-38\n",
      "Loss for batch i  39 0.63945\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-39\n",
      "Loss for batch i  40 0.51004\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-40\n",
      "Loss for batch i  41 0.743162\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-41\n",
      "Loss for batch i  42 0.6812\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-42\n",
      "Loss for batch i  43 0.69085\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-43\n",
      "Loss for batch i  44 0.882802\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-44\n",
      "Loss for batch i  45 0.741705\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-45\n",
      "Loss for batch i  46 0.702775\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-46\n",
      "Loss for batch i  47 0.822874\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-47\n",
      "Loss for batch i  48 0.658558\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-48\n",
      "Loss for batch i  49 0.839303\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-49\n",
      "Loss for batch i  50 0.874529\n",
      "Loss for batch i  51 0.727759\n",
      "Loss for batch i  52 0.795445\n",
      "Loss for batch i  53 0.786148\n",
      "Loss for batch i  54 0.991683\n",
      "Loss for batch i  55 0.982038\n",
      "Loss for batch i  56 0.676435\n",
      "Loss for batch i  57 0.821853\n",
      "Loss for batch i  58 0.914832\n",
      "Loss for batch i  59 1.4645\n",
      "Loss for batch i  60 1.21346\n",
      "Loss for batch i  61 1.27926\n",
      "Loss for batch i  62 1.1807\n",
      "Loss for batch i  63 1.06632\n",
      "Loss for batch i  64 1.16614\n",
      "Loss for batch i  65 1.31127\n",
      "Loss for batch i  66 1.02487\n",
      "Loss for batch i  67 0.902989\n",
      "Loss for batch i  68 0.77834\n",
      "Loss for batch i  69 0.844909\n",
      "Loss for batch i  70 0.794728\n",
      "Loss for batch i  71 1.12772\n",
      "Loss for batch i  72 0.77467\n",
      "Loss for batch i  73 0.72305\n",
      "Loss for batch i  74 0.679705\n",
      "Loss for batch i  75 0.803643\n",
      "Loss for batch i  76 0.747986\n",
      "Loss for batch i  77 1.14322\n",
      "Loss for batch i  78 0.814141\n",
      "Loss for batch i  79 1.69335\n",
      "Loss for batch i  80 2.10932\n",
      "Loss for batch i  81 1.76465\n",
      "Loss for batch i  82 1.72889\n",
      "Loss for batch i  83 2.17944\n",
      "Loss for batch i  84 1.70002\n",
      "Loss for batch i  85 1.66033\n",
      "Loss for batch i  86 2.00529\n",
      "Loss for batch i  87 1.71433\n",
      "Loss for batch i  88 1.76083\n",
      "Loss for batch i  89 1.6156\n",
      "Loss for batch i  90 1.37867\n",
      "Loss for batch i  91 1.35384\n",
      "Loss for batch i  92 1.58859\n",
      "Loss for batch i  93 1.68317\n",
      "Loss for batch i  94 1.67835\n",
      "Loss for batch i  95 1.59173\n",
      "Loss for batch i  96 1.6971\n",
      "Loss for batch i  97 1.45167\n",
      "Loss for batch i  98 1.63406\n",
      "Loss for batch i  99 1.49043\n",
      "Loss for batch i  100 1.65726\n",
      "Loss for batch i  101 1.63689\n",
      "Loss for batch i  102 1.15346\n",
      "Loss for batch i  103 1.55529\n",
      "Loss for batch i  104 1.61388\n",
      "Loss for batch i  105 1.48086\n",
      "Loss for batch i  106 1.44338\n",
      "Loss for batch i  107 1.40669\n",
      "Loss for batch i  108 1.3089\n",
      "Loss for batch i  109 1.33087\n",
      "Loss for batch i  110 1.39545\n",
      "Loss for batch i  111 1.33382\n",
      "Loss for batch i  112 1.33785\n",
      "Loss for batch i  113 1.30333\n",
      "Loss for batch i  114 1.16677\n",
      "Loss for batch i  115 1.31114\n",
      "Loss for batch i  116 1.16708\n",
      "Loss for batch i  117 1.43821\n",
      "Loss for batch i  118 1.33843\n",
      "Loss for batch i  119 1.16337\n",
      "Loss for batch i  120 1.28906\n",
      "Loss for batch i  121 1.20719\n",
      "Loss for batch i  122 1.26547\n",
      "Loss for batch i  123 1.29756\n",
      "Loss for batch i  124 1.10308\n",
      "Loss for batch i  125 1.11491\n",
      "Loss for batch i  126 1.2199\n",
      "Loss for batch i  127 0.947682\n",
      "Loss for batch i  128 1.03842\n",
      "Loss for batch i  129 1.06004\n",
      "Loss for batch i  130 0.973828\n",
      "Loss for batch i  131 1.1967\n",
      "Loss for batch i  132 1.11905\n",
      "Loss for batch i  133 0.955115\n",
      "Loss for batch i  134 1.14694\n",
      "Loss for batch i  135 0.921703\n",
      "Loss for batch i  136 0.973793\n",
      "Loss for batch i  137 0.838758\n",
      "Loss for batch i  138 0.837064\n",
      "Loss for batch i  139 0.912853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch i  140 0.988064\n",
      "Loss for batch i  141 0.771184\n",
      "Loss for batch i  142 0.930303\n",
      "Loss for batch i  143 0.746155\n",
      "Loss for batch i  144 0.705879\n",
      "Loss for batch i  145 0.681532\n",
      "Loss for batch i  146 0.527559\n",
      "Loss for batch i  147 0.71433\n",
      "Loss for batch i  148 0.853947\n",
      "Loss for batch i  149 0.874797\n",
      "Loss for batch i  150 0.586718\n",
      "Loss for batch i  151 0.813416\n",
      "Loss for batch i  152 0.684669\n",
      "Loss for batch i  153 0.695658\n",
      "Loss for batch i  154 0.75064\n",
      "Loss for batch i  155 0.696395\n",
      "Loss for batch i  156 0.71141\n",
      "Loss for batch i  157 0.764625\n",
      "Loss for batch i  158 0.671175\n",
      "Loss for batch i  159 0.668986\n",
      "Loss for batch i  160 0.72798\n",
      "Loss for batch i  161 0.578185\n",
      "Loss for batch i  162 0.617619\n",
      "Loss for batch i  163 0.494473\n",
      "Loss for batch i  164 0.549783\n",
      "Loss for batch i  165 0.549418\n",
      "Loss for batch i  166 0.481201\n",
      "Loss for batch i  167 0.448133\n",
      "Loss for batch i  168 0.628578\n",
      "Loss for batch i  169 0.634408\n",
      "Loss for batch i  170 0.902839\n",
      "Loss for batch i  171 0.5772\n",
      "Loss for batch i  172 0.715127\n",
      "Loss for batch i  173 0.707908\n",
      "Loss for batch i  174 0.713986\n",
      "Loss for batch i  175 0.815121\n",
      "Loss for batch i  176 0.704185\n",
      "Loss for batch i  177 0.544949\n",
      "Loss for batch i  178 0.589851\n",
      "Loss for batch i  179 0.668746\n",
      "Loss for batch i  180 0.857186\n",
      "Loss for batch i  181 0.508823\n",
      "Loss for batch i  182 0.534178\n",
      "Loss for batch i  183 0.546395\n",
      "Loss for batch i  184 0.367315\n",
      "Loss for batch i  185 0.39744\n",
      "Loss for batch i  186 0.57601\n",
      "Loss for batch i  187 1.57982\n",
      "Loss for batch i  188 2.17961\n",
      "Loss for batch i  189 2.21737\n",
      "Loss for batch i  190 2.08985\n",
      "Loss for batch i  191 1.92083\n",
      "Loss for batch i  192 1.97987\n",
      "Loss for batch i  193 1.93651\n",
      "Loss for batch i  194 2.09401\n",
      "Loss for batch i  195 1.99683\n",
      "Loss for batch i  196 1.91255\n",
      "Loss for batch i  197 1.76619\n",
      "Loss for batch i  198 1.90154\n",
      "Loss for batch i  199 1.58642\n",
      "Loss for batch i  0 1.70646\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-0\n",
      "Loss for batch i  1 1.63193\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-1\n",
      "Loss for batch i  2 1.7796\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-2\n",
      "Loss for batch i  3 1.57016\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-3\n",
      "Loss for batch i  4 1.7002\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-4\n",
      "Loss for batch i  5 1.53253\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-5\n",
      "Loss for batch i  6 1.70482\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-6\n",
      "Loss for batch i  7 1.70634\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-7\n",
      "Loss for batch i  8 1.54888\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-8\n",
      "Loss for batch i  9 1.50516\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-9\n",
      "Loss for batch i  10 1.57863\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-10\n",
      "Loss for batch i  11 1.51234\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-11\n",
      "Loss for batch i  12 1.36639\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-12\n",
      "Loss for batch i  13 1.37748\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-13\n",
      "Loss for batch i  14 1.56142\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-14\n",
      "Loss for batch i  15 1.41129\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-15\n",
      "Loss for batch i  16 1.53908\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-16\n",
      "Loss for batch i  17 1.42943\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-17\n",
      "Loss for batch i  18 1.53252\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-18\n",
      "Loss for batch i  19 1.56033\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-19\n",
      "Loss for batch i  20 1.45007\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-20\n",
      "Loss for batch i  21 1.46166\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-21\n",
      "Loss for batch i  22 1.46548\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-22\n",
      "Loss for batch i  23 1.6017\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-23\n",
      "Loss for batch i  24 1.42711\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-24\n",
      "Loss for batch i  25 1.36465\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-25\n",
      "Loss for batch i  26 1.46594\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-26\n",
      "Loss for batch i  27 1.50147\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-27\n",
      "Loss for batch i  28 1.30832\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-28\n",
      "Loss for batch i  29 1.39695\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-29\n",
      "Loss for batch i  30 1.30563\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-30\n",
      "Loss for batch i  31 1.48194\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-31\n",
      "Loss for batch i  32 1.23069\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-32\n",
      "Loss for batch i  33 1.49706\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-33\n",
      "Loss for batch i  34 1.37458\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-34\n",
      "Loss for batch i  35 1.18342\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-35\n",
      "Loss for batch i  36 1.32464\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-36\n",
      "Loss for batch i  37 1.29714\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-37\n",
      "Loss for batch i  38 1.41312\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-38\n",
      "Loss for batch i  39 1.19562\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-39\n",
      "Loss for batch i  40 1.17504\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-40\n",
      "Loss for batch i  41 1.27588\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-41\n",
      "Loss for batch i  42 1.31434\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-42\n",
      "Loss for batch i  43 1.0654\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-43\n",
      "Loss for batch i  44 1.1812\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-44\n",
      "Loss for batch i  45 1.08868\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-45\n",
      "Loss for batch i  46 1.20101\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-46\n",
      "Loss for batch i  47 1.1696\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-47\n",
      "Loss for batch i  48 1.13506\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-48\n",
      "Loss for batch i  49 1.13415\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-49\n",
      "Loss for batch i  50 1.30115\n",
      "Loss for batch i  51 1.19509\n",
      "Loss for batch i  52 1.04372\n",
      "Loss for batch i  53 1.24797\n",
      "Loss for batch i  54 1.26328\n",
      "Loss for batch i  55 1.14797\n",
      "Loss for batch i  56 1.1621\n",
      "Loss for batch i  57 1.22123\n",
      "Loss for batch i  58 1.04164\n",
      "Loss for batch i  59 1.09234\n",
      "Loss for batch i  60 0.948564\n",
      "Loss for batch i  61 1.13848\n",
      "Loss for batch i  62 1.1385\n",
      "Loss for batch i  63 0.970777\n",
      "Loss for batch i  64 1.12284\n",
      "Loss for batch i  65 1.19183\n",
      "Loss for batch i  66 0.961003\n",
      "Loss for batch i  67 0.929203\n",
      "Loss for batch i  68 0.86112\n",
      "Loss for batch i  69 0.875737\n",
      "Loss for batch i  70 0.944804\n",
      "Loss for batch i  71 1.16961\n",
      "Loss for batch i  72 0.890694\n",
      "Loss for batch i  73 0.790825\n",
      "Loss for batch i  74 0.8876\n",
      "Loss for batch i  75 0.909361\n",
      "Loss for batch i  76 0.75268\n",
      "Loss for batch i  77 0.847829\n",
      "Loss for batch i  78 0.736083\n",
      "Loss for batch i  79 0.871341\n",
      "Loss for batch i  80 0.726361\n",
      "Loss for batch i  81 0.530076\n",
      "Loss for batch i  82 0.560207\n",
      "Loss for batch i  83 0.803312\n",
      "Loss for batch i  84 0.519625\n",
      "Loss for batch i  85 0.53363\n",
      "Loss for batch i  86 0.68229\n",
      "Loss for batch i  87 0.537633\n",
      "Loss for batch i  88 0.509604\n",
      "Loss for batch i  89 0.547693\n",
      "Loss for batch i  90 0.368035\n",
      "Loss for batch i  91 0.405471\n",
      "Loss for batch i  92 0.43889\n",
      "Loss for batch i  93 0.454475\n",
      "Loss for batch i  94 0.449115\n",
      "Loss for batch i  95 0.497246\n",
      "Loss for batch i  96 0.42768\n",
      "Loss for batch i  97 0.367587\n",
      "Loss for batch i  98 0.467407\n",
      "Loss for batch i  99 0.326408\n",
      "Loss for batch i  100 0.547012\n",
      "Loss for batch i  101 0.601572\n",
      "Loss for batch i  102 0.451575\n",
      "Loss for batch i  103 0.634098\n",
      "Loss for batch i  104 0.80521\n",
      "Loss for batch i  105 0.695205\n",
      "Loss for batch i  106 0.75531\n",
      "Loss for batch i  107 0.59638\n",
      "Loss for batch i  108 0.474839\n",
      "Loss for batch i  109 0.8692\n",
      "Loss for batch i  110 0.711034\n",
      "Loss for batch i  111 0.418352\n",
      "Loss for batch i  112 0.542905\n",
      "Loss for batch i  113 0.631766\n",
      "Loss for batch i  114 0.465775\n",
      "Loss for batch i  115 0.698804\n",
      "Loss for batch i  116 1.01528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch i  117 1.51403\n",
      "Loss for batch i  118 1.31952\n",
      "Loss for batch i  119 1.32127\n",
      "Loss for batch i  120 1.45715\n",
      "Loss for batch i  121 1.89308\n",
      "Loss for batch i  122 1.8875\n",
      "Loss for batch i  123 1.83988\n",
      "Loss for batch i  124 1.61441\n",
      "Loss for batch i  125 1.59152\n",
      "Loss for batch i  126 1.71521\n",
      "Loss for batch i  127 1.51207\n",
      "Loss for batch i  128 1.52619\n",
      "Loss for batch i  129 1.5406\n",
      "Loss for batch i  130 1.47482\n",
      "Loss for batch i  131 1.67007\n",
      "Loss for batch i  132 1.59223\n",
      "Loss for batch i  133 1.46213\n",
      "Loss for batch i  134 1.66033\n",
      "Loss for batch i  135 1.31487\n",
      "Loss for batch i  136 1.38481\n",
      "Loss for batch i  137 1.32641\n",
      "Loss for batch i  138 1.28107\n",
      "Loss for batch i  139 1.51851\n",
      "Loss for batch i  140 1.36184\n",
      "Loss for batch i  141 1.24781\n",
      "Loss for batch i  142 1.35337\n",
      "Loss for batch i  143 1.19212\n",
      "Loss for batch i  144 1.20444\n",
      "Loss for batch i  145 1.14726\n",
      "Loss for batch i  146 1.10793\n",
      "Loss for batch i  147 1.15294\n",
      "Loss for batch i  148 1.34395\n",
      "Loss for batch i  149 1.22777\n",
      "Loss for batch i  150 0.990136\n",
      "Loss for batch i  151 1.32653\n",
      "Loss for batch i  152 1.0552\n",
      "Loss for batch i  153 1.16252\n",
      "Loss for batch i  154 1.09607\n",
      "Loss for batch i  155 1.01817\n",
      "Loss for batch i  156 0.993553\n",
      "Loss for batch i  157 1.02414\n",
      "Loss for batch i  158 1.03437\n",
      "Loss for batch i  159 1.0503\n",
      "Loss for batch i  160 0.969578\n",
      "Loss for batch i  161 0.852128\n",
      "Loss for batch i  162 0.896767\n",
      "Loss for batch i  163 0.78484\n",
      "Loss for batch i  164 0.768868\n",
      "Loss for batch i  165 0.890221\n",
      "Loss for batch i  166 0.719811\n",
      "Loss for batch i  167 0.739058\n",
      "Loss for batch i  168 0.877036\n",
      "Loss for batch i  169 0.923405\n",
      "Loss for batch i  170 0.82125\n",
      "Loss for batch i  171 0.61018\n",
      "Loss for batch i  172 0.799926\n",
      "Loss for batch i  173 0.604173\n",
      "Loss for batch i  174 0.646279\n",
      "Loss for batch i  175 0.837977\n",
      "Loss for batch i  176 0.756569\n",
      "Loss for batch i  177 0.559856\n",
      "Loss for batch i  178 0.687039\n",
      "Loss for batch i  179 0.636212\n",
      "Loss for batch i  180 0.685375\n",
      "Loss for batch i  181 0.610714\n",
      "Loss for batch i  182 0.578879\n",
      "Loss for batch i  183 0.463994\n",
      "Loss for batch i  184 0.62939\n",
      "Loss for batch i  185 0.456246\n",
      "Loss for batch i  186 0.641948\n",
      "Loss for batch i  187 0.509637\n",
      "Loss for batch i  188 0.704654\n",
      "Loss for batch i  189 0.854147\n",
      "Loss for batch i  190 0.799952\n",
      "Loss for batch i  191 1.08573\n",
      "Loss for batch i  192 1.30936\n",
      "Loss for batch i  193 1.09113\n",
      "Loss for batch i  194 1.35342\n",
      "Loss for batch i  195 1.45692\n",
      "Loss for batch i  196 1.19916\n",
      "Loss for batch i  197 1.17084\n",
      "Loss for batch i  198 1.40058\n",
      "Loss for batch i  199 1.01773\n",
      "Loss for batch i  0 1.12426\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-0\n",
      "Loss for batch i  1 1.11251\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-1\n",
      "Loss for batch i  2 1.21675\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-2\n",
      "Loss for batch i  3 0.966455\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-3\n",
      "Loss for batch i  4 0.991099\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-4\n",
      "Loss for batch i  5 0.902775\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-5\n",
      "Loss for batch i  6 1.14438\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-6\n",
      "Loss for batch i  7 1.24627\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-7\n",
      "Loss for batch i  8 1.03354\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-8\n",
      "Loss for batch i  9 0.816517\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-9\n",
      "Loss for batch i  10 1.02107\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-10\n",
      "Loss for batch i  11 0.886458\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-11\n",
      "Loss for batch i  12 0.822361\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-12\n",
      "Loss for batch i  13 0.774516\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-13\n",
      "Loss for batch i  14 0.884731\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-14\n",
      "Loss for batch i  15 0.803341\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-15\n",
      "Loss for batch i  16 0.787031\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-16\n",
      "Loss for batch i  17 0.676194\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-17\n",
      "Loss for batch i  18 0.693323\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-18\n",
      "Loss for batch i  19 0.676166\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-19\n",
      "Loss for batch i  20 0.643008\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-20\n",
      "Loss for batch i  21 0.527638\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-21\n",
      "Loss for batch i  22 0.741468\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-22\n",
      "Loss for batch i  23 0.617726\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-23\n",
      "Loss for batch i  24 0.5559\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-24\n",
      "Loss for batch i  25 0.521522\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-25\n",
      "Loss for batch i  26 0.570985\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-26\n",
      "Loss for batch i  27 0.51276\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-27\n",
      "Loss for batch i  28 0.414995\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-28\n",
      "Loss for batch i  29 0.49687\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-29\n",
      "Loss for batch i  30 0.503361\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-30\n",
      "Loss for batch i  31 0.4924\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-31\n",
      "Loss for batch i  32 0.431571\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-32\n",
      "Loss for batch i  33 0.601417\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-33\n",
      "Loss for batch i  34 0.596059\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-34\n",
      "Loss for batch i  35 0.484271\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-35\n",
      "Loss for batch i  36 0.670321\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-36\n",
      "Loss for batch i  37 0.783047\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-37\n",
      "Loss for batch i  38 0.866629\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-38\n",
      "Loss for batch i  39 0.809334\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-39\n",
      "Loss for batch i  40 0.621205\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-40\n",
      "Loss for batch i  41 0.64183\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-41\n",
      "Loss for batch i  42 0.79536\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-42\n",
      "Loss for batch i  43 0.740149\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-43\n",
      "Loss for batch i  44 0.569736\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-44\n",
      "Loss for batch i  45 0.709278\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-45\n",
      "Loss for batch i  46 0.78453\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-46\n",
      "Loss for batch i  47 0.521806\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-47\n",
      "Loss for batch i  48 0.640365\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-48\n",
      "Loss for batch i  49 0.702182\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-49\n",
      "Loss for batch i  50 0.710837\n",
      "Loss for batch i  51 0.686185\n",
      "Loss for batch i  52 0.505115\n",
      "Loss for batch i  53 0.726874\n",
      "Loss for batch i  54 0.661226\n",
      "Loss for batch i  55 0.778137\n",
      "Loss for batch i  56 0.665636\n",
      "Loss for batch i  57 0.707471\n",
      "Loss for batch i  58 0.650578\n",
      "Loss for batch i  59 0.599662\n",
      "Loss for batch i  60 0.653677\n",
      "Loss for batch i  61 0.782469\n",
      "Loss for batch i  62 0.818969\n",
      "Loss for batch i  63 0.654486\n",
      "Loss for batch i  64 0.921829\n",
      "Loss for batch i  65 1.54843\n",
      "Loss for batch i  66 1.0267\n",
      "Loss for batch i  67 0.686647\n",
      "Loss for batch i  68 0.676788\n",
      "Loss for batch i  69 0.958356\n",
      "Loss for batch i  70 0.940933\n",
      "Loss for batch i  71 1.20856\n",
      "Loss for batch i  72 0.748779\n",
      "Loss for batch i  73 0.721456\n",
      "Loss for batch i  74 0.811735\n",
      "Loss for batch i  75 0.86173\n",
      "Loss for batch i  76 0.819879\n",
      "Loss for batch i  77 0.797859\n",
      "Loss for batch i  78 0.646186\n",
      "Loss for batch i  79 0.785195\n",
      "Loss for batch i  80 0.826575\n",
      "Loss for batch i  81 0.669017\n",
      "Loss for batch i  82 0.696039\n",
      "Loss for batch i  83 0.940895\n",
      "Loss for batch i  84 0.653381\n",
      "Loss for batch i  85 0.592822\n",
      "Loss for batch i  86 0.807702\n",
      "Loss for batch i  87 0.714945\n",
      "Loss for batch i  88 0.678915\n",
      "Loss for batch i  89 0.719568\n",
      "Loss for batch i  90 0.447459\n",
      "Loss for batch i  91 0.454006\n",
      "Loss for batch i  92 0.649573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch i  93 0.737057\n",
      "Loss for batch i  94 0.851003\n",
      "Loss for batch i  95 1.1084\n",
      "Loss for batch i  96 1.17365\n",
      "Loss for batch i  97 1.04451\n",
      "Loss for batch i  98 0.97338\n",
      "Loss for batch i  99 0.801839\n",
      "Loss for batch i  100 1.03958\n",
      "Loss for batch i  101 0.918268\n",
      "Loss for batch i  102 0.811546\n",
      "Loss for batch i  103 0.67178\n",
      "Loss for batch i  104 0.829183\n",
      "Loss for batch i  105 0.76947\n",
      "Loss for batch i  106 0.483457\n",
      "Loss for batch i  107 0.49839\n",
      "Loss for batch i  108 0.532995\n",
      "Loss for batch i  109 0.555204\n",
      "Loss for batch i  110 0.491371\n",
      "Loss for batch i  111 0.569649\n",
      "Loss for batch i  112 0.52128\n",
      "Loss for batch i  113 0.437743\n",
      "Loss for batch i  114 0.313183\n",
      "Loss for batch i  115 0.506393\n",
      "Loss for batch i  116 0.452632\n",
      "Loss for batch i  117 0.538221\n",
      "Loss for batch i  118 0.332467\n",
      "Loss for batch i  119 0.335477\n",
      "Loss for batch i  120 0.378826\n",
      "Loss for batch i  121 0.392661\n",
      "Loss for batch i  122 0.250484\n",
      "Loss for batch i  123 0.332911\n",
      "Loss for batch i  124 0.226864\n",
      "Loss for batch i  125 0.340106\n",
      "Loss for batch i  126 0.317492\n",
      "Loss for batch i  127 0.261374\n",
      "Loss for batch i  128 0.295375\n",
      "Loss for batch i  129 0.337944\n",
      "Loss for batch i  130 0.281279\n",
      "Loss for batch i  131 0.280902\n",
      "Loss for batch i  132 0.264391\n",
      "Loss for batch i  133 0.295876\n",
      "Loss for batch i  134 0.353004\n",
      "Loss for batch i  135 0.179205\n",
      "Loss for batch i  136 0.250658\n",
      "Loss for batch i  137 0.361265\n",
      "Loss for batch i  138 0.17127\n",
      "Loss for batch i  139 0.222794\n",
      "Loss for batch i  140 0.274569\n",
      "Loss for batch i  141 0.233872\n",
      "Loss for batch i  142 0.195693\n",
      "Loss for batch i  143 0.256939\n",
      "Loss for batch i  144 0.170077\n",
      "Loss for batch i  145 0.172017\n",
      "Loss for batch i  146 0.404386\n",
      "Loss for batch i  147 0.168303\n",
      "Loss for batch i  148 0.295513\n",
      "Loss for batch i  149 0.504948\n",
      "Loss for batch i  150 0.276906\n",
      "Loss for batch i  151 0.67357\n",
      "Loss for batch i  152 0.652604\n",
      "Loss for batch i  153 0.72512\n",
      "Loss for batch i  154 0.653062\n",
      "Loss for batch i  155 0.453949\n",
      "Loss for batch i  156 0.80087\n",
      "Loss for batch i  157 1.41792\n",
      "Loss for batch i  158 1.51172\n",
      "Loss for batch i  159 1.67526\n",
      "Loss for batch i  160 1.62325\n",
      "Loss for batch i  161 1.45111\n",
      "Loss for batch i  162 1.62583\n",
      "Loss for batch i  163 1.39071\n",
      "Loss for batch i  164 1.43083\n",
      "Loss for batch i  165 1.60967\n",
      "Loss for batch i  166 1.56546\n",
      "Loss for batch i  167 1.33216\n",
      "Loss for batch i  168 1.58572\n",
      "Loss for batch i  169 1.56074\n",
      "Loss for batch i  170 1.52016\n",
      "Loss for batch i  171 1.08003\n",
      "Loss for batch i  172 1.31922\n",
      "Loss for batch i  173 1.20008\n",
      "Loss for batch i  174 1.16064\n",
      "Loss for batch i  175 1.35309\n",
      "Loss for batch i  176 1.14268\n",
      "Loss for batch i  177 1.06216\n",
      "Loss for batch i  178 1.12005\n",
      "Loss for batch i  179 1.06361\n",
      "Loss for batch i  180 1.08169\n",
      "Loss for batch i  181 0.809751\n",
      "Loss for batch i  182 0.860687\n",
      "Loss for batch i  183 0.888072\n",
      "Loss for batch i  184 0.597205\n",
      "Loss for batch i  185 0.691889\n",
      "Loss for batch i  186 0.681213\n",
      "Loss for batch i  187 0.65606\n",
      "Loss for batch i  188 0.605168\n",
      "Loss for batch i  189 0.713261\n",
      "Loss for batch i  190 0.652572\n",
      "Loss for batch i  191 0.636828\n",
      "Loss for batch i  192 0.586881\n",
      "Loss for batch i  193 0.54108\n",
      "Loss for batch i  194 0.665143\n",
      "Loss for batch i  195 0.494397\n",
      "Loss for batch i  196 0.483761\n",
      "Loss for batch i  197 0.598313\n",
      "Loss for batch i  198 0.467022\n",
      "Loss for batch i  199 0.420124\n",
      "Loss for batch i  0 0.543638\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-0\n",
      "Loss for batch i  1 0.362177\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-1\n",
      "Loss for batch i  2 0.515213\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-2\n",
      "Loss for batch i  3 0.492509\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-3\n",
      "Loss for batch i  4 0.463232\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-4\n",
      "Loss for batch i  5 0.443092\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-5\n",
      "Loss for batch i  6 0.609717\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-6\n",
      "Loss for batch i  7 0.852799\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-7\n",
      "Loss for batch i  8 0.678506\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-8\n",
      "Loss for batch i  9 0.493019\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-9\n",
      "Loss for batch i  10 0.654511\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-10\n",
      "Loss for batch i  11 0.479828\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-11\n",
      "Loss for batch i  12 0.474896\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-12\n",
      "Loss for batch i  13 0.513222\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-13\n",
      "Loss for batch i  14 0.55918\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-14\n",
      "Loss for batch i  15 0.47206\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-15\n",
      "Loss for batch i  16 0.458863\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-16\n",
      "Loss for batch i  17 0.368444\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-17\n",
      "Loss for batch i  18 0.306565\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-18\n",
      "Loss for batch i  19 0.48789\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-19\n",
      "Loss for batch i  20 0.418342\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-20\n",
      "Loss for batch i  21 0.427194\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-21\n",
      "Loss for batch i  22 0.573808\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-22\n",
      "Loss for batch i  23 0.651473\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-23\n",
      "Loss for batch i  24 0.643934\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-24\n",
      "Loss for batch i  25 0.544189\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-25\n",
      "Loss for batch i  26 0.5663\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-26\n",
      "Loss for batch i  27 0.689617\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-27\n",
      "Loss for batch i  28 0.570812\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-28\n",
      "Loss for batch i  29 0.570654\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-29\n",
      "Loss for batch i  30 0.528376\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-30\n",
      "Loss for batch i  31 0.661668\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-31\n",
      "Loss for batch i  32 0.54159\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-32\n",
      "Loss for batch i  33 0.46742\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-33\n",
      "Loss for batch i  34 0.542836\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-34\n",
      "Loss for batch i  35 0.379959\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-35\n",
      "Loss for batch i  36 0.445595\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-36\n",
      "Loss for batch i  37 0.373731\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-37\n",
      "Loss for batch i  38 0.419308\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-38\n",
      "Loss for batch i  39 0.255792\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-39\n",
      "Loss for batch i  40 0.35858\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-40\n",
      "Loss for batch i  41 0.359907\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-41\n",
      "Loss for batch i  42 0.400409\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-42\n",
      "Loss for batch i  43 0.254439\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-43\n",
      "Loss for batch i  44 0.343965\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-44\n",
      "Loss for batch i  45 0.338301\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-45\n",
      "Loss for batch i  46 0.440004\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-46\n",
      "Loss for batch i  47 0.391628\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-47\n",
      "Loss for batch i  48 0.361392\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-48\n",
      "Loss for batch i  49 0.286341\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-49\n",
      "Loss for batch i  50 0.259487\n",
      "Loss for batch i  51 0.289534\n",
      "Loss for batch i  52 0.388202\n",
      "Loss for batch i  53 0.451563\n",
      "Loss for batch i  54 0.37236\n",
      "Loss for batch i  55 0.50524\n",
      "Loss for batch i  56 0.277584\n",
      "Loss for batch i  57 0.399425\n",
      "Loss for batch i  58 0.36296\n",
      "Loss for batch i  59 0.831874\n",
      "Loss for batch i  60 0.935995\n",
      "Loss for batch i  61 1.35969\n",
      "Loss for batch i  62 1.45518\n",
      "Loss for batch i  63 1.52534\n",
      "Loss for batch i  64 1.8153\n",
      "Loss for batch i  65 1.98334\n",
      "Loss for batch i  66 1.6708\n",
      "Loss for batch i  67 1.68607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch i  68 1.55363\n",
      "Loss for batch i  69 1.7783\n",
      "Loss for batch i  70 1.77989\n",
      "Loss for batch i  71 2.07483\n",
      "Loss for batch i  72 1.65637\n",
      "Loss for batch i  73 1.44641\n",
      "Loss for batch i  74 1.69202\n",
      "Loss for batch i  75 1.73021\n",
      "Loss for batch i  76 1.61985\n",
      "Loss for batch i  77 1.6432\n",
      "Loss for batch i  78 1.56309\n",
      "Loss for batch i  79 1.57575\n",
      "Loss for batch i  80 1.584\n",
      "Loss for batch i  81 1.43046\n",
      "Loss for batch i  82 1.42624\n",
      "Loss for batch i  83 1.65134\n",
      "Loss for batch i  84 1.42368\n",
      "Loss for batch i  85 1.41331\n",
      "Loss for batch i  86 1.64762\n",
      "Loss for batch i  87 1.44742\n",
      "Loss for batch i  88 1.50192\n",
      "Loss for batch i  89 1.39619\n",
      "Loss for batch i  90 1.2032\n",
      "Loss for batch i  91 1.14782\n",
      "Loss for batch i  92 1.35843\n",
      "Loss for batch i  93 1.39329\n",
      "Loss for batch i  94 1.49288\n",
      "Loss for batch i  95 1.37566\n",
      "Loss for batch i  96 1.43133\n",
      "Loss for batch i  97 1.22351\n",
      "Loss for batch i  98 1.38759\n",
      "Loss for batch i  99 1.25332\n",
      "Loss for batch i  100 1.39829\n",
      "Loss for batch i  101 1.36675\n",
      "Loss for batch i  102 0.996334\n",
      "Loss for batch i  103 1.35918\n",
      "Loss for batch i  104 1.39635\n",
      "Loss for batch i  105 1.2363\n",
      "Loss for batch i  106 1.26279\n",
      "Loss for batch i  107 1.23675\n",
      "Loss for batch i  108 1.08588\n",
      "Loss for batch i  109 1.13806\n",
      "Loss for batch i  110 1.15804\n",
      "Loss for batch i  111 1.10833\n",
      "Loss for batch i  112 1.04935\n",
      "Loss for batch i  113 1.08841\n",
      "Loss for batch i  114 0.948155\n",
      "Loss for batch i  115 1.0634\n",
      "Loss for batch i  116 0.972265\n",
      "Loss for batch i  117 1.18823\n",
      "Loss for batch i  118 0.975269\n",
      "Loss for batch i  119 0.889486\n",
      "Loss for batch i  120 0.986686\n",
      "Loss for batch i  121 0.883448\n",
      "Loss for batch i  122 0.875111\n",
      "Loss for batch i  123 0.954934\n",
      "Loss for batch i  124 0.750432\n",
      "Loss for batch i  125 0.781841\n",
      "Loss for batch i  126 0.805966\n",
      "Loss for batch i  127 0.64258\n",
      "Loss for batch i  128 0.74285\n",
      "Loss for batch i  129 0.691457\n",
      "Loss for batch i  130 0.633402\n",
      "Loss for batch i  131 0.896288\n",
      "Loss for batch i  132 0.89023\n",
      "Loss for batch i  133 0.725147\n",
      "Loss for batch i  134 0.811712\n",
      "Loss for batch i  135 0.65413\n",
      "Loss for batch i  136 0.567708\n",
      "Loss for batch i  137 0.661616\n",
      "Loss for batch i  138 0.600749\n",
      "Loss for batch i  139 0.736372\n",
      "Loss for batch i  140 0.699414\n",
      "Loss for batch i  141 0.558458\n",
      "Loss for batch i  142 0.74212\n",
      "Loss for batch i  143 0.530078\n",
      "Loss for batch i  144 0.519458\n",
      "Loss for batch i  145 0.615033\n",
      "Loss for batch i  146 0.379564\n",
      "Loss for batch i  147 0.45242\n",
      "Loss for batch i  148 0.693738\n",
      "Loss for batch i  149 0.526348\n",
      "Loss for batch i  150 0.469107\n",
      "Loss for batch i  151 0.767919\n",
      "Loss for batch i  152 0.65905\n",
      "Loss for batch i  153 0.693828\n",
      "Loss for batch i  154 0.77748\n",
      "Loss for batch i  155 1.27983\n",
      "Loss for batch i  156 1.28394\n",
      "Loss for batch i  157 1.54592\n",
      "Loss for batch i  158 1.4579\n",
      "Loss for batch i  159 1.59517\n",
      "Loss for batch i  160 1.46174\n",
      "Loss for batch i  161 1.28676\n",
      "Loss for batch i  162 1.40928\n",
      "Loss for batch i  163 1.25243\n",
      "Loss for batch i  164 1.27671\n",
      "Loss for batch i  165 1.36887\n",
      "Loss for batch i  166 1.16853\n",
      "Loss for batch i  167 1.03152\n",
      "Loss for batch i  168 1.25565\n",
      "Loss for batch i  169 1.27441\n",
      "Loss for batch i  170 1.11915\n",
      "Loss for batch i  171 0.853181\n",
      "Loss for batch i  172 1.10969\n",
      "Loss for batch i  173 0.994182\n",
      "Loss for batch i  174 0.868805\n",
      "Loss for batch i  175 0.984562\n",
      "Loss for batch i  176 0.839425\n",
      "Loss for batch i  177 0.713637\n",
      "Loss for batch i  178 0.897614\n",
      "Loss for batch i  179 0.815409\n",
      "Loss for batch i  180 0.675523\n",
      "Loss for batch i  181 0.549847\n",
      "Loss for batch i  182 0.471168\n",
      "Loss for batch i  183 0.592723\n",
      "Loss for batch i  184 0.526179\n",
      "Loss for batch i  185 0.604009\n",
      "Loss for batch i  186 0.499872\n",
      "Loss for batch i  187 0.509377\n",
      "Loss for batch i  188 0.568919\n",
      "Loss for batch i  189 0.555469\n",
      "Loss for batch i  190 0.504792\n",
      "Loss for batch i  191 0.642311\n",
      "Loss for batch i  192 0.445844\n",
      "Loss for batch i  193 0.459236\n",
      "Loss for batch i  194 0.716707\n",
      "Loss for batch i  195 0.70873\n",
      "Loss for batch i  196 0.675425\n",
      "Loss for batch i  197 0.518563\n",
      "Loss for batch i  198 0.534613\n",
      "Loss for batch i  199 0.446969\n",
      "Loss for batch i  0 0.486479\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-0\n",
      "Loss for batch i  1 0.420068\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-1\n",
      "Loss for batch i  2 0.623561\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-2\n",
      "Loss for batch i  3 0.410663\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-3\n",
      "Loss for batch i  4 0.647911\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-4\n",
      "Loss for batch i  5 0.336411\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-5\n",
      "Loss for batch i  6 0.514873\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-6\n",
      "Loss for batch i  7 0.477917\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-7\n",
      "Loss for batch i  8 0.519527\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-8\n",
      "Loss for batch i  9 0.367649\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-9\n",
      "Loss for batch i  10 0.463621\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-10\n",
      "Loss for batch i  11 0.323069\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-11\n",
      "Loss for batch i  12 0.456038\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-12\n",
      "Loss for batch i  13 0.260531\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-13\n",
      "Loss for batch i  14 0.553668\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-14\n",
      "Loss for batch i  15 0.455672\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-15\n",
      "Loss for batch i  16 0.476727\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-16\n",
      "Loss for batch i  17 0.563816\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-17\n",
      "Loss for batch i  18 0.935136\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-18\n",
      "Loss for batch i  19 0.925179\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-19\n",
      "Loss for batch i  20 0.727567\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-20\n",
      "Loss for batch i  21 0.576741\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-21\n",
      "Loss for batch i  22 0.634047\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-22\n",
      "Loss for batch i  23 0.54394\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-23\n",
      "Loss for batch i  24 0.471195\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-24\n",
      "Loss for batch i  25 0.398936\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-25\n",
      "Loss for batch i  26 0.508927\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-26\n",
      "Loss for batch i  27 0.617717\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-27\n",
      "Loss for batch i  28 0.433199\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-28\n",
      "Loss for batch i  29 0.421424\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-29\n",
      "Loss for batch i  30 0.391252\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-30\n",
      "Loss for batch i  31 0.404825\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-31\n",
      "Loss for batch i  32 0.385987\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-32\n",
      "Loss for batch i  33 0.403491\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-33\n",
      "Loss for batch i  34 0.307399\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-34\n",
      "Loss for batch i  35 0.228387\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-35\n",
      "Loss for batch i  36 0.393616\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-36\n",
      "Loss for batch i  37 0.273959\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-37\n",
      "Loss for batch i  38 0.45657\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-38\n",
      "Loss for batch i  39 0.363091\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-39\n",
      "Loss for batch i  40 0.319653\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-40\n",
      "Loss for batch i  41 0.530043\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-41\n",
      "Loss for batch i  42 0.510615\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-42\n",
      "Loss for batch i  43 0.159505\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-43\n",
      "Loss for batch i  44 0.194363\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-44\n",
      "Loss for batch i  45 0.182219\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-45\n",
      "Loss for batch i  46 0.256672\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-46\n",
      "Loss for batch i  47 0.258486\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch i  48 0.351595\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-48\n",
      "Loss for batch i  49 0.182083\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-49\n",
      "Loss for batch i  50 0.328289\n",
      "Loss for batch i  51 0.36957\n",
      "Loss for batch i  52 0.319704\n",
      "Loss for batch i  53 0.401325\n",
      "Loss for batch i  54 0.180018\n",
      "Loss for batch i  55 0.436489\n",
      "Loss for batch i  56 0.233594\n",
      "Loss for batch i  57 0.357179\n",
      "Loss for batch i  58 0.526797\n",
      "Loss for batch i  59 0.374151\n",
      "Loss for batch i  60 0.310472\n",
      "Loss for batch i  61 0.385631\n",
      "Loss for batch i  62 0.560384\n",
      "Loss for batch i  63 0.373312\n",
      "Loss for batch i  64 0.495084\n",
      "Loss for batch i  65 0.752275\n",
      "Loss for batch i  66 0.747646\n",
      "Loss for batch i  67 1.55598\n",
      "Loss for batch i  68 2.0034\n",
      "Loss for batch i  69 2.24795\n",
      "Loss for batch i  70 2.34644\n",
      "Loss for batch i  71 2.649\n",
      "Loss for batch i  72 2.02425\n",
      "Loss for batch i  73 1.72397\n",
      "Loss for batch i  74 2.02874\n",
      "Loss for batch i  75 2.12186\n",
      "Loss for batch i  76 1.95524\n",
      "Loss for batch i  77 2.00353\n",
      "Loss for batch i  78 1.92989\n",
      "Loss for batch i  79 2.00587\n",
      "Loss for batch i  80 1.95835\n",
      "Loss for batch i  81 1.60203\n",
      "Loss for batch i  82 1.56814\n",
      "Loss for batch i  83 1.82644\n",
      "Loss for batch i  84 1.59952\n",
      "Loss for batch i  85 1.54942\n",
      "Loss for batch i  86 1.87574\n",
      "Loss for batch i  87 1.64412\n",
      "Loss for batch i  88 1.6778\n",
      "Loss for batch i  89 1.49329\n",
      "Loss for batch i  90 1.36272\n",
      "Loss for batch i  91 1.29524\n",
      "Loss for batch i  92 1.52175\n",
      "Loss for batch i  93 1.56926\n",
      "Loss for batch i  94 1.64924\n",
      "Loss for batch i  95 1.53761\n",
      "Loss for batch i  96 1.55324\n",
      "Loss for batch i  97 1.38174\n",
      "Loss for batch i  98 1.54045\n",
      "Loss for batch i  99 1.37918\n",
      "Loss for batch i  100 1.61023\n",
      "Loss for batch i  101 1.58086\n",
      "Loss for batch i  102 1.17752\n",
      "Loss for batch i  103 1.47266\n",
      "Loss for batch i  104 1.53663\n",
      "Loss for batch i  105 1.39052\n",
      "Loss for batch i  106 1.38177\n",
      "Loss for batch i  107 1.31993\n",
      "Loss for batch i  108 1.29473\n",
      "Loss for batch i  109 1.27331\n",
      "Loss for batch i  110 1.27173\n",
      "Loss for batch i  111 1.27532\n",
      "Loss for batch i  112 1.22655\n",
      "Loss for batch i  113 1.25047\n",
      "Loss for batch i  114 1.08064\n",
      "Loss for batch i  115 1.20576\n",
      "Loss for batch i  116 1.06539\n",
      "Loss for batch i  117 1.32626\n",
      "Loss for batch i  118 1.23165\n",
      "Loss for batch i  119 1.05473\n",
      "Loss for batch i  120 1.16711\n",
      "Loss for batch i  121 1.08587\n",
      "Loss for batch i  122 1.10492\n",
      "Loss for batch i  123 1.11194\n",
      "Loss for batch i  124 0.998155\n",
      "Loss for batch i  125 0.972434\n",
      "Loss for batch i  126 1.08338\n",
      "Loss for batch i  127 0.850528\n",
      "Loss for batch i  128 0.871637\n",
      "Loss for batch i  129 0.850779\n",
      "Loss for batch i  130 0.809858\n",
      "Loss for batch i  131 1.03926\n",
      "Loss for batch i  132 0.985619\n",
      "Loss for batch i  133 0.855033\n",
      "Loss for batch i  134 1.02656\n",
      "Loss for batch i  135 0.772265\n",
      "Loss for batch i  136 0.848094\n",
      "Loss for batch i  137 0.777287\n",
      "Loss for batch i  138 0.711279\n",
      "Loss for batch i  139 0.824791\n",
      "Loss for batch i  140 0.807883\n",
      "Loss for batch i  141 0.617751\n",
      "Loss for batch i  142 0.690354\n",
      "Loss for batch i  143 0.66732\n",
      "Loss for batch i  144 0.600846\n",
      "Loss for batch i  145 0.670461\n",
      "Loss for batch i  146 0.446887\n",
      "Loss for batch i  147 0.628031\n",
      "Loss for batch i  148 0.671158\n",
      "Loss for batch i  149 0.776997\n",
      "Loss for batch i  150 0.684661\n",
      "Loss for batch i  151 0.914523\n",
      "Loss for batch i  152 0.743748\n",
      "Loss for batch i  153 0.892296\n",
      "Loss for batch i  154 0.625978\n",
      "Loss for batch i  155 0.68059\n",
      "Loss for batch i  156 0.692513\n",
      "Loss for batch i  157 0.701173\n",
      "Loss for batch i  158 0.702167\n",
      "Loss for batch i  159 0.895562\n",
      "Loss for batch i  160 0.890241\n",
      "Loss for batch i  161 0.802236\n",
      "Loss for batch i  162 0.718768\n",
      "Loss for batch i  163 0.636893\n",
      "Loss for batch i  164 0.627319\n",
      "Loss for batch i  165 0.648804\n",
      "Loss for batch i  166 0.694014\n",
      "Loss for batch i  167 0.514482\n",
      "Loss for batch i  168 0.782087\n",
      "Loss for batch i  169 0.680226\n",
      "Loss for batch i  170 0.657202\n",
      "Loss for batch i  171 0.490131\n",
      "Loss for batch i  172 0.541329\n",
      "Loss for batch i  173 0.518561\n",
      "Loss for batch i  174 0.391007\n",
      "Loss for batch i  175 0.487033\n",
      "Loss for batch i  176 0.444272\n",
      "Loss for batch i  177 0.420947\n",
      "Loss for batch i  178 0.450033\n",
      "Loss for batch i  179 0.421362\n",
      "Loss for batch i  180 0.41746\n",
      "Loss for batch i  181 0.436953\n",
      "Loss for batch i  182 0.299032\n",
      "Loss for batch i  183 0.325223\n",
      "Loss for batch i  184 0.874773\n",
      "Loss for batch i  185 1.42902\n",
      "Loss for batch i  186 1.51635\n",
      "Loss for batch i  187 1.62541\n",
      "Loss for batch i  188 1.42586\n",
      "Loss for batch i  189 1.44556\n",
      "Loss for batch i  190 1.43074\n",
      "Loss for batch i  191 1.44243\n",
      "Loss for batch i  192 1.35379\n",
      "Loss for batch i  193 1.41356\n",
      "Loss for batch i  194 1.58663\n",
      "Loss for batch i  195 1.5082\n",
      "Loss for batch i  196 1.26957\n",
      "Loss for batch i  197 1.27439\n",
      "Loss for batch i  198 1.27076\n",
      "Loss for batch i  199 0.942069\n",
      "Loss for batch i  0 1.09561\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-0\n",
      "Loss for batch i  1 1.04235\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-1\n",
      "Loss for batch i  2 1.24326\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-2\n",
      "Loss for batch i  3 1.08769\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-3\n",
      "Loss for batch i  4 1.02109\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-4\n",
      "Loss for batch i  5 0.952997\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-5\n",
      "Loss for batch i  6 1.10325\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-6\n",
      "Loss for batch i  7 1.01829\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-7\n",
      "Loss for batch i  8 0.927506\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-8\n",
      "Loss for batch i  9 0.864225\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-9\n",
      "Loss for batch i  10 0.964264\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-10\n",
      "Loss for batch i  11 0.75795\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-11\n",
      "Loss for batch i  12 0.786694\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-12\n",
      "Loss for batch i  13 0.644374\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-13\n",
      "Loss for batch i  14 0.757001\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-14\n",
      "Loss for batch i  15 0.646751\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-15\n",
      "Loss for batch i  16 0.701208\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-16\n",
      "Loss for batch i  17 0.593559\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-17\n",
      "Loss for batch i  18 0.605652\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-18\n",
      "Loss for batch i  19 0.751577\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-19\n",
      "Loss for batch i  20 0.638346\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-20\n",
      "Loss for batch i  21 0.583467\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-21\n",
      "Loss for batch i  22 0.608457\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-22\n",
      "Loss for batch i  23 0.730889\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-23\n",
      "Loss for batch i  24 0.441413\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-24\n",
      "Loss for batch i  25 0.480964\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-25\n",
      "Loss for batch i  26 0.71881\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-26\n",
      "Loss for batch i  27 0.745651\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-27\n",
      "Loss for batch i  28 0.496646\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-28\n",
      "Loss for batch i  29 0.572454\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-29\n",
      "Loss for batch i  30 0.629839\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-30\n",
      "Loss for batch i  31 0.612738\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-31\n",
      "Loss for batch i  32 0.526837\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-32\n",
      "Loss for batch i  33 0.603\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-33\n",
      "Loss for batch i  34 0.473664\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-34\n",
      "Loss for batch i  35 0.409704\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-35\n",
      "Loss for batch i  36 0.353475\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-36\n",
      "Loss for batch i  37 0.402946\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-37\n",
      "Loss for batch i  38 0.475726\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-38\n",
      "Loss for batch i  39 0.372394\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch i  40 0.348569\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-40\n",
      "Loss for batch i  41 0.421856\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-41\n",
      "Loss for batch i  42 0.346894\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-42\n",
      "Loss for batch i  43 0.221108\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-43\n",
      "Loss for batch i  44 0.292881\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-44\n",
      "Loss for batch i  45 0.268709\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-45\n",
      "Loss for batch i  46 0.235064\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-46\n",
      "Loss for batch i  47 0.254681\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-47\n",
      "Loss for batch i  48 0.294381\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-48\n",
      "Loss for batch i  49 0.272855\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-49\n",
      "Loss for batch i  50 0.367021\n",
      "Loss for batch i  51 0.321028\n",
      "Loss for batch i  52 0.191154\n",
      "Loss for batch i  53 0.274127\n",
      "Loss for batch i  54 0.334146\n",
      "Loss for batch i  55 0.28494\n",
      "Loss for batch i  56 0.250017\n",
      "Loss for batch i  57 0.302625\n",
      "Loss for batch i  58 0.245136\n",
      "Loss for batch i  59 0.211582\n",
      "Loss for batch i  60 0.232606\n",
      "Loss for batch i  61 0.344327\n",
      "Loss for batch i  62 0.451624\n",
      "Loss for batch i  63 0.332714\n",
      "Loss for batch i  64 0.463235\n",
      "Loss for batch i  65 1.36886\n",
      "Loss for batch i  66 1.4308\n",
      "Loss for batch i  67 1.5024\n",
      "Loss for batch i  68 1.35077\n",
      "Loss for batch i  69 1.59596\n",
      "Loss for batch i  70 1.47606\n",
      "Loss for batch i  71 1.92728\n",
      "Loss for batch i  72 1.42332\n",
      "Loss for batch i  73 1.05112\n",
      "Loss for batch i  74 1.39771\n",
      "Loss for batch i  75 1.36248\n",
      "Loss for batch i  76 1.25885\n",
      "Loss for batch i  77 1.22756\n",
      "Loss for batch i  78 1.03552\n",
      "Loss for batch i  79 1.08858\n",
      "Loss for batch i  80 0.990278\n",
      "Loss for batch i  81 0.906784\n",
      "Loss for batch i  82 0.816973\n",
      "Loss for batch i  83 1.11017\n",
      "Loss for batch i  84 0.803063\n",
      "Loss for batch i  85 0.694534\n",
      "Loss for batch i  86 0.958671\n",
      "Loss for batch i  87 0.79483\n",
      "Loss for batch i  88 0.580985\n",
      "Loss for batch i  89 0.689028\n",
      "Loss for batch i  90 0.546634\n",
      "Loss for batch i  91 0.592628\n",
      "Loss for batch i  92 0.509932\n",
      "Loss for batch i  93 0.594658\n",
      "Loss for batch i  94 0.61921\n",
      "Loss for batch i  95 0.659973\n",
      "Loss for batch i  96 0.735974\n",
      "Loss for batch i  97 0.786614\n",
      "Loss for batch i  98 0.757337\n",
      "Loss for batch i  99 0.545167\n",
      "Loss for batch i  100 0.783887\n",
      "Loss for batch i  101 0.590828\n",
      "Loss for batch i  102 0.449152\n",
      "Loss for batch i  103 0.716175\n",
      "Loss for batch i  104 0.800724\n",
      "Loss for batch i  105 0.681605\n",
      "Loss for batch i  106 0.506352\n",
      "Loss for batch i  107 0.468911\n",
      "Loss for batch i  108 0.514355\n",
      "Loss for batch i  109 0.552553\n",
      "Loss for batch i  110 0.493783\n",
      "Loss for batch i  111 0.519252\n",
      "Loss for batch i  112 0.487\n",
      "Loss for batch i  113 0.469709\n",
      "Loss for batch i  114 0.422952\n",
      "Loss for batch i  115 0.757379\n",
      "Loss for batch i  116 0.566843\n",
      "Loss for batch i  117 0.738941\n",
      "Loss for batch i  118 0.685935\n",
      "Loss for batch i  119 0.583459\n",
      "Loss for batch i  120 0.622623\n",
      "Loss for batch i  121 0.618539\n",
      "Loss for batch i  122 0.636217\n",
      "Loss for batch i  123 0.585556\n",
      "Loss for batch i  124 0.606913\n",
      "Loss for batch i  125 0.50891\n",
      "Loss for batch i  126 0.593502\n",
      "Loss for batch i  127 0.38178\n",
      "Loss for batch i  128 0.571032\n",
      "Loss for batch i  129 0.468554\n",
      "Loss for batch i  130 0.418285\n",
      "Loss for batch i  131 0.559897\n",
      "Loss for batch i  132 0.45537\n",
      "Loss for batch i  133 0.379307\n",
      "Loss for batch i  134 0.468927\n",
      "Loss for batch i  135 0.428253\n",
      "Loss for batch i  136 0.323513\n",
      "Loss for batch i  137 0.322287\n",
      "Loss for batch i  138 0.2501\n",
      "Loss for batch i  139 0.427733\n",
      "Loss for batch i  140 0.362425\n",
      "Loss for batch i  141 0.292568\n",
      "Loss for batch i  142 0.386505\n",
      "Loss for batch i  143 0.311072\n",
      "Loss for batch i  144 0.394512\n",
      "Loss for batch i  145 0.542817\n",
      "Loss for batch i  146 0.627703\n",
      "Loss for batch i  147 0.871931\n",
      "Loss for batch i  148 0.999318\n",
      "Loss for batch i  149 0.934626\n",
      "Loss for batch i  150 0.631821\n",
      "Loss for batch i  151 1.06152\n",
      "Loss for batch i  152 0.75702\n",
      "Loss for batch i  153 0.764677\n",
      "Loss for batch i  154 0.738887\n",
      "Loss for batch i  155 0.670008\n",
      "Loss for batch i  156 0.66293\n",
      "Loss for batch i  157 0.687017\n",
      "Loss for batch i  158 0.626081\n",
      "Loss for batch i  159 0.636212\n",
      "Loss for batch i  160 0.55825\n",
      "Loss for batch i  161 0.568294\n",
      "Loss for batch i  162 0.526935\n",
      "Loss for batch i  163 0.429925\n",
      "Loss for batch i  164 0.443099\n",
      "Loss for batch i  165 0.32267\n",
      "Loss for batch i  166 0.355554\n",
      "Loss for batch i  167 0.240134\n",
      "Loss for batch i  168 0.314991\n",
      "Loss for batch i  169 0.486036\n",
      "Loss for batch i  170 0.438991\n",
      "Loss for batch i  171 0.3122\n",
      "Loss for batch i  172 0.371803\n",
      "Loss for batch i  173 0.314139\n",
      "Loss for batch i  174 0.315527\n",
      "Loss for batch i  175 0.354584\n",
      "Loss for batch i  176 0.479283\n",
      "Loss for batch i  177 0.391913\n",
      "Loss for batch i  178 0.583657\n",
      "Loss for batch i  179 0.712067\n",
      "Loss for batch i  180 0.50708\n",
      "Loss for batch i  181 0.43395\n",
      "Loss for batch i  182 0.328443\n",
      "Loss for batch i  183 0.590855\n",
      "Loss for batch i  184 0.352754\n",
      "Loss for batch i  185 0.278687\n",
      "Loss for batch i  186 0.415991\n",
      "Loss for batch i  187 0.411329\n",
      "Loss for batch i  188 0.442777\n",
      "Loss for batch i  189 0.212913\n",
      "Loss for batch i  190 0.282748\n",
      "Loss for batch i  191 0.253793\n",
      "Loss for batch i  192 0.240336\n",
      "Loss for batch i  193 0.320887\n",
      "Loss for batch i  194 0.257471\n",
      "Loss for batch i  195 0.314876\n",
      "Loss for batch i  196 0.295386\n",
      "Loss for batch i  197 0.174234\n",
      "Loss for batch i  198 0.210931\n",
      "Loss for batch i  199 0.183289\n",
      "Loss for batch i  0 0.157428\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-0\n",
      "Loss for batch i  1 0.195638\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-1\n",
      "Loss for batch i  2 0.230826\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-2\n",
      "Loss for batch i  3 0.180288\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-3\n",
      "Loss for batch i  4 0.148783\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-4\n",
      "Loss for batch i  5 0.153927\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-5\n",
      "Loss for batch i  6 0.283563\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-6\n",
      "Loss for batch i  7 0.194107\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-7\n",
      "Loss for batch i  8 0.149933\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-8\n",
      "Loss for batch i  9 0.122393\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-9\n",
      "Loss for batch i  10 0.158772\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-10\n",
      "Loss for batch i  11 0.127769\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-11\n",
      "Loss for batch i  12 0.204393\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-12\n",
      "Loss for batch i  13 0.141972\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-13\n",
      "Loss for batch i  14 0.230116\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-14\n",
      "Loss for batch i  15 0.225063\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-15\n",
      "Loss for batch i  16 0.212548\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-16\n",
      "Loss for batch i  17 0.125406\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-17\n",
      "Loss for batch i  18 0.121423\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-18\n",
      "Loss for batch i  19 0.249226\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-19\n",
      "Loss for batch i  20 0.143142\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-20\n",
      "Loss for batch i  21 0.134997\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-21\n",
      "Loss for batch i  22 0.240409\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-22\n",
      "Loss for batch i  23 0.205242\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-23\n",
      "Loss for batch i  24 0.18566\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-24\n",
      "Loss for batch i  25 0.272816\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-25\n",
      "Loss for batch i  26 0.184821\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-26\n",
      "Loss for batch i  27 0.18771\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-27\n",
      "Loss for batch i  28 0.169639\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-28\n",
      "Loss for batch i  29 0.17553\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-29\n",
      "Loss for batch i  30 0.158281\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-30\n",
      "Loss for batch i  31 0.195709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-31\n",
      "Loss for batch i  32 0.206807\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-32\n",
      "Loss for batch i  33 0.142259\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-33\n",
      "Loss for batch i  34 0.236897\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-34\n",
      "Loss for batch i  35 0.229535\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-35\n",
      "Loss for batch i  36 0.333825\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-36\n",
      "Loss for batch i  37 0.398877\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-37\n",
      "Loss for batch i  38 0.59881\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-38\n",
      "Loss for batch i  39 0.387574\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-39\n",
      "Loss for batch i  40 0.24723\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-40\n",
      "Loss for batch i  41 0.593907\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-41\n",
      "Loss for batch i  42 0.492516\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-42\n",
      "Loss for batch i  43 0.276151\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-43\n",
      "Loss for batch i  44 0.500297\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-44\n",
      "Loss for batch i  45 0.425556\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-45\n",
      "Loss for batch i  46 0.482196\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-46\n",
      "Loss for batch i  47 0.444925\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-47\n",
      "Loss for batch i  48 0.587454\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-48\n",
      "Loss for batch i  49 0.556864\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-49\n",
      "Loss for batch i  50 0.535955\n",
      "Loss for batch i  51 0.708384\n",
      "Loss for batch i  52 0.360325\n",
      "Loss for batch i  53 0.620214\n",
      "Loss for batch i  54 0.575192\n",
      "Loss for batch i  55 0.499266\n",
      "Loss for batch i  56 0.318496\n",
      "Loss for batch i  57 0.579999\n",
      "Loss for batch i  58 0.288689\n",
      "Loss for batch i  59 0.385564\n",
      "Loss for batch i  60 0.491563\n",
      "Loss for batch i  61 0.487397\n",
      "Loss for batch i  62 0.442493\n",
      "Loss for batch i  63 0.3956\n",
      "Loss for batch i  64 0.342483\n",
      "Loss for batch i  65 0.549792\n",
      "Loss for batch i  66 0.441863\n",
      "Loss for batch i  67 0.290794\n",
      "Loss for batch i  68 0.246194\n",
      "Loss for batch i  69 0.338088\n",
      "Loss for batch i  70 0.313989\n",
      "Loss for batch i  71 0.45694\n",
      "Loss for batch i  72 0.292824\n",
      "Loss for batch i  73 0.323099\n",
      "Loss for batch i  74 0.283789\n",
      "Loss for batch i  75 0.300278\n",
      "Loss for batch i  76 0.239855\n",
      "Loss for batch i  77 0.31274\n",
      "Loss for batch i  78 0.276644\n",
      "Loss for batch i  79 0.276039\n",
      "Loss for batch i  80 0.314365\n",
      "Loss for batch i  81 0.228712\n",
      "Loss for batch i  82 0.206244\n",
      "Loss for batch i  83 0.222029\n",
      "Loss for batch i  84 0.347481\n",
      "Loss for batch i  85 0.228935\n",
      "Loss for batch i  86 0.470964\n",
      "Loss for batch i  87 0.48618\n",
      "Loss for batch i  88 0.399684\n",
      "Loss for batch i  89 0.618809\n",
      "Loss for batch i  90 0.513834\n",
      "Loss for batch i  91 0.245928\n",
      "Loss for batch i  92 0.347908\n",
      "Loss for batch i  93 0.243642\n",
      "Loss for batch i  94 0.329363\n",
      "Loss for batch i  95 0.226707\n",
      "Loss for batch i  96 0.309902\n",
      "Loss for batch i  97 0.205112\n",
      "Loss for batch i  98 0.322876\n",
      "Loss for batch i  99 0.310184\n",
      "Loss for batch i  100 0.366632\n",
      "Loss for batch i  101 0.356697\n",
      "Loss for batch i  102 0.236392\n",
      "Loss for batch i  103 0.342228\n",
      "Loss for batch i  104 0.3019\n",
      "Loss for batch i  105 0.33367\n",
      "Loss for batch i  106 0.261282\n",
      "Loss for batch i  107 0.332926\n",
      "Loss for batch i  108 0.224167\n",
      "Loss for batch i  109 0.147179\n",
      "Loss for batch i  110 0.154198\n",
      "Loss for batch i  111 0.196835\n",
      "Loss for batch i  112 0.274131\n",
      "Loss for batch i  113 0.252566\n",
      "Loss for batch i  114 0.133241\n",
      "Loss for batch i  115 0.196768\n",
      "Loss for batch i  116 0.223459\n",
      "Loss for batch i  117 0.17801\n",
      "Loss for batch i  118 0.212332\n",
      "Loss for batch i  119 0.237791\n",
      "Loss for batch i  120 0.31792\n",
      "Loss for batch i  121 0.120617\n",
      "Loss for batch i  122 0.156579\n",
      "Loss for batch i  123 0.154666\n",
      "Loss for batch i  124 0.239805\n",
      "Loss for batch i  125 0.2265\n",
      "Loss for batch i  126 0.277167\n",
      "Loss for batch i  127 0.213764\n",
      "Loss for batch i  128 0.214517\n",
      "Loss for batch i  129 0.132909\n",
      "Loss for batch i  130 0.277589\n",
      "Loss for batch i  131 0.213509\n",
      "Loss for batch i  132 0.585041\n",
      "Loss for batch i  133 0.253347\n",
      "Loss for batch i  134 0.260263\n",
      "Loss for batch i  135 0.268802\n",
      "Loss for batch i  136 0.578044\n",
      "Loss for batch i  137 0.349446\n",
      "Loss for batch i  138 0.556403\n",
      "Loss for batch i  139 0.340875\n",
      "Loss for batch i  140 0.456537\n",
      "Loss for batch i  141 0.307392\n",
      "Loss for batch i  142 0.256048\n",
      "Loss for batch i  143 0.206581\n",
      "Loss for batch i  144 0.31003\n",
      "Loss for batch i  145 0.313517\n",
      "Loss for batch i  146 0.279083\n",
      "Loss for batch i  147 0.299215\n",
      "Loss for batch i  148 0.399155\n",
      "Loss for batch i  149 0.387532\n",
      "Loss for batch i  150 0.360525\n",
      "Loss for batch i  151 0.648394\n",
      "Loss for batch i  152 0.511091\n",
      "Loss for batch i  153 0.56522\n",
      "Loss for batch i  154 0.175879\n",
      "Loss for batch i  155 0.349213\n",
      "Loss for batch i  156 0.416587\n",
      "Loss for batch i  157 0.522695\n",
      "Loss for batch i  158 0.713476\n",
      "Loss for batch i  159 0.65377\n",
      "Loss for batch i  160 0.527141\n",
      "Loss for batch i  161 0.594704\n",
      "Loss for batch i  162 0.351697\n",
      "Loss for batch i  163 0.497396\n",
      "Loss for batch i  164 0.422638\n",
      "Loss for batch i  165 0.409812\n",
      "Loss for batch i  166 0.416467\n",
      "Loss for batch i  167 0.5611\n",
      "Loss for batch i  168 0.482491\n",
      "Loss for batch i  169 0.500691\n",
      "Loss for batch i  170 0.558855\n",
      "Loss for batch i  171 0.177758\n",
      "Loss for batch i  172 0.342203\n",
      "Loss for batch i  173 0.267323\n",
      "Loss for batch i  174 0.232895\n",
      "Loss for batch i  175 0.430004\n",
      "Loss for batch i  176 0.543637\n",
      "Loss for batch i  177 0.336359\n",
      "Loss for batch i  178 0.46801\n",
      "Loss for batch i  179 0.443792\n",
      "Loss for batch i  180 0.2256\n",
      "Loss for batch i  181 0.239578\n",
      "Loss for batch i  182 0.309224\n",
      "Loss for batch i  183 0.228203\n",
      "Loss for batch i  184 0.312753\n",
      "Loss for batch i  185 0.273899\n",
      "Loss for batch i  186 0.367573\n",
      "Loss for batch i  187 0.39131\n",
      "Loss for batch i  188 0.24476\n",
      "Loss for batch i  189 0.193632\n",
      "Loss for batch i  190 0.216844\n",
      "Loss for batch i  191 0.340688\n",
      "Loss for batch i  192 0.252949\n",
      "Loss for batch i  193 0.25579\n",
      "Loss for batch i  194 0.289047\n",
      "Loss for batch i  195 0.240082\n",
      "Loss for batch i  196 0.134846\n",
      "Loss for batch i  197 0.181598\n",
      "Loss for batch i  198 0.211138\n",
      "Loss for batch i  199 0.132303\n",
      "Saved file: seq_seq_checkpoints/seq_seq_train1503085280-199\n"
     ]
    }
   ],
   "source": [
    "for encoderInputs,encoderInputLength,decoderInputs,decoderInputLength,decoderTargets,decoderTargetLength,batch_num in generateBatch(train,target,BATCH_SIZE,epoch):\n",
    "    feedDict = {ENCODER_INPUTS:encoderInputs,EncoderSeqLength:encoderInputLength,DecoderSeqLength:decoderInputLength,DECODER_INPUTS:decoderInputs,decoder_targets:decoderTargets,lr:0.001,decoder_batch_size:np.array([BATCH_SIZE])}\n",
    "    _,l = sess.run([optimiser,loss],feed_dict=feedDict)\n",
    "    predict_ = sess.run(decoder_prediction,feed_dict=feedDict)\n",
    "    print(\"Loss for batch i \", batch_num,l)\n",
    "    if batch_num // 50 == 0:\n",
    "        saved_file = saver.save(sess, 'seq_seq_checkpoints/seq_seq_train' + timestamp, global_step=batch_num)\n",
    "        print(\"Saved file: \" + saved_file)\n",
    "saved_file = saver.save(sess, 'seq_seq_checkpoints/seq_seq_train' + timestamp, global_step=batch_num)\n",
    "print(\"Saved file: \" + saved_file) \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'rnn/while/Exit_2:0' shape=(?, 512) dtype=float32>,\n",
       " <tf.Tensor 'rnn/while/Exit_3:0' shape=(?, 512) dtype=float32>,\n",
       " AttentionWrapperState(cell_state=<tf.Tensor 'rnn/while/Exit_4:0' shape=(?, 512) dtype=float32>, attention=<tf.Tensor 'AttentionWrapperZeroState/zeros_1:0' shape=(?, 512) dtype=float32>, time=<tf.Tensor 'AttentionWrapperZeroState/zeros:0' shape=() dtype=int32>, alignments=<tf.Tensor 'AttentionWrapperZeroState/zeros_2:0' shape=(?, ?) dtype=float32>, alignment_history=()))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "decoderStartTokens = tf.placeholder(tf.int32,shape=[None], name=\"start_token\")\n",
    "decoderEndToken = EOS\n",
    "## since decoder during inference has no input, so we use GreedyEmbeddingHelper which takes as input only the embeddingmatrix.\n",
    "## during training, input to decoder is seq of words started with GO, output target output shifted by 1 and ending with EOS.\n",
    "##So we specify those charcaters also as input for the GreedyEmbedding helper;\n",
    "## Uses the argmax of the output (treated as logits) and passes the result through an embedding layer to get the next input.\n",
    "decoderHelper = tf.contrib.seq2seq.GreedyEmbeddingHelper(decoder_embeddings,decoderStartTokens,decoderEndToken)\n",
    "inference_decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell,decoderHelper,decoder_input_state,decoder_output_layer)\n",
    "final_inference_outputs,final_inference_state,final_inference_sequence_lengths= tf.contrib.seq2seq.dynamic_decode(inference_decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_input = [[0,2,4,6,8,10],[1,3,5,7,9,11,13,15,17,2]]\n",
    "test_input,input_length = modifyBatch(test_input)\n",
    "decoder_start_token = np.array([GO,GO])\n",
    "feed_Dict = {ENCODER_INPUTS: test_input,EncoderSeqLength:np.array(input_length),decoderStartTokens:decoder_start_token,decoder_batch_size:np.array([2])}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  2,  4,  6,  8, 10, 21, 21, 21, 21],\n",
       "       [ 1,  3,  5,  7,  9, 11, 13, 15, 17,  2]], dtype=int32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BasicDecoderOutput(rnn_output=<tf.Tensor 'decoder_1/transpose:0' shape=(?, ?, 23) dtype=float32>, sample_id=<tf.Tensor 'decoder_1/transpose_1:0' shape=(?, ?) dtype=int32>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_inference_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_saver = tf.train.import_meta_graph('seq_seq_checkpoints1/seq_seq_train1500951341-49.meta')\n",
    "#new_saver.restore(sess, \"seq_seq_checkpoints1/seq_seq_train1500951341-49\")\n",
    "predict_test,seqLength = sess.run([final_inference_outputs,final_inference_sequence_lengths],feed_dict = feed_Dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  8,  6,  4,  2,  0, 20, 20, 20, 20, 20],\n",
       "       [ 2, 17, 15, 13, 11,  9,  7,  5,  3,  1, 20]], dtype=int32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_test.sample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 11, 23)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
